{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import clrs\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import pprint\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2**32))"
   ],
   "metadata": {
    "id": "2MzxRB1X7hRs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eee4d47e-b689-497b-e9b5-021121cac2b7"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 23:10:48.446232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_sampler, spec = clrs.build_sampler(\n",
    "    name='auction_matching',\n",
    "    num_samples=100,\n",
    "    length=16,\n",
    "    weighted=True) # number of nodes\n",
    "\n",
    "test_sampler, spec = clrs.build_sampler(\n",
    "    name='auction_matching',\n",
    "    num_samples=40, # TODO set back to more\n",
    "    length=64,\n",
    "    weighted=True) # testing on much larger\n",
    "# TODO how do you know aren't generating same graphs? (well not possible here since different size but in general?)\n",
    "\n",
    "pprint.pprint(spec) # spec is the algorithm specification, all the probes\n",
    "\n",
    "def _iterate_sampler(sampler, batch_size):\n",
    "  while True:\n",
    "    yield sampler.next(batch_size)\n",
    "\n",
    "# TODO put back normal batch values\n",
    "train_sampler = _iterate_sampler(train_sampler, batch_size=32)\n",
    "test_sampler = _iterate_sampler(test_sampler, batch_size=40) # full batch for the test set\n",
    "\n"
   ],
   "metadata": {
    "id": "OEo_Gj1j3Z6M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ef53c51-9bc2-4a49-b4cf-592a32dfa5b7"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ('input', 'edge', 'scalar'),\n",
      " 'adj': ('input', 'edge', 'mask'),\n",
      " 'buyers': ('input', 'node', 'mask'),\n",
      " 'in_queue': ('hint', 'node', 'mask'),\n",
      " 'owners': ('output', 'node', 'pointer'),\n",
      " 'owners_h': ('hint', 'node', 'pointer'),\n",
      " 'p': ('hint', 'node', 'scalar'),\n",
      " 'pos': ('input', 'node', 'scalar')}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "processor_factory = clrs.get_processor_factory('mpnn', use_ln=True, nb_triplet_fts=0) #use_ln => use layer norm\n",
    "# processor_factory = clrs.get_processor_factory('gat', use_ln=True, nb_heads = 4, nb_triplet_fts = 0)\n",
    "model_params = dict(\n",
    "    processor_factory=processor_factory, # contains the processor_factory\n",
    "    hidden_dim=32, # TODO put back to 32 if no difference, indeed not much diff for MPNN\n",
    "    encode_hints=True,\n",
    "    decode_hints=True,\n",
    "    #decode_diffs=False,\n",
    "    #hint_teacher_forcing_noise=1.0,\n",
    "    hint_teacher_forcing=1.0,\n",
    "    use_lstm=False,\n",
    "    learning_rate=0.001,\n",
    "    checkpoint_path='/tmp/checkpt',\n",
    "    freeze_processor=False, # Good for post step\n",
    "    dropout_prob=0.5,\n",
    "    # nb_msg_passing_steps=3,\n",
    ")\n",
    "\n",
    "dummy_trajectory = next(train_sampler) # jax needs a trajectory that is plausible looking to init\n",
    "\n",
    "model = clrs.models.BaselineModel(\n",
    "    spec=spec,\n",
    "    dummy_trajectory=dummy_trajectory,\n",
    "    **model_params\n",
    ")\n",
    "\n",
    "model.init(dummy_trajectory.features, 1234) # 1234 is a random seed"
   ],
   "metadata": {
    "id": "L-p0jOCq5sPV"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# No evaluation since we are postprocessing with soft: TO CHANGE -> baselines.py line 336 outs change hard to False\n",
    "step = 0\n",
    "\n",
    "while step <= 100:\n",
    "    feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "    rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "    cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "    rng_key = new_rng_key\n",
    "    if step % 10 == 0:\n",
    "        print(step)\n",
    "    step += 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# import copy\n",
    "#\n",
    "# step = 0\n",
    "# while step <= 20:\n",
    "#   feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "#\n",
    "#   rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "#   cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "#   rng_key = new_rng_key\n",
    "#   # if step % 10 == 0:\n",
    "#   #   predictions_val, _ = model.predict(rng_key, feedback.features)\n",
    "#   #   out_val = clrs.evaluate(feedback.outputs, predictions_val)\n",
    "#   #   predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "#   #   out = clrs.evaluate(test_feedback.outputs, predictions)\n",
    "#   #   print(f'step = {step} | loss = {cur_loss} | val_acc = {out_val[\"score\"]} | test_acc = {out[\"score\"]}') # here, val accuracy is actually training accuracy, not great but is example\n",
    "#   step += 1\n",
    "# model2 = copy.deepcopy(model)"
   ],
   "metadata": {
    "id": "3pSKQ2wi62Br",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1acd203-e256-4f39-e059-9c7e55942a87"
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n",
      "-------------- step -----------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some intermediate results\n",
    "ALL: 0.5 dropout\n",
    "\n",
    "MPNN 100 train, 40 test, 100 epochs, self-loops -> loss = 0.9931782484054565 | val_acc = 0.8515625 | test_acc = 0.7484375238418579 | accuracy = 0.65, average nb non-matched: 13.45\n",
    "\n",
    "MPNN 200 train, 40 test, 100 epochs, self-loops -> loss = 0.8129420280456543 | val_acc = 0.8125 | test_acc = 0.77734375 | accuracy = 0.72, average nb non-matched: 11.125\n",
    "\n",
    "MPNN 100 train, 40 test, 100 epochs, double links -> loss = 0.8689386248588562 | val_acc = 0.7109375 | test_acc = 0.42695313692092896 | accuracy =? NOTE: only started \"learning\" in the last epochs => trying more, interestingly has less loss than self-loops but less accuracy too\n",
    "\n",
    "MPNN 100 train, 40 test, 200 epochs, double links -> step = 100 | loss = 0.6802611351013184 | val_acc = 0.806640625 | test_acc = 0.681640625 | accuracy = 0.89, average nb non-matched: 5.65\n",
    "\n",
    "MPNN 300 train, 40 test, 400 epochs, double links -> loss = 0.5485531091690063 | val_acc = 0.775390625 | test_acc = 0.6910156607627869 | accuracy = 0.928, average nb non-matched: 4.075 Note: best test_acc 0.727, similar test_acc to 100 train 200 epochs but better accuracy + still does not converge on training accuracy though\n",
    "\n",
    "MPNN 200 train, 40 test, 200 epochs, double links -> 94.4% acc without added matches, 95.7% with added matches\n",
    "\n",
    "Diff: length 100 testing instead of 64\n",
    "MPNN 100 train, 40 test LENGTH 100, 200 epochs, double links -> loss = 0.6958761215209961 | val_acc = 0.787109375 | test_acc = 0.503250002861023 | accuracy = 0.759, average nb non-matched: 7.9/100\n",
    "\n",
    "\n",
    "#### Now with actually bipartite graph (no owner-owner / good-good edges)\n",
    "Doesn't really change results\n",
    "\n",
    "ALL with 0 dropout\n",
    "\n",
    "#### No hints\n",
    "0 dropout Can get up to 0.78 of OPT, average nb non-matched: 10.5/64\n",
    "\n",
    "\n",
    "#### Training with 64 hidden dimensions\n",
    "MPNN 100 train, 40 test, 0 dropout, double links -> Get to 90% acc in 30 iterations, 93% in 60\n",
    "GAT 100 train, 40 test, 0 dropout, double links -> Get to 0.78 in 30 iterations, 91% in 60, 92% in 100, 92% in 200\n",
    "\n",
    "Same with 0.5 dropout\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links -> Get to 85% in 30 iterations, 93% in 60 iterations, 93% in 100 iterations\n",
    "GAT 100 train, 40 test, 0.5 dropout, double links -> 94.7% in 200\n",
    "\n",
    "#### 3 message passing steps\n",
    "TLDR not great\n",
    "64 dims, 3 message passing steps\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links -> 91.7% in 100 iterations (worse than 1 MP step), 93% in 200 iterations\n",
    "\n",
    "Back to 1 message passing step\n",
    "\n",
    "#### Larger MLP\n",
    "TLDR not great\n",
    "[out_size, out_size, out_size] MLP\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links, 3 layer MLP -> loss = 0.6642395257949829 | val_acc = 0.75390625 | test_acc = 0.699999988079071 | 90% in 100 iterations (worse than smaller MLP) | 93% in 200 iterations\n",
    "\n",
    "# Add max matching to greedy\n",
    "\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links, 64 hidden -> trained on 100 iterations, goes from 91% to 94% with added max matching, 200 iterations 91.7 -> 94.7 (exactly 3% more again)\n",
    "\n",
    "#### On 32 vs 64\n",
    "32 learns slower (will need 200 iterations to get to good acc) but basically peaks at same values as 64\n",
    "\n",
    "#### Partial matchings\n",
    "All tested with the usual 200 epochs of MPNN where greedy gets up to 0.92-ish\n",
    "\n",
    "Softmax gets 0.66 with max weights of around 1.4 in average (so close to opt since maximum with 1.5 would be 0.66)\n",
    "\n",
    "Normalized gets 0.34 with max weight edges of around 1.55 in average, not very close to opt, surprising that it is much worse than softmax\n",
    "\n",
    "Min with softmax does much better, gets 0.86 with an average outgoing edge weight of 0.86 (opt should be 1) then divided by max outgoing but basically no difference in result.\n",
    "\n",
    "Min with normalized does not great, get 0.47 though average outgoing edge weight similar at around 0.85, seems like the predictions are plainly just worse for some reason.\n",
    "\n",
    "My intuition for why it's worse in normalized: the model is trained to find the best and put the others to 0 after softmax and the thing about softmax is that, even if the others have predicted weight close to the max, they'll be pushed much more towards 0 (thus is less of a problem and aren't pushed more toward 0). Doing without softmax, then, removes this maximum bias and thus the other, less good predictions have higher weight than they'd have under softmax.\n",
    "\n",
    "Demonstrated by the fact that min with argmax instead of softmax (i.e. basically greedy but worse) does 0.93"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def matching_value(samples, predictions, partial = False, match_rest = False):\n",
    "    features = samples.features\n",
    "    gt_matchings = samples.outputs[0].data\n",
    "    # inputs for the matrix A are at index 1 (see spec.py)\n",
    "    data = features.inputs[1].data\n",
    "    masks = features.inputs[3].data\n",
    "    pred_acc_greedy = 0\n",
    "    pred_acc_softmax = 0\n",
    "    pred_acc_normalized = 0\n",
    "    pred_acc_min = 0\n",
    "\n",
    "    # Iterating over all the samples\n",
    "    for i in range(data.shape[0]):\n",
    "        max_weight = compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n",
    "\n",
    "        predicted_matching = predictions[\"owners\"].data[i]\n",
    "\n",
    "        if partial:\n",
    "            preds_weight_softmax = compute_partial_matching_weight_softmax(i, data, masks, predicted_matching)\n",
    "            preds_weight_normalized = compute_partial_matching_weight_normalized(i, data, masks, predicted_matching)\n",
    "            preds_weight_min = compute_partial_matching_weight_min_edges(i, data, masks, predicted_matching)\n",
    "            print(f\"opt: {max_weight}, partial softmax: {preds_weight_softmax}, partial normalized: {preds_weight_normalized}, partial minimum: {preds_weight_min}\")\n",
    "            assert preds_weight_softmax <= max_weight\n",
    "            assert preds_weight_normalized <= max_weight\n",
    "            assert preds_weight_min <= max_weight\n",
    "            pred_acc_softmax += preds_weight_softmax / max_weight\n",
    "            pred_acc_normalized += preds_weight_normalized / max_weight\n",
    "            pred_acc_min += preds_weight_min / max_weight\n",
    "        else:\n",
    "            preds_weight_greedy = compute_greedy_matching_weight(i, data, masks, predicted_matching, match_rest = match_rest)\n",
    "            print(f\"opt: {max_weight}, partial: {preds_weight_greedy}\")\n",
    "\n",
    "            assert preds_weight_greedy <= max_weight\n",
    "            pred_acc_greedy += preds_weight_greedy / max_weight\n",
    "\n",
    "    print(f\"--------------------\")\n",
    "    if partial:\n",
    "        pred_acc_softmax /= data.shape[0]\n",
    "        pred_acc_normalized /= data.shape[0]\n",
    "        pred_acc_min /= data.shape[0]\n",
    "        print(f\"average accuracy: softmax {pred_acc_softmax:.4f}, normalized {pred_acc_normalized:.4f}, min: {pred_acc_min:.4f}\")\n",
    "    else:\n",
    "        pred_acc_greedy /= data.shape[0]\n",
    "        print(f\"average accuracy: greedy {pred_acc_greedy}\")\n",
    "\n",
    "def compute_greedy_matching_weight(i, data, masks, matching, match_rest = False):\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "\n",
    "\n",
    "    # Only consider the matching values for consumers\n",
    "    matching = np.where(goods_mask == 1, matching, -1)\n",
    "    unmatched_goods = set(range(n, n+m))\n",
    "    unmatched_buyers = set(range(n))\n",
    "\n",
    "    for buyer in range(n):\n",
    "        if buyer in matching:\n",
    "            # If several goods point to the same buyer, keep the one with maximum weight\n",
    "            mask = matching == buyer\n",
    "            matching_weight += np.max(A[buyer, mask])\n",
    "            # Recovering the index of the maximum, inspired by http://seanlaw.github.io/2015/09/10/numpy-argmin-with-a-condition/\n",
    "            subset_idx = np.argmax(A[buyer, mask])\n",
    "            good = np.arange(A.shape[1])[mask][subset_idx]\n",
    "            unmatched_goods.remove(good)\n",
    "            unmatched_buyers.remove(buyer)\n",
    "\n",
    "    if match_rest and len(unmatched_goods) > 0 and len(unmatched_buyers) > 0:\n",
    "        # Compute optimal matching on the remaining unmatched nodes\n",
    "        mask = np.zeros(A.shape)\n",
    "        # TODO this is a horrible solution, there's definitely a prettier solution\n",
    "        mask[list(unmatched_buyers)] += 1\n",
    "        mask[:, list(unmatched_goods)] += 1\n",
    "        mask = np.where(mask == 2, True, False)\n",
    "        remaining_bipartite_graph = A * mask\n",
    "        row_ind, col_ind = linear_sum_assignment(remaining_bipartite_graph, maximize = True)\n",
    "        opt = A[row_ind, col_ind].sum() / 2 #TODO do I always need the division by 2\n",
    "        matching_weight += opt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return matching_weight\n",
    "\n",
    "def compute_partial_matching_weight_softmax(i, data, masks, matching):\n",
    "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
    "\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # We only care about the buyer -> good connections\n",
    "    A_submatrix = A[:n, n:n+m]\n",
    "    matching = matching[:n, n:n+m]\n",
    "\n",
    "    max_weight_edge = np.max(np.sum(matching, axis = 0))\n",
    "    print(f\"max weight edge softmax: {max_weight_edge}\")\n",
    "    matching /= max_weight_edge\n",
    "    return np.sum(matching * A_submatrix)\n",
    "\n",
    "def compute_partial_matching_weight_normalized(i, data, masks, matching):\n",
    "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
    "    # Has already been softmaxed => we turn it back into log values\n",
    "\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "\n",
    "    # We only care about the buyer -> good connections\n",
    "    A_submatrix = A[:n, n:n+m]\n",
    "    matching = matching[:n, n:n+m]\n",
    "\n",
    "    matching = np.log(matching)\n",
    "    matching -= np.min(matching, axis = 1).reshape((-1, 1)) #TODO test out with +1\n",
    "    matching += 0\n",
    "    matching /= np.sum(matching, axis = 1).reshape((-1, 1))\n",
    "\n",
    "\n",
    "    max_weight_edge = np.max(np.sum(matching, axis = 0))\n",
    "    print(f\"max weight edge normalized: {max_weight_edge}\")\n",
    "    matching /= max_weight_edge\n",
    "    return np.sum(matching * A_submatrix)\n",
    "\n",
    "\n",
    "def compute_partial_matching_weight_min_edges(i, data, masks, matching):\n",
    "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
    "\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "\n",
    "    # For testing argmax instead of softmax\n",
    "    # new_matching = np.zeros(matching.shape)\n",
    "    # print(matching)\n",
    "    # new_matching[np.arange(n+m), np.argmax(matching, axis = 1)] = 1\n",
    "    # matching = new_matching\n",
    "    # print(matching)\n",
    "    # print(np.sum(matching, axis = 1))\n",
    "\n",
    "\n",
    "    # We only care about the buyer -> good connections\n",
    "    A_submatrix = A[:n, n:n+m]\n",
    "    matching_1 = matching[:n, n:n+m]\n",
    "    matching_2 = matching[n:n+m, :n].T\n",
    "\n",
    "    matching = np.minimum(matching_1, matching_2)\n",
    "    matching *= 1/np.max(np.sum(matching, axis = 1))\n",
    "\n",
    "    print(f\"average outgoing edge weight: {np.mean(np.sum(matching, axis = 1))}\")\n",
    "\n",
    "    return np.sum(matching * A_submatrix)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "test_feedback = next(test_sampler)\n",
    "predictions, _ = model.predict(rng_key, test_feedback.features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max weight edge softmax: 1.283907413482666\n",
      "max weight edge normalized: 1.6004301309585571\n",
      "average outgoing edge weight: 0.8716641664505005\n",
      "opt: 24.122013578126086, partial softmax: 18.43136978149414, partial normalized: 8.58033682688231, partial minimum: 21.37261755190633\n",
      "max weight edge softmax: 1.6178152561187744\n",
      "max weight edge normalized: 1.71533203125\n",
      "average outgoing edge weight: 0.8057221174240112\n",
      "opt: 24.27531631767341, partial softmax: 14.904389381408691, partial normalized: 8.065111701376873, partial minimum: 20.15252750675249\n",
      "max weight edge softmax: 1.3881453275680542\n",
      "max weight edge normalized: 1.7349904775619507\n",
      "average outgoing edge weight: 0.8779746294021606\n",
      "opt: 24.09069655259647, partial softmax: 17.274770736694336, partial normalized: 7.91917318669128, partial minimum: 21.651993481193635\n",
      "max weight edge softmax: 1.4463163614273071\n",
      "max weight edge normalized: 1.3664538860321045\n",
      "average outgoing edge weight: 0.8771687746047974\n",
      "opt: 23.77244764558711, partial softmax: 16.153493881225586, partial normalized: 10.047829061809844, partial minimum: 20.937908983764665\n",
      "max weight edge softmax: 2.426001787185669\n",
      "max weight edge normalized: 2.8307278156280518\n",
      "average outgoing edge weight: 0.8261968493461609\n",
      "opt: 22.58315684702401, partial softmax: 8.996990203857422, partial normalized: 4.692915695075314, partial minimum: 19.438266477787217\n",
      "max weight edge softmax: 1.3476433753967285\n",
      "max weight edge normalized: 1.8378883600234985\n",
      "average outgoing edge weight: 0.8651484847068787\n",
      "opt: 23.19969377595835, partial softmax: 17.053619384765625, partial normalized: 7.556778108613528, partial minimum: 20.83467659367721\n",
      "max weight edge softmax: 2.5439062118530273\n",
      "max weight edge normalized: 2.9664857387542725\n",
      "average outgoing edge weight: 0.8667458891868591\n",
      "opt: 23.326193414555355, partial softmax: 8.77724552154541, partial normalized: 4.5108565305409964, partial minimum: 20.322235585994566\n",
      "max weight edge softmax: 1.2393625974655151\n",
      "max weight edge normalized: 1.460651159286499\n",
      "average outgoing edge weight: 0.8388522267341614\n",
      "opt: 22.171563916414392, partial softmax: 17.716516494750977, partial normalized: 9.10662502201641, partial minimum: 19.17907837763579\n",
      "max weight edge softmax: 1.7297312021255493\n",
      "max weight edge normalized: 1.6883906126022339\n",
      "average outgoing edge weight: 0.7749303579330444\n",
      "opt: 22.40102018006173, partial softmax: 13.02696704864502, partial normalized: 7.715400775622859, partial minimum: 18.60373637757595\n",
      "max weight edge softmax: 2.737633228302002\n",
      "max weight edge normalized: 2.6824889183044434\n",
      "average outgoing edge weight: 0.837411642074585\n",
      "opt: 23.675967644247542, partial softmax: 8.33029556274414, partial normalized: 4.850132499447978, partial minimum: 20.433308220217427\n",
      "max weight edge softmax: 1.3881453275680542\n",
      "max weight edge normalized: 1.7349904775619507\n",
      "average outgoing edge weight: 0.8779746294021606\n",
      "opt: 24.09069655259647, partial softmax: 17.274770736694336, partial normalized: 7.91917318669128, partial minimum: 21.651993481193635\n",
      "max weight edge softmax: 1.3394581079483032\n",
      "max weight edge normalized: 1.4050501585006714\n",
      "average outgoing edge weight: 0.8866263628005981\n",
      "opt: 23.694007552711078, partial softmax: 17.64253807067871, partial normalized: 9.72301520517543, partial minimum: 21.53518728614651\n",
      "max weight edge softmax: 1.2790802717208862\n",
      "max weight edge normalized: 1.3273845911026\n",
      "average outgoing edge weight: 0.8121833205223083\n",
      "opt: 22.660198089721156, partial softmax: 17.439353942871094, partial normalized: 10.030298343192591, partial minimum: 18.54750396946624\n",
      "max weight edge softmax: 1.6178152561187744\n",
      "max weight edge normalized: 1.71533203125\n",
      "average outgoing edge weight: 0.8057221174240112\n",
      "opt: 24.27531631767341, partial softmax: 14.904389381408691, partial normalized: 8.065111701376873, partial minimum: 20.15252750675249\n",
      "max weight edge softmax: 2.5439062118530273\n",
      "max weight edge normalized: 2.9664857387542725\n",
      "average outgoing edge weight: 0.8667458891868591\n",
      "opt: 23.326193414555355, partial softmax: 8.77724552154541, partial normalized: 4.5108565305409964, partial minimum: 20.322235585994566\n",
      "max weight edge softmax: 1.6178152561187744\n",
      "max weight edge normalized: 1.71533203125\n",
      "average outgoing edge weight: 0.8057221174240112\n",
      "opt: 24.27531631767341, partial softmax: 14.904389381408691, partial normalized: 8.065111701376873, partial minimum: 20.15252750675249\n",
      "max weight edge softmax: 1.3455690145492554\n",
      "max weight edge normalized: 1.5891504287719727\n",
      "average outgoing edge weight: 0.883449375629425\n",
      "opt: 25.004353177994123, partial softmax: 18.234291076660156, partial normalized: 9.156545917732728, partial minimum: 22.15623603327152\n",
      "max weight edge softmax: 2.426001787185669\n",
      "max weight edge normalized: 2.8307278156280518\n",
      "average outgoing edge weight: 0.8261968493461609\n",
      "opt: 22.58315684702401, partial softmax: 8.996990203857422, partial normalized: 4.692915695075314, partial minimum: 19.438266477787217\n",
      "max weight edge softmax: 1.7141039371490479\n",
      "max weight edge normalized: 1.45167076587677\n",
      "average outgoing edge weight: 0.8191860914230347\n",
      "opt: 22.332644393380995, partial softmax: 12.999723434448242, partial normalized: 9.235921597809387, partial minimum: 19.143258537939033\n",
      "max weight edge softmax: 1.3476433753967285\n",
      "max weight edge normalized: 1.8378883600234985\n",
      "average outgoing edge weight: 0.8651484847068787\n",
      "opt: 23.19969377595835, partial softmax: 17.053619384765625, partial normalized: 7.556778108613528, partial minimum: 20.83467659367721\n",
      "max weight edge softmax: 1.6178152561187744\n",
      "max weight edge normalized: 1.71533203125\n",
      "average outgoing edge weight: 0.8057221174240112\n",
      "opt: 24.27531631767341, partial softmax: 14.904389381408691, partial normalized: 8.065111701376873, partial minimum: 20.15252750675249\n",
      "max weight edge softmax: 1.3476433753967285\n",
      "max weight edge normalized: 1.8378883600234985\n",
      "average outgoing edge weight: 0.8651484847068787\n",
      "opt: 23.19969377595835, partial softmax: 17.053619384765625, partial normalized: 7.556778108613528, partial minimum: 20.83467659367721\n",
      "max weight edge softmax: 1.3069816827774048\n",
      "max weight edge normalized: 1.4129791259765625\n",
      "average outgoing edge weight: 0.8454497456550598\n",
      "opt: 23.697247705157636, partial softmax: 17.993854522705078, partial normalized: 9.563291143633553, partial minimum: 20.86093004525769\n",
      "max weight edge softmax: 1.181831955909729\n",
      "max weight edge normalized: 1.3998370170593262\n",
      "average outgoing edge weight: 0.8974531292915344\n",
      "opt: 24.253053382724158, partial softmax: 20.42726707458496, partial normalized: 10.062477863329422, partial minimum: 22.24883493909663\n",
      "max weight edge softmax: 1.3881453275680542\n",
      "max weight edge normalized: 1.7349904775619507\n",
      "average outgoing edge weight: 0.8779746294021606\n",
      "opt: 24.09069655259647, partial softmax: 17.274770736694336, partial normalized: 7.91917318669128, partial minimum: 21.651993481193635\n",
      "max weight edge softmax: 1.3644683361053467\n",
      "max weight edge normalized: 1.5899546146392822\n",
      "average outgoing edge weight: 0.845443844795227\n",
      "opt: 22.881268516584214, partial softmax: 16.471778869628906, partial normalized: 8.685021162036724, partial minimum: 19.714224493118305\n",
      "max weight edge softmax: 1.5916327238082886\n",
      "max weight edge normalized: 1.605776309967041\n",
      "average outgoing edge weight: 0.868392825126648\n",
      "opt: 24.30745199353397, partial softmax: 15.102516174316406, partial normalized: 8.512621180184652, partial minimum: 21.915133096670605\n",
      "max weight edge softmax: 1.2175774574279785\n",
      "max weight edge normalized: 1.6336767673492432\n",
      "average outgoing edge weight: 0.9206598997116089\n",
      "opt: 23.569833824851834, partial softmax: 19.231409072875977, partial normalized: 8.563468037854019, partial minimum: 22.00663767176416\n",
      "max weight edge softmax: 1.3394581079483032\n",
      "max weight edge normalized: 1.4050501585006714\n",
      "average outgoing edge weight: 0.8866263628005981\n",
      "opt: 23.694007552711078, partial softmax: 17.64253807067871, partial normalized: 9.72301520517543, partial minimum: 21.53518728614651\n",
      "max weight edge softmax: 1.7297312021255493\n",
      "max weight edge normalized: 1.6883906126022339\n",
      "average outgoing edge weight: 0.7749303579330444\n",
      "opt: 22.40102018006173, partial softmax: 13.02696704864502, partial normalized: 7.715400775622859, partial minimum: 18.60373637757595\n",
      "max weight edge softmax: 1.3196364641189575\n",
      "max weight edge normalized: 1.6469647884368896\n",
      "average outgoing edge weight: 0.8488115072250366\n",
      "opt: 23.753888237550665, partial softmax: 17.439607620239258, partial normalized: 8.275393514454269, partial minimum: 20.27780060590884\n",
      "max weight edge softmax: 1.3394581079483032\n",
      "max weight edge normalized: 1.4050501585006714\n",
      "average outgoing edge weight: 0.8866263628005981\n",
      "opt: 23.694007552711078, partial softmax: 17.64253807067871, partial normalized: 9.72301520517543, partial minimum: 21.53518728614651\n",
      "max weight edge softmax: 3.080259323120117\n",
      "max weight edge normalized: 2.8339478969573975\n",
      "average outgoing edge weight: 0.838350236415863\n",
      "opt: 24.026348665401954, partial softmax: 7.378108501434326, partial normalized: 4.916142563320219, partial minimum: 20.36259130477526\n",
      "max weight edge softmax: 1.7297312021255493\n",
      "max weight edge normalized: 1.6883906126022339\n",
      "average outgoing edge weight: 0.7749303579330444\n",
      "opt: 22.40102018006173, partial softmax: 13.02696704864502, partial normalized: 7.715400775622859, partial minimum: 18.60373637757595\n",
      "max weight edge softmax: 1.9792433977127075\n",
      "max weight edge normalized: 1.5085231065750122\n",
      "average outgoing edge weight: 0.8426050543785095\n",
      "opt: 24.315575084816228, partial softmax: 12.53480339050293, partial normalized: 9.285388281533974, partial minimum: 21.213167640926347\n",
      "max weight edge softmax: 2.5439062118530273\n",
      "max weight edge normalized: 2.9664857387542725\n",
      "average outgoing edge weight: 0.8667458891868591\n",
      "opt: 23.326193414555355, partial softmax: 8.77724552154541, partial normalized: 4.5108565305409964, partial minimum: 20.322235585994566\n",
      "max weight edge softmax: 2.426001787185669\n",
      "max weight edge normalized: 2.8307278156280518\n",
      "average outgoing edge weight: 0.8261968493461609\n",
      "opt: 22.58315684702401, partial softmax: 8.996990203857422, partial normalized: 4.692915695075314, partial minimum: 19.438266477787217\n",
      "max weight edge softmax: 1.3455690145492554\n",
      "max weight edge normalized: 1.5891504287719727\n",
      "average outgoing edge weight: 0.883449375629425\n",
      "opt: 25.004353177994123, partial softmax: 18.234291076660156, partial normalized: 9.156545917732728, partial minimum: 22.15623603327152\n",
      "max weight edge softmax: 2.5439062118530273\n",
      "max weight edge normalized: 2.9664857387542725\n",
      "average outgoing edge weight: 0.8667458891868591\n",
      "opt: 23.326193414555355, partial softmax: 8.77724552154541, partial normalized: 4.5108565305409964, partial minimum: 20.322235585994566\n",
      "max weight edge softmax: 1.3340028524398804\n",
      "max weight edge normalized: 1.5481022596359253\n",
      "average outgoing edge weight: 0.8429844975471497\n",
      "opt: 23.369927811079865, partial softmax: 17.31279182434082, partial normalized: 8.376983854217332, partial minimum: 20.34058005320498\n",
      "--------------------\n",
      "average accuracy: softmax 0.6238, normalized 0.3285, min: 0.8719\n"
     ]
    }
   ],
   "source": [
    "matching_value(test_feedback, predictions, partial = True, match_rest = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminary results\n",
    "random permutation/matching: 0.18\n",
    "\n",
    "MPNN:\n",
    "learned predictions: 0.67\n",
    "\n",
    "GAT:\n",
    "learned predictions: 0.72\n",
    "\n",
    "Got better with double ended predictions\n",
    "\n",
    "Partial: 0.64 while greedy was doing about 0.92 on the same instance. Main reason seems to be that max weight is around 1.5 => can get at most 2/3 OPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting the number of matching constraints violated"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 32.0\n"
     ]
    }
   ],
   "source": [
    "# For two-way\n",
    "count = 0\n",
    "data = predictions[\"owners\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    for i in range(32):\n",
    "        owner = data[datapoint][i]\n",
    "        good = data[datapoint][int(owner)]\n",
    "        if good != i:\n",
    "            count += 1\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 27.0\n"
     ]
    }
   ],
   "source": [
    "# For self-loops\n",
    "count = 0\n",
    "data = predictions[\"owners\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    owners = set(np.array(data[datapoint][32:64]))\n",
    "    count += 32 - len(owners)\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repred: True\n",
      "HINTS FROM NETS\n",
      "[DataPoint(name=\"owners_h\",\tlocation=node,\ttype=pointer,\tdata=Array(220, 1, 64)), DataPoint(name=\"p\",\tlocation=node,\ttype=scalar,\tdata=Array(220, 1, 64)), DataPoint(name=\"in_queue\",\tlocation=node,\ttype=mask,\tdata=Array(220, 1, 64))]\n"
     ]
    }
   ],
   "source": [
    "predictions, hints = model.predict(rng_key, test_feedback.features, return_hints = True, return_all_outputs = True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (118648624.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[15], line 3\u001B[0;36m\u001B[0m\n\u001B[0;31m    \u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(hints)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "arr[np.arange(len(arr)) % 2 == 0]\n",
    "arr[::2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_feedback.features.inputs[1].data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 2])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1,2],[3,4]])\n",
    "np.min(arr, axis = 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
