{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import clrs\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import pprint\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2**32))"
   ],
   "metadata": {
    "id": "2MzxRB1X7hRs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eee4d47e-b689-497b-e9b5-021121cac2b7"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_sampler, spec = clrs.build_sampler(\n",
    "    name='auction_matching',\n",
    "    num_samples=100,\n",
    "    length=16,\n",
    "    weighted=True) # number of nodes\n",
    "\n",
    "test_sampler, spec = clrs.build_sampler(\n",
    "    name='auction_matching',\n",
    "    num_samples=40, # TODO set back to more\n",
    "    length=64,\n",
    "    weighted=True) # testing on much larger\n",
    "# TODO how do you know aren't generating same graphs? (well not possible here since different size but in general?)\n",
    "\n",
    "pprint.pprint(spec) # spec is the algorithm specification, all the probes\n",
    "\n",
    "def _iterate_sampler(sampler, batch_size):\n",
    "  while True:\n",
    "    yield sampler.next(batch_size)\n",
    "\n",
    "# TODO put back normal batch values\n",
    "train_sampler = _iterate_sampler(train_sampler, batch_size=32)\n",
    "test_sampler = _iterate_sampler(test_sampler, batch_size=40) # full batch for the test set\n",
    "\n"
   ],
   "metadata": {
    "id": "OEo_Gj1j3Z6M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ef53c51-9bc2-4a49-b4cf-592a32dfa5b7"
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ('input', 'edge', 'scalar'),\n",
      " 'adj': ('input', 'edge', 'mask'),\n",
      " 'buyers': ('input', 'node', 'mask'),\n",
      " 'in_queue': ('hint', 'node', 'mask'),\n",
      " 'owners': ('output', 'node', 'pointer'),\n",
      " 'owners_h': ('hint', 'node', 'pointer'),\n",
      " 'p': ('hint', 'node', 'scalar'),\n",
      " 'pos': ('input', 'node', 'scalar')}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "processor_factory = clrs.get_processor_factory('mpnn', use_ln=True, nb_triplet_fts=0) #use_ln => use layer norm\n",
    "# processor_factory = clrs.get_processor_factory('gat', use_ln=True, nb_heads = 4, nb_triplet_fts = 0)\n",
    "model_params = dict(\n",
    "    processor_factory=processor_factory, # contains the processor_factory\n",
    "    hidden_dim=64, # TODO put back to 32 if no difference\n",
    "    encode_hints=True,\n",
    "    decode_hints=True,\n",
    "    #decode_diffs=False,\n",
    "    #hint_teacher_forcing_noise=1.0,\n",
    "    hint_teacher_forcing=1.0,\n",
    "    use_lstm=False,\n",
    "    learning_rate=0.001,\n",
    "    checkpoint_path='/tmp/checkpt',\n",
    "    freeze_processor=False, # Good for post step\n",
    "    dropout_prob=0.5,\n",
    "    # nb_msg_passing_steps=3,\n",
    ")\n",
    "\n",
    "dummy_trajectory = next(train_sampler) # jax needs a trajectory that is plausible looking to init\n",
    "\n",
    "model = clrs.models.BaselineModel(\n",
    "    spec=spec,\n",
    "    dummy_trajectory=dummy_trajectory,\n",
    "    **model_params\n",
    ")\n",
    "\n",
    "model.init(dummy_trajectory.features, 1234) # 1234 is a random seed"
   ],
   "metadata": {
    "id": "L-p0jOCq5sPV"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# No evaluation since we are postprocessing with soft: TO CHANGE -> baselines.py line 336 outs change hard to False\n",
    "# step = 0\n",
    "#\n",
    "# while step <= 1:\n",
    "#     feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "#     rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "#     cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "#     rng_key = new_rng_key\n",
    "#     if step % 10 == 0:\n",
    "#         print(step)\n",
    "#     step += 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "step = 0\n",
    "\n",
    "while step <= 100:\n",
    "  feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "  # TODO remove - testing if uses hints on tests\n",
    "  # shape = test_feedback.features.hints[0].data[0].shape\n",
    "  # test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\n",
    "\n",
    "  rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "  cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "  rng_key = new_rng_key\n",
    "  if step % 10 == 0:\n",
    "    predictions_val, _ = model.predict(rng_key, feedback.features)\n",
    "    out_val = clrs.evaluate(feedback.outputs, predictions_val)\n",
    "    predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "    out = clrs.evaluate(test_feedback.outputs, predictions)\n",
    "    print(f'step = {step} | loss = {cur_loss} | val_acc = {out_val[\"score\"]} | test_acc = {out[\"score\"]}') # here, val accuracy is actually training accuracy, not great but is example\n",
    "  step += 1\n",
    "model2 = copy.deepcopy(model)"
   ],
   "metadata": {
    "id": "3pSKQ2wi62Br",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1acd203-e256-4f39-e059-9c7e55942a87"
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 | loss = 0.7008389234542847 | val_acc = 0.810546875 | test_acc = 0.657421886920929\n",
      "step = 10 | loss = 0.6964547634124756 | val_acc = 0.724609375 | test_acc = 0.6617187857627869\n",
      "step = 20 | loss = 0.6573083996772766 | val_acc = 0.82421875 | test_acc = 0.559374988079071\n",
      "step = 30 | loss = 0.6298770308494568 | val_acc = 0.841796875 | test_acc = 0.643359363079071\n",
      "step = 40 | loss = 0.6153847575187683 | val_acc = 0.7734375 | test_acc = 0.612109363079071\n",
      "step = 50 | loss = 0.6017027497291565 | val_acc = 0.716796875 | test_acc = 0.673828125\n",
      "step = 60 | loss = 0.60247802734375 | val_acc = 0.6953125 | test_acc = 0.676562488079071\n",
      "step = 70 | loss = 0.5785822868347168 | val_acc = 0.767578125 | test_acc = 0.685546875\n",
      "step = 80 | loss = 0.575739860534668 | val_acc = 0.6875 | test_acc = 0.670703113079071\n",
      "step = 90 | loss = 0.5428829193115234 | val_acc = 0.7265625 | test_acc = 0.678515613079071\n",
      "step = 100 | loss = 0.5632157325744629 | val_acc = 0.697265625 | test_acc = 0.688281238079071\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some intermediate results\n",
    "ALL: 0.5 dropout\n",
    "\n",
    "MPNN 100 train, 40 test, 100 epochs, self-loops -> loss = 0.9931782484054565 | val_acc = 0.8515625 | test_acc = 0.7484375238418579 | accuracy = 0.65, average nb non-matched: 13.45\n",
    "\n",
    "MPNN 200 train, 40 test, 100 epochs, self-loops -> loss = 0.8129420280456543 | val_acc = 0.8125 | test_acc = 0.77734375 | accuracy = 0.72, average nb non-matched: 11.125\n",
    "\n",
    "MPNN 100 train, 40 test, 100 epochs, double links -> loss = 0.8689386248588562 | val_acc = 0.7109375 | test_acc = 0.42695313692092896 | accuracy =? NOTE: only started \"learning\" in the last epochs => trying more, interestingly has less loss than self-loops but less accuracy too\n",
    "\n",
    "MPNN 100 train, 40 test, 200 epochs, double links -> step = 100 | loss = 0.6802611351013184 | val_acc = 0.806640625 | test_acc = 0.681640625 | accuracy = 0.89, average nb non-matched: 5.65\n",
    "\n",
    "MPNN 300 train, 40 test, 400 epochs, double links -> loss = 0.5485531091690063 | val_acc = 0.775390625 | test_acc = 0.6910156607627869 | accuracy = 0.928, average nb non-matched: 4.075 Note: best test_acc 0.727, similar test_acc to 100 train 200 epochs but better accuracy + still does not converge on training accuracy though\n",
    "\n",
    "Diff: length 100 testing instead of 64\n",
    "MPNN 100 train, 40 test LENGTH 100, 200 epochs, double links -> loss = 0.6958761215209961 | val_acc = 0.787109375 | test_acc = 0.503250002861023 | accuracy = 0.759, average nb non-matched: 7.9/100\n",
    "\n",
    "\n",
    "#### Now with actually bipartite graph (no owner-owner / good-good edges)\n",
    "Doesn't really change results\n",
    "\n",
    "ALL with 0 dropout\n",
    "\n",
    "#### No hints\n",
    "0 dropout Can get up to 0.78 of OPT, average nb non-matched: 10.5/64\n",
    "\n",
    "\n",
    "#### Training with 64 hidden dimensions\n",
    "MPNN 100 train, 40 test, 0 dropout, double links -> Get to 90% acc in 30 iterations, 93% in 60\n",
    "GAT 100 train, 40 test, 0 dropout, double links -> Get to 0.78 in 30 iterations, 91% in 60, 92% in 100, 92% in 200\n",
    "\n",
    "Same with 0.5 dropout\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links -> Get to 85% in 30 iterations, 93% in 60 iterations, 93% in 100 iterations\n",
    "GAT 100 train, 40 test, 0.5 dropout, double links -> 94.7% in 200\n",
    "\n",
    "#### 3 message passing steps\n",
    "64 dims, 3 message passing steps\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links -> 91.7% in 100 iterations (worse than 1 MP step), 93% in 200 iterations\n",
    "\n",
    "Back to 1 message passing step\n",
    "\n",
    "#### Larger MLP\n",
    "[out_size, out_size, out_size] MLP\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links, 3 layer MLP -> loss = 0.6642395257949829 | val_acc = 0.75390625 | test_acc = 0.699999988079071 | 90% in 100 iterations (worse than smaller MLP) | 93% in 200 iterations\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def matching_value(samples, predictions, partial = False, match_rest = False):\n",
    "    features = samples.features\n",
    "    gt_matchings = samples.outputs[0].data\n",
    "    # inputs for the matrix A are at index 1 (see spec.py)\n",
    "    data = features.inputs[1].data\n",
    "    masks = features.inputs[3].data\n",
    "    pred_accuracy = 0\n",
    "\n",
    "    # Iterating over all the samples\n",
    "    for i in range(data.shape[0]):\n",
    "        max_weight = compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n",
    "\n",
    "        # TODO remove\n",
    "        predicted_matching = predictions[\"owners\"].data[i]\n",
    "        # buyers_mask = masks[i]\n",
    "        # n = int(np.sum(buyers_mask))\n",
    "        # permutation = np.random.permutation(np.arange(np.sum(buyers_mask == 0)))\n",
    "        # predicted_matching = np.concatenate((np.zeros(n), permutation))\n",
    "        if partial:\n",
    "            preds_weight = compute_partial_matching_weight(i, data, masks, predicted_matching)\n",
    "            print(f\"opt: {max_weight}, partial: {preds_weight}\")\n",
    "        else:\n",
    "            preds_weight = compute_greedy_matching_weight(i, data, masks, predicted_matching)\n",
    "            print(f\"opt: {max_weight}, partial: {preds_weight}\")\n",
    "\n",
    "        assert preds_weight <= max_weight\n",
    "        pred_accuracy += preds_weight / max_weight\n",
    "\n",
    "    return pred_accuracy / data.shape[0]\n",
    "\n",
    "def compute_greedy_matching_weight(i, data, masks, matching, match_rest = False):\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "\n",
    "\n",
    "\n",
    "    # Only consider the matching values for consumers\n",
    "    matching = np.where(goods_mask == 1, matching, -1)\n",
    "    matched = set(range(n))\n",
    "\n",
    "    for buyer in range(n):\n",
    "        if buyer in matching:\n",
    "            # If several goods point to the same buyer, keep the one with maximum weight\n",
    "            candidates = A[buyer, matching == buyer]\n",
    "            matching_weight += np.max(candidates)\n",
    "            matched.remove(np.argmax(A))\n",
    "\n",
    "    if match_rest:\n",
    "        # Compute optimal matching on the remaining unmatched nodes\n",
    "\n",
    "\n",
    "    return matching_weight\n",
    "\n",
    "def compute_partial_matching_weight(i, data, masks, matching):\n",
    "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
    "\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # We only care about the buyer -> good connections\n",
    "    A_submatrix = A[:n, n:n+m]\n",
    "    matching = matching[:n, n:n+m]\n",
    "\n",
    "    max_weight = np.max(np.sum(matching, axis = 0))\n",
    "    print(f\"max weight: {max_weight}\")\n",
    "    matching /= max_weight\n",
    "    return np.sum(matching * A_submatrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "test_feedback = next(test_sampler)\n",
    "predictions, _ = model.predict(rng_key, test_feedback.features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt: 23.673689819703025, partial: 22.90835044118225\n",
      "opt: 22.355036248740607, partial: 19.85250484380736\n",
      "opt: 24.06514990800616, partial: 23.46716671325547\n",
      "opt: 23.002062339038712, partial: 21.893111430972922\n",
      "opt: 22.857922652434382, partial: 22.07175429594781\n",
      "opt: 23.002062339038712, partial: 21.893111430972922\n",
      "opt: 23.58924307167064, partial: 22.260979406370744\n",
      "opt: 23.47159909352794, partial: 21.71489062990129\n",
      "opt: 23.840851586275427, partial: 22.231141258977125\n",
      "opt: 23.47159909352794, partial: 21.71489062990129\n",
      "opt: 22.786286058566244, partial: 20.9501284811016\n",
      "opt: 23.840851586275427, partial: 22.231141258977125\n",
      "opt: 23.840851586275427, partial: 22.231141258977125\n",
      "opt: 22.502009668820723, partial: 21.439062655425303\n",
      "opt: 24.06514990800616, partial: 23.46716671325547\n",
      "opt: 23.02162847884334, partial: 21.911761179076\n",
      "opt: 23.31666004059874, partial: 21.568141715608842\n",
      "opt: 23.588145361419752, partial: 21.524198989196062\n",
      "opt: 22.786286058566244, partial: 20.9501284811016\n",
      "opt: 23.470303433193745, partial: 21.229620161507256\n",
      "opt: 23.696539577418662, partial: 21.529992516794486\n",
      "opt: 22.73590404715686, partial: 21.436687007924156\n",
      "opt: 23.557541670248117, partial: 22.00124850811864\n",
      "opt: 24.160106434618413, partial: 22.503600723411225\n",
      "opt: 23.110274428450815, partial: 21.030798663102704\n",
      "opt: 22.73590404715686, partial: 21.436687007924156\n",
      "opt: 24.06514990800616, partial: 23.46716671325547\n",
      "opt: 23.96404249483499, partial: 22.49613783865412\n",
      "opt: 23.810733976849164, partial: 21.918194372657823\n",
      "opt: 23.673689819703025, partial: 22.90835044118225\n",
      "opt: 22.06659759051142, partial: 19.822031885380163\n",
      "opt: 23.110274428450815, partial: 21.030798663102704\n",
      "opt: 23.02162847884334, partial: 21.911761179076\n",
      "opt: 23.047214327571815, partial: 21.451486241158527\n",
      "opt: 23.30590692700588, partial: 21.10760658618489\n",
      "opt: 22.73590404715686, partial: 21.436687007924156\n",
      "opt: 23.02162847884334, partial: 21.911761179076\n",
      "opt: 23.557541670248117, partial: 22.00124850811864\n",
      "opt: 24.19968997326353, partial: 22.40798004461076\n",
      "opt: 24.6363759090799, partial: 21.969159173512118\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9341666194451476"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_value(test_feedback, predictions, partial = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminary results\n",
    "random permutation/matching: 0.18\n",
    "\n",
    "MPNN:\n",
    "learned predictions: 0.67\n",
    "\n",
    "GAT:\n",
    "learned predictions: 0.72\n",
    "\n",
    "Got better with double ended predictions\n",
    "\n",
    "Partial: 0.64 while greedy was doing about 0.92 on the same instance. Main reason seems to be that max weight is around 1.5 => can get at most 2/3 OPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting the number of matching constraints violated"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 32.0\n"
     ]
    }
   ],
   "source": [
    "# For two-way\n",
    "count = 0\n",
    "data = predictions[\"owners\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    for i in range(32):\n",
    "        owner = data[datapoint][i]\n",
    "        good = data[datapoint][int(owner)]\n",
    "        if good != i:\n",
    "            count += 1\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 27.0\n"
     ]
    }
   ],
   "source": [
    "# For self-loops\n",
    "count = 0\n",
    "data = predictions[\"owners\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    owners = set(np.array(data[datapoint][32:64]))\n",
    "    count += 32 - len(owners)\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repred: True\n",
      "HINTS FROM NETS\n",
      "[DataPoint(name=\"owners_h\",\tlocation=node,\ttype=pointer,\tdata=Array(220, 1, 64)), DataPoint(name=\"p\",\tlocation=node,\ttype=scalar,\tdata=Array(220, 1, 64)), DataPoint(name=\"in_queue\",\tlocation=node,\ttype=mask,\tdata=Array(220, 1, 64))]\n"
     ]
    }
   ],
   "source": [
    "predictions, hints = model.predict(rng_key, test_feedback.features, return_hints = True, return_all_outputs = True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 1, 64)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['owners'].data[::100].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (118648624.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[15], line 3\u001B[0;36m\u001B[0m\n\u001B[0;31m    \u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "np.argmax(hints[10]['owners_h'][0], axis =\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(hints)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "arr[np.arange(len(arr)) % 2 == 0]\n",
    "arr[::2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_feedback.features.inputs[1].data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arr = np.array([[1,2],[2,3]])\n",
    "np.max(arr, axis = 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
