{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MzxRB1X7hRs",
    "outputId": "eee4d47e-b689-497b-e9b5-021121cac2b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 22:06:08.845269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/anders/miniconda3/envs/clrs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import clrs\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import pprint\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2**32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEo_Gj1j3Z6M",
    "outputId": "5ef53c51-9bc2-4a49-b4cf-592a32dfa5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ('input', 'edge', 'scalar'),\n",
      " 'L': ('input', 'node', 'mask'),\n",
      " 'M_h': ('hint', 'edge', 'scalar'),\n",
      " 'match': ('output', 'node', 'pointer'),\n",
      " 'match_h': ('hint', 'node', 'pointer'),\n",
      " 'pos': ('input', 'node', 'scalar')}\n"
     ]
    }
   ],
   "source": [
    "# =================== BIPARTITE GRAPH GENERATOR SPECS =========================\n",
    "# - Erdos Reyni (ER)\n",
    "#     Generates an Erdos-Reyni random graph with edge probability [p]. If\n",
    "#       [weighted], edge weights are iid uniform([low], [high])\n",
    "#\n",
    "#     {'generator': ER, 'weighted': False, 'p': 0.5, 'low': 0.0, 'high': 1.0}\n",
    "#\n",
    "# - Barabasi-Albert (BA)\n",
    "#     Generates a Barabasi-Albert random graph with parameter [ba_param]. If\n",
    "#       [weighted], edge weights are iid uniform([low], [high])\n",
    "#\n",
    "#     {'generator': BA, 'ba_param': 1, 'weighted': False, 'low': 0.0, 'high': 1.0}\n",
    "#\n",
    "# - Geometric (GEOMETRIC)\n",
    "#     Generates a random graph by embedding nodes uniformly over the unit\n",
    "#       square. Edge weights are the euclidean distance between two nodes,\n",
    "#       with weights below [threshold] set to 0, then scaled by [scaling].\n",
    "#\n",
    "#     {'generator': GEOMETRIC, 'threshold': 0.25, 'scaling': 1.0}\n",
    "#\n",
    "# - Flow (FLOW)\n",
    "#     Generates a random ER reduction to a max flow input, with edge\n",
    "#       probability [p].\n",
    "#     {'generator': FLOW, 'p': 0.5}\n",
    "\n",
    "\n",
    "\n",
    "train_sampler, spec = clrs.build_sampler(\n",
    "    name='simplified_min_sum',\n",
    "    num_samples=1,\n",
    "    length=5,\n",
    "    generator='BA',\n",
    "    ba_param=2,\n",
    "    weighted=True,\n",
    "    low=1.0,\n",
    "    high=2.0\n",
    "    )\n",
    "\n",
    "test_sampler, spec = clrs.build_sampler(\n",
    "    name='simplified_min_sum',\n",
    "    num_samples=1,\n",
    "    length=4,\n",
    "    weighted=True,\n",
    "    generator='ER')\n",
    "\n",
    "pprint.pprint(spec)\n",
    "\n",
    "def _iterate_sampler(sampler, batch_size):\n",
    "  while True:\n",
    "    yield sampler.next(batch_size)\n",
    "\n",
    "train_sampler = _iterate_sampler(train_sampler, batch_size=1)\n",
    "test_sampler = _iterate_sampler(test_sampler, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 1.30032796, 1.06683875],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 1.33751051, 0.        ],\n",
       "        [1.30032796, 0.        , 1.33751051, 0.        , 0.        ],\n",
       "        [1.06683875, 0.        , 0.        , 0.        , 0.        ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_sampler)[0][0][1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L-p0jOCq5sPV"
   },
   "outputs": [],
   "source": [
    "processor_factory = clrs.get_processor_factory('mpnn', use_ln=True, nb_triplet_fts=0) #use_ln => use layer norm\n",
    "# processor_factory = clrs.get_processor_factory('gat', use_ln=True, nb_heads = 4, nb_triplet_fts = 0)\n",
    "model_params = dict(\n",
    "    processor_factory=processor_factory, # contains the processor_factory\n",
    "    hidden_dim=64, # TODO put back to 32 if no difference\n",
    "    encode_hints=True,\n",
    "    decode_hints=True,\n",
    "    #decode_diffs=False,\n",
    "    #hint_teacher_forcing_noise=1.0,\n",
    "    hint_teacher_forcing=1.0,\n",
    "    use_lstm=False,\n",
    "    learning_rate=0.001,\n",
    "    checkpoint_path='/tmp/checkpt',\n",
    "    freeze_processor=False, # Good for post step\n",
    "    dropout_prob=0.5,\n",
    "    # nb_msg_passing_steps=3,\n",
    ")\n",
    "\n",
    "dummy_trajectory = next(train_sampler) # jax needs a trajectory that is plausible looking to init\n",
    "\n",
    "model = clrs.models.BaselineModel(\n",
    "    spec=spec,\n",
    "    dummy_trajectory=dummy_trajectory,\n",
    "    **model_params\n",
    ")\n",
    "\n",
    "model.init(dummy_trajectory.features, 1234) # 1234 is a random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pSKQ2wi62Br",
    "outputId": "b1acd203-e256-4f39-e059-9c7e55942a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 0 | loss = 6.914782524108887 | val_acc = 0.240234375 | test_acc = 0.15000000596046448\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 10 | loss = 2.262831211090088 | val_acc = 0.513671875 | test_acc = 0.38750001788139343\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 20 | loss = 1.125622034072876 | val_acc = 0.63671875 | test_acc = 0.44414064288139343\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 30 | loss = 0.7239480018615723 | val_acc = 0.69921875 | test_acc = 0.583984375\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 40 | loss = 0.5074636936187744 | val_acc = 0.654296875 | test_acc = 0.6363281607627869\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 50 | loss = 0.43849965929985046 | val_acc = 0.693359375 | test_acc = 0.581250011920929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# TODO remove - testing if uses hints on tests\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# shape = test_feedback.features.hints[0].data[0].shape\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m rng_key, new_rng_key \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(rng_key) \u001b[39m# jax needs new random seed at step\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m cur_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfeedback(rng_key, feedback) \u001b[39m# loss is contained in model somewhere\u001b[39;00m\n\u001b[1;32m     13\u001b[0m rng_key \u001b[39m=\u001b[39m new_rng_key\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/clrs/clrs/clrs/_src/baselines.py:371\u001b[0m, in \u001b[0;36mBaselineModel.feedback\u001b[0;34m(self, rng_key, feedback, algorithm_index)\u001b[0m\n\u001b[1;32m    369\u001b[0m rng_keys \u001b[39m=\u001b[39m _maybe_pmap_rng_key(rng_key)  \u001b[39m# pytype: disable=wrong-arg-types  # numpy-scalars\u001b[39;00m\n\u001b[1;32m    370\u001b[0m feedback \u001b[39m=\u001b[39m _maybe_pmap_data(feedback)\n\u001b[0;32m--> 371\u001b[0m loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_params, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_opt_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjitted_feedback(\n\u001b[1;32m    372\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device_params, rng_keys, feedback,\n\u001b[1;32m    373\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device_opt_state, algorithm_index)\n\u001b[1;32m    374\u001b[0m loss \u001b[39m=\u001b[39m _maybe_pick_first_pmapped(loss)\n\u001b[1;32m    375\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "step = 0\n",
    "\n",
    "while step <= 100:\n",
    "  feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "  # TODO remove - testing if uses hints on tests\n",
    "  # shape = test_feedback.features.hints[0].data[0].shape\n",
    "  # test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\n",
    "\n",
    "  rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "  cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "  rng_key = new_rng_key\n",
    "  if step % 10 == 0:\n",
    "    predictions_val, _ = model.predict(rng_key, feedback.features)\n",
    "    out_val = clrs.evaluate(feedback.outputs, predictions_val)\n",
    "    predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "    out = clrs.evaluate(test_feedback.outputs, predictions)\n",
    "    print(predictions)\n",
    "    print(f'step = {step} | loss = {cur_loss} | val_acc = {out_val[\"score\"]} | test_acc = {out[\"score\"]}') # here, val accuracy is actually training accuracy, not great but is example\n",
    "  step += 1\n",
    "model2 = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Some intermediate results\n",
    "ALL: 0.5 dropout\n",
    "\n",
    "MPNN 100 train, 40 test, 100 epochs, self-loops -> loss = 0.9931782484054565 | val_acc = 0.8515625 | test_acc = 0.7484375238418579 | accuracy = 0.65, average nb non-matched: 13.45\n",
    "\n",
    "MPNN 200 train, 40 test, 100 epochs, self-loops -> loss = 0.8129420280456543 | val_acc = 0.8125 | test_acc = 0.77734375 | accuracy = 0.72, average nb non-matched: 11.125\n",
    "\n",
    "MPNN 100 train, 40 test, 100 epochs, double links -> loss = 0.8689386248588562 | val_acc = 0.7109375 | test_acc = 0.42695313692092896 | accuracy =? NOTE: only started \"learning\" in the last epochs => trying more, interestingly has less loss than self-loops but less accuracy too\n",
    "\n",
    "MPNN 100 train, 40 test, 200 epochs, double links -> step = 100 | loss = 0.6802611351013184 | val_acc = 0.806640625 | test_acc = 0.681640625 | accuracy = 0.89, average nb non-matched: 5.65\n",
    "\n",
    "MPNN 300 train, 40 test, 400 epochs, double links -> loss = 0.5485531091690063 | val_acc = 0.775390625 | test_acc = 0.6910156607627869 | accuracy = 0.928, average nb non-matched: 4.075 Note: best test_acc 0.727, similar test_acc to 100 train 200 epochs but better accuracy + still does not converge on training accuracy though\n",
    "\n",
    "Diff: length 100 testing instead of 64\n",
    "MPNN 100 train, 40 test LENGTH 100, 200 epochs, double links -> loss = 0.6958761215209961 | val_acc = 0.787109375 | test_acc = 0.503250002861023 | accuracy = 0.759, average nb non-matched: 7.9/100\n",
    "\n",
    "\n",
    "#### Now with actually bipartite graph (no owner-owner / good-good edges)\n",
    "Doesn't really change results\n",
    "\n",
    "ALL with 0 dropout\n",
    "\n",
    "#### No hints\n",
    "0 dropout Can get up to 0.78 of OPT, average nb non-matched: 10.5/64\n",
    "\n",
    "\n",
    "#### Training with 64 hidden dimensions\n",
    "MPNN 100 train, 40 test, 0 dropout, double links -> Get to 90% acc in 30 iterations, 93% in 60\n",
    "GAT 100 train, 40 test, 0 dropout, double links -> Get to 0.78 in 30 iterations, 91% in 60, 92% in 100, 92% in 200\n",
    "\n",
    "Same with 0.5 dropout\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links -> Get to 85% in 30 iterations, 93% in 60 iterations, 93% in 100 iterations\n",
    "GAT 100 train, 40 test, 0.5 dropout, double links -> 94.7% in 200\n",
    "\n",
    "#### 3 message passing steps\n",
    "64 dims, 3 message passing steps\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links -> 91.7% in 100 iterations (worse than 1 MP step), 93% in 200 iterations\n",
    "\n",
    "Back to 1 message passing step\n",
    "\n",
    "#### Larger MLP\n",
    "[out_size, out_size, out_size] MLP\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links, 3 layer MLP -> loss = 0.6642395257949829 | val_acc = 0.75390625 | test_acc = 0.699999988079071 | 90% in 100 iterations (worse than smaller MLP) | 93% in 200 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def matching_value(samples, predictions, partial = False, match_rest = False):\n",
    "    features = samples.features\n",
    "    gt_matchings = samples.outputs[0].data\n",
    "    # inputs for the matrix A are at index 1 (see spec.py)\n",
    "    data = features.inputs[1].data\n",
    "    masks = features.inputs[2].data\n",
    "    pred_accuracy = 0\n",
    "\n",
    "    # Iterating over all the samples\n",
    "    for i in range(data.shape[0]):\n",
    "        max_weight = compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n",
    "\n",
    "        # TODO remove\n",
    "        predicted_matching = predictions[\"match\"].data[i]\n",
    "        # buyers_mask = masks[i]\n",
    "        # n = int(np.sum(buyers_mask))\n",
    "        # permutation = np.random.permutation(np.arange(np.sum(buyers_mask == 0)))\n",
    "        # predicted_matching = np.concatenate((np.zeros(n), permutation))\n",
    "        if partial:\n",
    "            preds_weight = compute_partial_matching_weight(i, data, masks, predicted_matching)\n",
    "            print(f\"opt: {max_weight}, partial: {preds_weight}\")\n",
    "        else:\n",
    "            preds_weight = compute_greedy_matching_weight(i, data, masks, predicted_matching)\n",
    "            print(f\"opt: {max_weight}, partial: {preds_weight}\")\n",
    "\n",
    "        assert preds_weight <= max_weight\n",
    "        pred_accuracy += preds_weight / max_weight\n",
    "\n",
    "    return pred_accuracy / data.shape[0]\n",
    "\n",
    "def compute_greedy_matching_weight(i, data, masks, matching, match_rest = False):\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "\n",
    "\n",
    "\n",
    "    # Only consider the matching values for consumers\n",
    "    matching = np.where(goods_mask == 1, matching, -1)\n",
    "    matched = set(range(n))\n",
    "\n",
    "    for buyer in range(n):\n",
    "        if buyer in matching:\n",
    "            # If several goods point to the same buyer, keep the one with maximum weight\n",
    "            candidates = A[buyer, matching == buyer]\n",
    "            matching_weight += np.max(candidates)\n",
    "            matched.remove(np.argmax(A))\n",
    "\n",
    "\n",
    "    return matching_weight\n",
    "\n",
    "def compute_partial_matching_weight(i, data, masks, matching):\n",
    "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
    "\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # We only care about the buyer -> good connections\n",
    "    A_submatrix = A[:n, n:n+m]\n",
    "    matching = matching[:n, n:n+m]\n",
    "\n",
    "    max_weight = np.max(np.sum(matching, axis = 0))\n",
    "    print(f\"max weight: {max_weight}\")\n",
    "    matching /= max_weight\n",
    "    return np.sum(matching * A_submatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feedback = next(test_sampler)\n",
    "predictions, _ = model.predict(rng_key, test_feedback.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  3.,  4., 12., 28., 25.,  3.,  0.,\n",
       "        3.,  3., 12.,  3.,  5., 22.,  0., 17.,  0.,  2., 22., 13.,  9.,\n",
       "       11.,  7.,  3.,  0., 11.,  5., 11., 17., 12.,  2.,  0., 13.],      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['match'].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45, 63, 61, 42, 33, 43, 47, 53, 37, -1, 57, 58, -1, -1, 36, 62, -1,\n",
       "       40, -1, -1, -1, 38, -1, 32, -1, -1, 44, 46, 35, -1, 59, 56, 23,  4,\n",
       "       -1, 28, 14,  8, 21, -1, 17, -1,  3,  5, 26,  0, 27,  6, -1, -1, -1,\n",
       "       -1, -1,  7, -1, -1, 31, 10, 11, 30, -1,  2, 15,  1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feedback.outputs[0].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1004",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matching_value(test_feedback, predictions, partial \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mmatching_value\u001b[0;34m(samples, predictions, partial, match_rest)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Iterating over all the samples\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m---> 11\u001b[0m     max_weight \u001b[39m=\u001b[39m compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n\u001b[1;32m     13\u001b[0m     \u001b[39m# TODO remove\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     predicted_matching \u001b[39m=\u001b[39m predictions[\u001b[39m\"\u001b[39m\u001b[39mmatch\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mdata[i]\n",
      "Cell \u001b[0;32mIn[17], line 49\u001b[0m, in \u001b[0;36mcompute_greedy_matching_weight\u001b[0;34m(i, data, masks, matching, match_rest)\u001b[0m\n\u001b[1;32m     47\u001b[0m         candidates \u001b[39m=\u001b[39m A[buyer, matching \u001b[39m==\u001b[39m buyer]\n\u001b[1;32m     48\u001b[0m         matching_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(candidates)\n\u001b[0;32m---> 49\u001b[0m         matched\u001b[39m.\u001b[39;49mremove(np\u001b[39m.\u001b[39;49margmax(A))\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m matching_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: 1004"
     ]
    }
   ],
   "source": [
    "matching_value(test_feedback, predictions, partial = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preliminary results\n",
    "random permutation/matching: 0.18\n",
    "\n",
    "MPNN:\n",
    "learned predictions: 0.67\n",
    "\n",
    "GAT:\n",
    "learned predictions: 0.72\n",
    "\n",
    "Got better with double ended predictions\n",
    "\n",
    "Partial: 0.64 while greedy was doing about 0.92 on the same instance. Main reason seems to be that max weight is around 1.5 => can get at most 2/3 OPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Counting the number of matching constraints violated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 32.0\n"
     ]
    }
   ],
   "source": [
    "# For two-way\n",
    "count = 0\n",
    "data = predictions[\"owners\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    for i in range(32):\n",
    "        owner = data[datapoint][i]\n",
    "        good = data[datapoint][int(owner)]\n",
    "        if good != i:\n",
    "            count += 1\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 27.0\n"
     ]
    }
   ],
   "source": [
    "# For self-loops\n",
    "count = 0\n",
    "data = predictions[\"owners\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    owners = set(np.array(data[datapoint][32:64]))\n",
    "    count += 32 - len(owners)\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repred: True\n",
      "HINTS FROM NETS\n",
      "[DataPoint(name=\"owners_h\",\tlocation=node,\ttype=pointer,\tdata=Array(220, 1, 64)), DataPoint(name=\"p\",\tlocation=node,\ttype=scalar,\tdata=Array(220, 1, 64)), DataPoint(name=\"in_queue\",\tlocation=node,\ttype=mask,\tdata=Array(220, 1, 64))]\n"
     ]
    }
   ],
   "source": [
    "predictions, hints = model.predict(rng_key, test_feedback.features, return_hints = True, return_all_outputs = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['owners'].data[::100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (118648624.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "np.argmax(hints[10]['owners_h'][0], axis =\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "arr[np.arange(len(arr)) % 2 == 0]\n",
    "arr[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feedback.features.inputs[1].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([[1,2],[2,3]])\n",
    "np.max(arr, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
