{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 10:07:03.756387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import jax\n",
    "import clrs\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2 ** 32))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# If you don't want BipartiteMatching, just pass empty generator list and\n",
    "# length separately\n",
    "\n",
    "train_sampler_spec = {\n",
    "    'num_samples': 100,\n",
    "    'batch_size': 32,\n",
    "    'schematics': [\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 1,\n",
    "            'length': 16,\n",
    "            'kwargs': {'low': 0, 'high': 1, 'weighted': True}\n",
    "        },\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 0,\n",
    "            'length': 10,\n",
    "            'length_2': 50,\n",
    "            'kwargs': {'p': 0.1, 'low': 0, 'high': 1, 'weighted': True}\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "test_sampler_spec = {\n",
    "    'num_samples': 40,\n",
    "    'batch_size': 40,\n",
    "    'schematics': [\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 0,\n",
    "            'length': 100,\n",
    "            'kwargs': {'p': 0.05, 'low': 0, 'high': 1, 'weighted': True}\n",
    "        },\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 1,\n",
    "            'length': 64,\n",
    "            'kwargs': {'low': 0, 'high': 1, 'weighted': True}\n",
    "        },\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 0,\n",
    "            'length': 20,\n",
    "            'length_2': 200,\n",
    "            'kwargs': {'p': 0.2, 'low': 0, 'high': 1, 'weighted': True}\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "def samplers(sampler_spec, **kwargs):\n",
    "    batch_size = sampler_spec.get('batch_size', 1)\n",
    "    num_samples = sampler_spec['num_samples']\n",
    "    if batch_size > num_samples:\n",
    "        batch_size = num_samples\n",
    "\n",
    "    def _iterate_sampler(sampler, batch_size):\n",
    "        while True:\n",
    "            yield sampler.next(batch_size)\n",
    "\n",
    "    sampler, spec = clrs.build_sampler(\n",
    "        name = 'simplified_min_sum',\n",
    "        sampler_spec = sampler_spec,\n",
    "        **kwargs)  # number of nodes\n",
    "\n",
    "    sampler = _iterate_sampler(sampler, batch_size = batch_size)\n",
    "    return sampler, spec\n",
    "\n",
    "train_sampler, spec = samplers(train_sampler_spec)\n",
    "test_sampler, _ = samplers(test_sampler_spec)"
   ],
   "metadata": {
    "id": "OEo_Gj1j3Z6M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ef53c51-9bc2-4a49-b4cf-592a32dfa5b7"
   },
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clrs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 68\u001B[0m\n\u001B[1;32m     65\u001B[0m     sampler \u001B[38;5;241m=\u001B[39m _iterate_sampler(sampler, batch_size \u001B[38;5;241m=\u001B[39m batch_size)\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sampler, spec\n\u001B[0;32m---> 68\u001B[0m train_sampler, spec \u001B[38;5;241m=\u001B[39m \u001B[43msamplers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_sampler_spec\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m test_sampler, _ \u001B[38;5;241m=\u001B[39m samplers(test_sampler_spec)\n",
      "Cell \u001B[0;32mIn[1], line 60\u001B[0m, in \u001B[0;36msamplers\u001B[0;34m(sampler_spec, **kwargs)\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m sampler\u001B[38;5;241m.\u001B[39mnext(batch_size)\n\u001B[0;32m---> 60\u001B[0m sampler, spec \u001B[38;5;241m=\u001B[39m \u001B[43mclrs\u001B[49m\u001B[38;5;241m.\u001B[39mbuild_sampler(\n\u001B[1;32m     61\u001B[0m     name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msimplified_min_sum\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     62\u001B[0m     sampler_spec \u001B[38;5;241m=\u001B[39m sampler_spec,\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# number of nodes\u001B[39;00m\n\u001B[1;32m     65\u001B[0m sampler \u001B[38;5;241m=\u001B[39m _iterate_sampler(sampler, batch_size \u001B[38;5;241m=\u001B[39m batch_size)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sampler, spec\n",
      "\u001B[0;31mNameError\u001B[0m: name 'clrs' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(64, 64)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(test_sampler)\n",
    "sample.features.inputs[1].data[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def define_model(spec, train_sampler, model = \"mpnn\"):\n",
    "    if model == \"mpnn\":\n",
    "        processor_factory = clrs.get_processor_factory('mpnn', use_ln = True,\n",
    "                                                   nb_triplet_fts = 4)  #use_ln => use layer norm\n",
    "    elif model == \"gat\":\n",
    "        processor_factory = clrs.get_processor_factory('gat', use_ln=True, nb_heads = 4, nb_triplet_fts = 4)\n",
    "\n",
    "    elif model == \"mpnndoublemax\":\n",
    "        processor_factory = clrs.get_processor_factory('mpnndoublemax', use_ln = True, nb_triplet_fts = 0)  #use_ln => use layer norm\n",
    "\n",
    "    elif model == \"gmpnn\":\n",
    "        processor_factory = clrs.get_processor_factory('gmpnn', use_ln = True, nb_triplet_fts = 4)  #use_ln => use layer norm\n",
    "    elif model == \"pgn\":\n",
    "        processor_factory = clrs.get_processor_factory('pgn', use_ln = True, nb_triplet_fts = 32)  #use_ln => use layer norm\n",
    "    elif model == \"triplet_pgn_mask\":\n",
    "        processor_factory = clrs.get_processor_factory('triplet_pgn_mask', use_ln = True, nb_triplet_fts = 32)  #use_ln => use layer norm\n",
    "\n",
    "    model_params = dict(\n",
    "        processor_factory = processor_factory,  # contains the processor_factory\n",
    "        hidden_dim = 32,  # TODO put back to 32 if no difference\n",
    "        encode_hints = True,\n",
    "        decode_hints = True,\n",
    "        #decode_diffs=False,\n",
    "        #hint_teacher_forcing_noise=1.0,\n",
    "        hint_teacher_forcing = 1.0,\n",
    "        use_lstm = False,\n",
    "        learning_rate = 0.001,\n",
    "        checkpoint_path = '/tmp/checkpt',\n",
    "        freeze_processor = False,  # Good for post step\n",
    "        dropout_prob = 0.5,\n",
    "        # nb_msg_passing_steps=3,\n",
    "    )\n",
    "\n",
    "    dummy_trajectory = next(train_sampler)  # jax needs a trajectory that is plausible looking to init\n",
    "\n",
    "    model = clrs.models.BaselineModel(\n",
    "        spec = spec,\n",
    "        dummy_trajectory = dummy_trajectory,\n",
    "        **model_params\n",
    "    )\n",
    "\n",
    "    model.init(dummy_trajectory.features, 1234)  # 1234 is a random seed\n",
    "\n",
    "    return model\n",
    "\n",
    "model = define_model(spec, train_sampler, \"triplet_pgn_mask\")"
   ],
   "metadata": {
    "id": "L-p0jOCq5sPV"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# No evaluation since we are postprocessing with soft: TO CHANGE -> baselines.py line 336 outs change hard to False\n",
    "# step = 0\n",
    "#\n",
    "# while step <= 1:\n",
    "#     feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "#     rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "#     cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "#     rng_key = new_rng_key\n",
    "#     if step % 10 == 0:\n",
    "#         print(step)\n",
    "#     step += 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, epochs, train_sampler, test_sampler):\n",
    "    step = 0\n",
    "    rng_key = jax.random.PRNGKey(rng.randint(2 ** 32))\n",
    "\n",
    "    while step <= epochs:\n",
    "        feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "        # TODO remove - testing if uses hints on tests\n",
    "        # shape = test_feedback.features.hints[0].data[0].shape\n",
    "        # test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\n",
    "\n",
    "        rng_key, new_rng_key = jax.random.split(rng_key)  # jax needs new random seed at step\n",
    "        cur_loss = model.feedback(rng_key, feedback)  # loss is contained in model somewhere\n",
    "        rng_key = new_rng_key\n",
    "        if step % 50 == 0:\n",
    "            predictions_val, _ = model.predict(rng_key, feedback.features)\n",
    "            out_val = clrs.evaluate(feedback.outputs, predictions_val)\n",
    "            predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "            out = clrs.evaluate(test_feedback.outputs, predictions)\n",
    "            print(\n",
    "                f'step = {step} | loss = {cur_loss} | val_acc = {out_val[\"score\"]} | test_acc = {out[\"score\"]}')  # here, val accuracy is actually training accuracy, not great but is example\n",
    "        if step % 150 == 0:\n",
    "            learned, greedy = matching_value(test_feedback, predictions, partial = False, match_rest = False, opt_scipy = True)\n",
    "            print(f\"**learned: {learned}, greedy: {greedy}**\")\n",
    "        step += 1\n",
    "    return model"
   ],
   "metadata": {
    "id": "3pSKQ2wi62Br",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1acd203-e256-4f39-e059-9c7e55942a87"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def matching_value(samples, predictions, partial = False, match_rest = False, opt_scipy = False):\n",
    "    features = samples.features\n",
    "    gt_matchings = samples.outputs[0].data\n",
    "    # inputs for the matrix A are at index 1 (see spec.py)\n",
    "    data = features.inputs[1].data\n",
    "    masks = features.inputs[3].data\n",
    "    pred_accuracy = 0\n",
    "    greedy_accuracy = 0\n",
    "\n",
    "    #TODO remove\n",
    "    def _add_uniform_weights(adj, low, high):\n",
    "        n, m = adj.shape\n",
    "        weights = np.random.uniform(\n",
    "            low=low, high=high, size=(n, m)\n",
    "        )\n",
    "        return adj * high + low\n",
    "\n",
    "    # Iterating over all the samples\n",
    "    for i in range(data.shape[0]):\n",
    "\n",
    "        if opt_scipy:\n",
    "            row_ind, col_ind = linear_sum_assignment(data[i], maximize = True)\n",
    "            max_weight = data[i][row_ind, col_ind].sum() / 2  #TODO why /2\n",
    "        else:\n",
    "            max_weight = compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n",
    "\n",
    "        predicted_matching = predictions[\"match\"].data[i]\n",
    "\n",
    "        if partial:\n",
    "            preds_weight = compute_partial_matching_weight(i, data, masks, predicted_matching)\n",
    "            # print(f\"opt: {max_weight}, greedy learned: {preds_weight}\")\n",
    "        else:\n",
    "            preds_weight = compute_greedy_matching_weight(i, data, masks, predicted_matching, match_rest = match_rest)\n",
    "            # print(f\"opt: {max_weight}, partial: {preds_weight}\")\n",
    "\n",
    "        # assert preds_weight <= max_weight\n",
    "        greedy_matching_weight = naive_greedy(i, data, masks)\n",
    "        # print(f\"Naive greedy: {greedy_matching_weight}\")\n",
    "        greedy_accuracy += greedy_matching_weight / max_weight\n",
    "        pred_accuracy += preds_weight / max_weight\n",
    "\n",
    "    return pred_accuracy / data.shape[0], greedy_accuracy / data.shape[0]\n",
    "\n",
    "def naive_greedy(i, data, masks):\n",
    "    \"\"\"Computes a matching greedily by, for each node, adding the maximum neighbor that\n",
    "    hasn't yet been added to the matching\"\"\"\n",
    "\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    # At the start, all the right hand side values are possible matches\n",
    "    matching_mask = np.full(A.shape[0], True)\n",
    "\n",
    "    # for buyer in range(n):\n",
    "    #     # Checking if there are more elements to match (if more buyers than goods)\n",
    "    #     if A[buyer, matching_mask].shape[0] != 0:\n",
    "    #         matching_weight += np.max(A[buyer, matching_mask])\n",
    "    #         # Recovering the index of the maximum, inspired by http://seanlaw.github.io/2015/09/10/numpy-argmin-with-a-condition/\n",
    "    #         subset_idx = np.argmax(A[buyer, matching_mask])\n",
    "    #         good = np.arange(A.shape[1])[matching_mask][subset_idx]\n",
    "    #         # The corresponding good cannot be used anymore\n",
    "    #         matching_mask[good] = False\n",
    "\n",
    "    # Second method of computing a greedy matching\n",
    "    # Set of vertices already matched\n",
    "    matching = set()\n",
    "    # Get the indices of the weights in highest to lowest order (hence the negative sign), inspired by https://stackoverflow.com/questions/30577375/have-numpy-argsort-return-an-array-of-2d-indices\n",
    "    indices = np.dstack(np.unravel_index(np.argsort(- A.ravel()), A.shape))\n",
    "    for index in indices[0]:\n",
    "        if index[0] not in matching and index[1] not in matching:\n",
    "            matching_weight += A[tuple(index)]\n",
    "            matching.add(index[0])\n",
    "            matching.add(index[1])\n",
    "\n",
    "\n",
    "    return matching_weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_greedy_matching_weight(i, data, masks, matching, match_rest = False):\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # Only consider the matching values for consumers\n",
    "    matching = np.where(goods_mask == 1, matching, -1)\n",
    "    unmatched_goods = set(range(n, n + m))\n",
    "    unmatched_buyers = set(range(n))\n",
    "\n",
    "    for buyer in range(n):\n",
    "        if buyer in matching:\n",
    "            # If several goods point to the same buyer, keep the one with maximum weight\n",
    "            mask = matching == buyer\n",
    "            matching_weight += np.max(A[buyer, mask])\n",
    "            # Recovering the index of the maximum, inspired by http://seanlaw.github.io/2015/09/10/numpy-argmin-with-a-condition/\n",
    "            subset_idx = np.argmax(A[buyer, mask])\n",
    "            good = np.arange(A.shape[1])[mask][subset_idx]\n",
    "            unmatched_goods.remove(good)\n",
    "            unmatched_buyers.remove(buyer)\n",
    "\n",
    "    if match_rest and len(unmatched_goods) > 0 and len(unmatched_buyers) > 0:\n",
    "        # Compute optimal matching on the remaining unmatched nodes\n",
    "        mask = np.zeros(A.shape)\n",
    "        # TODO this is a horrible solution, there's definitely a prettier solution\n",
    "        mask[list(unmatched_buyers)] += 1\n",
    "        mask[:, list(unmatched_goods)] += 1\n",
    "        mask = np.where(mask == 2, True, False)\n",
    "        remaining_bipartite_graph = A * mask\n",
    "        row_ind, col_ind = linear_sum_assignment(remaining_bipartite_graph, maximize = True)\n",
    "        opt = A[row_ind, col_ind].sum() / 2  #TODO do I always need the division by 2\n",
    "        matching_weight += opt\n",
    "\n",
    "    return matching_weight\n",
    "\n",
    "\n",
    "def compute_partial_matching_weight(i, data, masks, matching):\n",
    "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
    "\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # We only care about the buyer -> good connections\n",
    "    A_submatrix = A[:n, n:n + m]\n",
    "    matching = matching[:n, n:n + m]\n",
    "\n",
    "    max_weight = np.max(np.sum(matching, axis = 0))\n",
    "    # print(f\"max weight: {max_weight}\")\n",
    "    matching /= max_weight\n",
    "    return np.sum(matching * A_submatrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 | loss = 5.347200870513916 | val_acc = 0.310546875 | test_acc = 0.30585938692092896\n",
      "**learned: 0.3276043090977613, greedy: 0.9540878531495511**\n",
      "step = 50 | loss = 0.678571879863739 | val_acc = 0.560546875 | test_acc = 0.536328136920929\n",
      "step = 100 | loss = 0.4104996919631958 | val_acc = 0.572265625 | test_acc = 0.4996093809604645\n",
      "step = 150 | loss = 0.2140585482120514 | val_acc = 0.4375 | test_acc = 0.2621093690395355\n",
      "**learned: 0.8161358912510988, greedy: 0.9559042388299057**\n"
     ]
    }
   ],
   "source": [
    "model = train(model, 400, train_sampler, test_sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "test_feedback = next(test_sampler)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9841672461694815, 0.9590556350722312)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_feedback = next(test_sampler)\n",
    "predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "matching_value(test_feedback, predictions, partial = False, match_rest = False, opt_scipy = False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting test generation\n",
      "finished test generation\n",
      "Bypassing training\n"
     ]
    },
    {
     "data": {
      "text/plain": "[({'num_samples': 100,\n   'batch_size': 32,\n   'schematics': [{'generator': 'ER',\n     'proportion': 1,\n     'length': 64,\n     'kwargs': {'low': 0, 'high': 0.001, 'weighted': True}}]},\n  {'num_samples': 10,\n   'batch_size': 10,\n   'schematics': [{'generator': 'ER',\n     'proportion': 1,\n     'length': 1000,\n     'kwargs': {'p': 0.1, 'low': 0, 'high': 1, 'weighted': True}}]},\n  (0.8577524175395072, 0.9537646702986899))]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def variation_testing(train_sampler_spec, test_sampler_spec, epochs = 300, model = None, bypass_training = False):\n",
    "    if model is None and bypass_training:\n",
    "        print(\"Need a model to bypass training\")\n",
    "        return\n",
    "\n",
    "\n",
    "    matching_values = []\n",
    "    for train_param, test_param in zip(train_sampler_spec, test_sampler_spec):\n",
    "        # test_param['num_samples'] = 40\n",
    "        # test_param['batch_size'] = 40\n",
    "        # schematics = test_param['schematics']\n",
    "        # schematics[0]['length'] = 1000\n",
    "        # test_param['schematics'] = schematics\n",
    "\n",
    "        print(\"starting test generation\")\n",
    "        test_sampler, _ = samplers(test_param)\n",
    "        print(\"finished test generation\")\n",
    "\n",
    "        if not bypass_training:\n",
    "            train_sampler, spec = samplers(train_param)\n",
    "            model = define_model(spec, train_sampler, model=\"mpnn\")\n",
    "            train(model, epochs, train_sampler, test_sampler)\n",
    "        else:\n",
    "            print(\"Bypassing training\")\n",
    "\n",
    "        test_feedback = next(test_sampler)\n",
    "        predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "        accuracy = matching_value(test_feedback, predictions, partial = False, match_rest = False, opt_scipy = True)\n",
    "\n",
    "        matching_values.append((train_param, test_param, accuracy))\n",
    "    return model, matching_values\n",
    "\n",
    "weight_params = [{\"low\": 0, \"high\": 0.001},\n",
    "                 {\"low\": 1, \"high\": 1.001},\n",
    "                 {\"low\": 1, \"high\": 1.1},\n",
    "                 {\"low\": 1, \"high\": 2},\n",
    "                 {\"low\": 0, \"high\": 0.1},\n",
    "                 {\"low\": 0, \"high\": 1},\n",
    "                 # {\"low\": 0, \"high\": 10},\n",
    "                 # {\"low\": 0, \"high\": 100},\n",
    "                 # {\"low\": 50, \"high\": 200},\n",
    "                 # {\"low\": 500, \"high\": 2000},\n",
    "                 # {\"low\": 5000, \"high\": 20000}\n",
    "                 ]\n",
    "\n",
    "\n",
    "train_sampler_spec = [\n",
    "    {\n",
    "        'num_samples': 100, 'batch_size': 32,\n",
    "        'schematics': [\n",
    "            {\n",
    "                'generator': 'ER',\n",
    "                'proportion': 1,\n",
    "                'length': 64,\n",
    "                'kwargs': {'low': 0, 'high': 0.001, 'weighted': True}\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "test_sampler = [\n",
    "    {\n",
    "        'num_samples': 10, 'batch_size': 10,\n",
    "        'schematics': [\n",
    "            {\n",
    "                'generator': 'ER',\n",
    "                'proportion': 1,\n",
    "                'length': 1000,\n",
    "                'kwargs': {'p': 0.1, 'low': 0, 'high': 1, 'weighted': True}\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "length_training = [{\"generator\": \"ER\"}]\n",
    "length_testing = [{\"generator\": \"ER\", \"length\": 1000, \"p\": 0.01}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model, results = variation_testing(train_sampler_spec, copy.deepcopy(test_sampler), model = model, bypass_training = True)\n",
    "\n",
    "results\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# import copy\n",
    "# model2 = copy.deepcopy(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ER p=0.25, 100 8x8 train and 40 32x32 test => 0.94 in 100 iterations\n",
    "\n",
    "BA param=3, 100 8x8 train and 40 32x32 test => 0.97 in 100 iterations\n",
    "\n",
    "BA param=5, 100 8x8 train and 40 32x32 test => 0.95 in 100 iterations (0.951 in 200 so has pretty much converged after 100)\n",
    "\n",
    "BA param=7, 100 8x8 train and 40 32x32 test => 0.946 in 100 iterations\n",
    "\n",
    "#### Cross training\n",
    "BA param=7 to BA param=3\n",
    "\n",
    "BA param=7 to ER p=0.25 0.946 with BA to 0.939 with ER (same as if trained only on BA)\n",
    "\n",
    "ER p=0.25 to BA param=3 went from 0.939 with ER to 0.967 with BA (BA param 3 was 0.97 so basically nothing lost)\n",
    "\n",
    "#### Weight variations\n",
    "Uniform\n",
    "* 0,0.001 -> 0.928\n",
    "* 1,1.001 -> 0.962\n",
    "* 0,0.1 -> 0.931\n",
    "* 0,10 -> 0.883\n",
    "* 0,100 -> 0.77\n",
    "* 50, 200 -> 0.72\n",
    "* 500, 2000 -> 0.69\n",
    "* 5000, 20000 -> 0.7\n",
    "\n",
    "Normal:\n",
    "Basically same.\n",
    "\n",
    "Gumbel\n",
    "* 0,0.001 -> 0.323\n",
    "* 1,1.001 -> 0.849\n",
    "* 0,0.1 -> 0.498\n",
    "* 5,10 -> 0.82\n",
    "* 5,100 -> 0.8\n",
    "\n",
    "#### Weight cross training\n",
    "Train ER p=0.25 unif 0,1:\n",
    "* 0,0.001 -> 0.948\n",
    "* 1,1.001 -> 0.967\n",
    "* 0,0.1 -> 0.916\n",
    "* 0,10 -> 0.86\n",
    "* 0,100 -> 0.75\n",
    "* 50, 200 -> 0.72\n",
    "* 500, 2000 -> 0.72\n",
    "* 5000, 20000 -> 0.69\n",
    "\n",
    "=> Seems to weight generalize quite well. Actually even better because basically no statistical difference with if we trained separately.\n",
    "\n",
    "Train normal 5000, 20000:\n",
    "* 0, 0.001 -> 0.39 (maybe it's the large to small that was a problem here? Also those values make little sense for a normal RV)\n",
    "\n",
    "Other direction train normal 0, 0.001 (got to 0.78):\n",
    "* 5000, 20000 -> 0.76\n",
    "\n",
    "=> small to large seems better\n",
    "\n",
    "\n",
    "#### Larger graphs\n",
    "Same training\n",
    "ER p=0.25 8x8 train:\n",
    "* 100x100 test goes to 0.88\n",
    "* 200x200 goes to 0.63 (only 12 prediction mismatches though)\n",
    "* 200x200 p=0.3 =>\n",
    "* 250x250 => 0.9448 (BUT p=0.1 to not kill my computer)\n",
    "*\n",
    "Try this but 16x16 train\n",
    "\n",
    "#### RIDESHARE\n",
    "8x8 train,\n",
    "* 32x32 test => 0.96\n",
    "* 50x50 test => 0.96\n",
    "* 100x100 test => 0.938\n",
    "* 250x250 test => 0.9\n",
    "\n",
    "#### Double max\n",
    "8x8 train 32x23 test\n",
    "300 iterations gets us to 0.93 as normal max (though normal max takes 100 iterations to get there), 600 iterations gets us to 0.965\n",
    "==> Testing single max on 600 iterations => 0.956\n",
    "==> Testing single max with 64 hidden dim embeddings on 600 iterations 0.96 (already in 200) (seeing if gain is only from more parameters or if double max is actually more aligned)\n",
    "\n",
    "Conclusion, it was mainly due to more iterations + some amount of more parameters but only 1% so probably not statistically significant.\n",
    "\n",
    "#### Training with scaling\n",
    "Train/test with 5000, 200000 weights ==> 0.76 accuracy\n",
    "But if normalize 0, 1 on training (or just train on normalized) ==> 0.91 (same acc as had train/testing on normalized)\n",
    "\n",
    "#### More weight scales training\n",
    "300 epochs for all, 4x4 train, 32x32 test\n",
    "* 0, 1: 0.946, 0.917\n",
    "* 1, 1.01: 0.993, 0.970\n",
    "* 1, 1.001: 0.956, 0.976\n",
    "* 1, 1.1: 0.989, 0.972\n",
    "* 1, 1.2: 0.986, 0.958\n",
    "* 1, 1.5: 0.966, 0.949\n",
    "* 1, 2: 0.957, 0.939\n",
    "* 2, 2.1: 0.9927, 0.9757\n",
    "* 10, 10.001: 0.4, 0.97\n",
    "Realization: shifting just doesn't makes sense (val + 1000) / (opt + 1000) > val / opt\n",
    "\n",
    "==> find the best range\n",
    "* 0, 0.001: 0.79, 0.929\n",
    "* 0, 0.01: 0.79, 0.922\n",
    "* 0, 0.1: 0.76, 0.934\n",
    "* 0, 1: 0.94, 0.9334\n",
    "* 0, 10: 0.897, 0.928\n",
    "* 0, 100: 0.7, 0.93\n",
    "\n",
    "### Teacher forcing\n",
    "1.0:  100 epochs => 0.92   | 200 epochs => 0.968\n",
    "0.75: 100 epochs => 0.956 | 200 epochs => 0.959 | 300 => 0.957 | 400 => 0.966 | 500 => 0.954 | 600 => 0.964\n",
    "0.5: 100 epochs => 0.943 | 200 => 0.943 | 300 => 0.952 | 400 => 0.938 | 500 => 0.928 | 0.953\n",
    "0.25: 100 => 0.923 | 200 => 0.935 | 300 => 0.932 | 400 => 0.936 | 500 => 0.948 | 600 => 0.94\n",
    "0: 100 => 0.9 | 200 => 0.977 | 300 => 0.937 | 400 => 0.924 | 500 => 0.923 | 600 => 0.958 | 800 => 0.98\n",
    "\n",
    "### GMPNN\n",
    "MPNN: 100 => 0.92  | 200 => 0.968\n",
    "GMPNN 100 => 0.956 | 200 => 0.957 | 300 => 0.948 | 400 => 0.935\n",
    "\n",
    "### Larger graphs\n",
    "GPMNN with 16 node (8x8) graphs as training\n",
    "100 => 0.965 | 200 => 0.968 | 300 => 0.969 | 400 => 0.971 | 500 => 0.972 | 600 => 0.971 | 700 => 0.981 | 800 => 0.973\n",
    "\n",
    "### Train on larger\n",
    "GPMNN with 16 node (8x8) graphs as training\n",
    "100 => 0.965 | 200 => 0.968 | 300 => 0.969 | 400 => 0.971 | 500 => 0.972 | 600 => 0.971 | 700 => 0.981 | 800 => 0.973\n",
    "\n",
    "### Soft pointers\n",
    "\n",
    "\n",
    "#### Cross training with p value for ER\n",
    "GMPNN 400 epochs\n",
    "ER\n",
    "0.05: 0.87, 0.96\n",
    "0.1: 0.76, 0.94\n",
    "0.2: 0.75, 0.91\n",
    "0.5: 0.93, 0.93\n",
    "0.75: 0.94, 0.95\n",
    "1: 0.88, 0.96\n",
    "\n",
    "Compare to if learned directly: (train on those parameters then test i.e. no cross-training) + is MPNN, not GMPNN\n",
    "0.05: 0.95, 0.96\n",
    "0.1: 0.86, 0.93\n",
    "0.2: 0.7, 0.9\n",
    "0.5: 0.9, 0.94\n",
    "0.75: 0.93, 0.95\n",
    "1: 0.85, 0.95\n",
    "\n",
    "Already got ER + Rideshare generalization + BA generalization to other parameters to larger graphs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminary results\n",
    "random permutation/matching: 0.18\n",
    "\n",
    "MPNN:\n",
    "learned predictions: 0.67\n",
    "\n",
    "GAT:\n",
    "learned predictions: 0.72\n",
    "\n",
    "Got better with double ended predictions\n",
    "\n",
    "Partial: 0.64 while greedy was doing about 0.92 on the same instance. Main reason seems to be that max weight is around 1.5 => can get at most 2/3 OPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting the number of matching constraints violated"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 12.2\n"
     ]
    }
   ],
   "source": [
    "# For two-way\n",
    "def count_mismatches_two_way(predictions):\n",
    "    count = 0\n",
    "    data = predictions[\"match\"].data\n",
    "    nb_graphs = data.shape[0]\n",
    "    for datapoint in range(data.shape[0]):\n",
    "        for i in range(32):\n",
    "            owner = data[datapoint][i]\n",
    "            good = data[datapoint][int(owner)]\n",
    "            if good != i:\n",
    "                count += 1\n",
    "    print(f\"average number of edges contradicting matching: {count / nb_graphs}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 0.4\n"
     ]
    }
   ],
   "source": [
    "# For self-loops\n",
    "def count_mismatches_self_loop(predictions):\n",
    "    count = 0\n",
    "    data = predictions[\"match\"].data\n",
    "    nb_graphs = data.shape[0]\n",
    "    for datapoint in range(data.shape[0]):\n",
    "        owners = set(np.array(data[datapoint][32:64]))\n",
    "        count += 32 - len(owners)\n",
    "    print(f\"average number of edges contradicting matching: {count / nb_graphs}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "b = np.array([2, 3])\n",
    "print(np.concatenate((a, b)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
