{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "2MzxRB1X7hRs",
            "outputId": "eee4d47e-b689-497b-e9b5-021121cac2b7"
         },
         "outputs": [],
         "source": [
            "import clrs\n",
            "import numpy as np\n",
            "import jax\n",
            "import jax.numpy as jnp\n",
            "\n",
            "import pprint\n",
            "\n",
            "rng = np.random.RandomState(1234)\n",
            "rng_key = jax.random.PRNGKey(rng.randint(2**32))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "OEo_Gj1j3Z6M",
            "outputId": "5ef53c51-9bc2-4a49-b4cf-592a32dfa5b7"
         },
         "outputs": [],
         "source": [
            "# =================== BIPARTITE GRAPH GENERATOR SPECS =========================\n",
            "# - Erdos Reyni (ER)\n",
            "#     Generates an Erdos-Reyni random graph with edge probability [p]. If\n",
            "#       [weighted], edge weights are iid uniform([low], [high])\n",
            "#\n",
            "#     {'generator': ER, 'weighted': False, 'p': 0.5, 'low': 0.0, 'high': 1.0}\n",
            "#\n",
            "# - Barabasi-Albert (BA)\n",
            "#     Generates a Barabasi-Albert random graph with parameter [ba_param]. If\n",
            "#       [weighted], edge weights are iid uniform([low], [high])\n",
            "#\n",
            "#     {'generator': BA, 'ba_param': 1, 'weighted': False, 'low': 0.0, 'high': 1.0}\n",
            "#\n",
            "# - Geometric (GEOMETRIC)\n",
            "#     Generates a random graph by embedding nodes uniformly over the unit\n",
            "#       square. Edge weights are the euclidean distance between two nodes,\n",
            "#       with weights below [threshold] set to 0, then scaled by [scaling].\n",
            "#\n",
            "#     {'generator': GEOMETRIC, 'threshold': 0.25, 'scaling': 1.0}\n",
            "#\n",
            "# - Flow (FLOW)\n",
            "#     Generates a random ER reduction to a max flow input, with edge\n",
            "#       probability [p].\n",
            "#     {'generator': FLOW, 'p': 0.5}\n",
            "\n",
            "\n",
            "\n",
            "train_sampler, spec = clrs.build_sampler(\n",
            "    name='simplified_min_sum',\n",
            "    num_samples=1,\n",
            "    length=16,\n",
            "    generator='DATASET',\n",
            "    filepath='data/gmission_edges.txt'\n",
            "    )\n",
            "\n",
            "test_sampler, spec = clrs.build_sampler(\n",
            "    name='simplified_min_sum',\n",
            "    num_samples=1,\n",
            "    length=4,\n",
            "    weighted=True,\n",
            "    generator='ER')\n",
            "\n",
            "pprint.pprint(spec)\n",
            "\n",
            "def _iterate_sampler(sampler, batch_size):\n",
            "  while True:\n",
            "    yield sampler.next(batch_size)\n",
            "\n",
            "train_sampler = _iterate_sampler(train_sampler, batch_size=1)\n",
            "test_sampler = _iterate_sampler(test_sampler, batch_size=1)\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "x = next(train_sampler)[0][0][1].data\n",
            "print(np.sum(x != 0))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "id": "L-p0jOCq5sPV"
         },
         "outputs": [],
         "source": [
            "processor_factory = clrs.get_processor_factory('mpnn', use_ln=True, nb_triplet_fts=0) #use_ln => use layer norm\n",
            "# processor_factory = clrs.get_processor_factory('gat', use_ln=True, nb_heads = 4, nb_triplet_fts = 0)\n",
            "model_params = dict(\n",
            "    processor_factory=processor_factory, # contains the processor_factory\n",
            "    hidden_dim=64, # TODO put back to 32 if no difference\n",
            "    encode_hints=True,\n",
            "    decode_hints=True,\n",
            "    #decode_diffs=False,\n",
            "    #hint_teacher_forcing_noise=1.0,\n",
            "    hint_teacher_forcing=1.0,\n",
            "    use_lstm=False,\n",
            "    learning_rate=0.001,\n",
            "    checkpoint_path='/tmp/checkpt',\n",
            "    freeze_processor=False, # Good for post step\n",
            "    dropout_prob=0.5,\n",
            "    # nb_msg_passing_steps=3,\n",
            ")\n",
            "\n",
            "dummy_trajectory = next(train_sampler) # jax needs a trajectory that is plausible looking to init\n",
            "\n",
            "model = clrs.models.BaselineModel(\n",
            "    spec=spec,\n",
            "    dummy_trajectory=dummy_trajectory,\n",
            "    **model_params\n",
            ")\n",
            "\n",
            "model.init(dummy_trajectory.features, 1234) # 1234 is a random seed"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "colab": {
               "base_uri": "https://localhost:8080/"
            },
            "id": "3pSKQ2wi62Br",
            "outputId": "b1acd203-e256-4f39-e059-9c7e55942a87"
         },
         "outputs": [],
         "source": [
            "import copy\n",
            "\n",
            "step = 0\n",
            "\n",
            "while step <= 100:\n",
            "  feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
            "  # TODO remove - testing if uses hints on tests\n",
            "  # shape = test_feedback.features.hints[0].data[0].shape\n",
            "  # test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\n",
            "\n",
            "  rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
            "  cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
            "  rng_key = new_rng_key\n",
            "  if step % 10 == 0:\n",
            "    predictions_val, _ = model.predict(rng_key, feedback.features)\n",
            "    out_val = clrs.evaluate(feedback.outputs, predictions_val)\n",
            "    predictions, _ = model.predict(rng_key, test_feedback.features)\n",
            "    out = clrs.evaluate(test_feedback.outputs, predictions)\n",
            "    print(predictions)\n",
            "    print(f'step = {step} | loss = {cur_loss} | val_acc = {out_val[\"score\"]} | test_acc = {out[\"score\"]}') # here, val accuracy is actually training accuracy, not great but is example\n",
            "  step += 1\n",
            "model2 = copy.deepcopy(model)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "collapsed": false
         },
         "source": [
            "## Some intermediate results\n",
            "ALL: 0.5 dropout\n",
            "\n",
            "MPNN 100 train, 40 test, 100 epochs, self-loops -> loss = 0.9931782484054565 | val_acc = 0.8515625 | test_acc = 0.7484375238418579 | accuracy = 0.65, average nb non-matched: 13.45\n",
            "\n",
            "MPNN 200 train, 40 test, 100 epochs, self-loops -> loss = 0.8129420280456543 | val_acc = 0.8125 | test_acc = 0.77734375 | accuracy = 0.72, average nb non-matched: 11.125\n",
            "\n",
            "MPNN 100 train, 40 test, 100 epochs, double links -> loss = 0.8689386248588562 | val_acc = 0.7109375 | test_acc = 0.42695313692092896 | accuracy =? NOTE: only started \"learning\" in the last epochs => trying more, interestingly has less loss than self-loops but less accuracy too\n",
            "\n",
            "MPNN 100 train, 40 test, 200 epochs, double links -> step = 100 | loss = 0.6802611351013184 | val_acc = 0.806640625 | test_acc = 0.681640625 | accuracy = 0.89, average nb non-matched: 5.65\n",
            "\n",
            "MPNN 300 train, 40 test, 400 epochs, double links -> loss = 0.5485531091690063 | val_acc = 0.775390625 | test_acc = 0.6910156607627869 | accuracy = 0.928, average nb non-matched: 4.075 Note: best test_acc 0.727, similar test_acc to 100 train 200 epochs but better accuracy + still does not converge on training accuracy though\n",
            "\n",
            "Diff: length 100 testing instead of 64\n",
            "MPNN 100 train, 40 test LENGTH 100, 200 epochs, double links -> loss = 0.6958761215209961 | val_acc = 0.787109375 | test_acc = 0.503250002861023 | accuracy = 0.759, average nb non-matched: 7.9/100\n",
            "\n",
            "\n",
            "#### Now with actually bipartite graph (no owner-owner / good-good edges)\n",
            "Doesn't really change results\n",
            "\n",
            "ALL with 0 dropout\n",
            "\n",
            "#### No hints\n",
            "0 dropout Can get up to 0.78 of OPT, average nb non-matched: 10.5/64\n",
            "\n",
            "\n",
            "#### Training with 64 hidden dimensions\n",
            "MPNN 100 train, 40 test, 0 dropout, double links -> Get to 90% acc in 30 iterations, 93% in 60\n",
            "GAT 100 train, 40 test, 0 dropout, double links -> Get to 0.78 in 30 iterations, 91% in 60, 92% in 100, 92% in 200\n",
            "\n",
            "Same with 0.5 dropout\n",
            "MPNN 100 train, 40 test, 0.5 dropout, double links -> Get to 85% in 30 iterations, 93% in 60 iterations, 93% in 100 iterations\n",
            "GAT 100 train, 40 test, 0.5 dropout, double links -> 94.7% in 200\n",
            "\n",
            "#### 3 message passing steps\n",
            "64 dims, 3 message passing steps\n",
            "MPNN 100 train, 40 test, 0.5 dropout, double links -> 91.7% in 100 iterations (worse than 1 MP step), 93% in 200 iterations\n",
            "\n",
            "Back to 1 message passing step\n",
            "\n",
            "#### Larger MLP\n",
            "[out_size, out_size, out_size] MLP\n",
            "MPNN 100 train, 40 test, 0.5 dropout, double links, 3 layer MLP -> loss = 0.6642395257949829 | val_acc = 0.75390625 | test_acc = 0.699999988079071 | 90% in 100 iterations (worse than smaller MLP) | 93% in 200 iterations\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "from scipy.optimize import linear_sum_assignment\n",
            "\n",
            "\n",
            "def matching_value(samples, predictions, partial = False, match_rest = False, opt_scipy = False):\n",
            "    features = samples.features\n",
            "    gt_matchings = samples.outputs[0].data\n",
            "    # inputs for the matrix A are at index 1 (see spec.py)\n",
            "    data = features.inputs[1].data\n",
            "    masks = features.inputs[2].data\n",
            "    pred_accuracy = 0\n",
            "\n",
            "    # Iterating over all the samples\n",
            "    for i in range(data.shape[0]):\n",
            "        max_weight = compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n",
            "\n",
            "        # TODO remove\n",
            "        predicted_matching = predictions[\"match\"].data[i]\n",
            "        # buyers_mask = masks[i]\n",
            "        # n = int(np.sum(buyers_mask))\n",
            "        # permutation = np.random.permutation(np.arange(np.sum(buyers_mask == 0)))\n",
            "        # predicted_matching = np.concatenate((np.zeros(n), permutation))\n",
            "        if partial:\n",
            "            preds_weight = compute_partial_matching_weight(i, data, masks, predicted_matching)\n",
            "            print(f\"opt: {max_weight}, greedy: {preds_weight}\")\n",
            "        else:\n",
            "            preds_weight = compute_greedy_matching_weight(i, data, masks, predicted_matching, match_rest = match_rest)\n",
            "            print(f\"opt: {max_weight}, partial: {preds_weight}\")\n",
            "\n",
            "        # assert preds_weight <= max_weight\n",
            "        pred_accuracy += preds_weight / max_weight\n",
            "\n",
            "    return pred_accuracy / data.shape[0]\n",
            "\n",
            "\n",
            "def compute_greedy_matching_weight(i, data, masks, matching, match_rest = False):\n",
            "    matching_weight = 0\n",
            "    A = data[i]\n",
            "    buyers_mask = masks[i]\n",
            "    n = int(np.sum(buyers_mask))\n",
            "    goods_mask = 1 - buyers_mask\n",
            "    m = int(np.sum(goods_mask))\n",
            "\n",
            "    # Only consider the matching values for consumers\n",
            "    matching = np.where(goods_mask == 1, matching, -1)\n",
            "    unmatched_goods = set(range(n, n + m))\n",
            "    unmatched_buyers = set(range(n))\n",
            "\n",
            "    for buyer in range(n):\n",
            "        if buyer in matching:\n",
            "            # If several goods point to the same buyer, keep the one with maximum weight\n",
            "            candidates = A[buyer, matching == buyer]\n",
            "            matching_weight += np.max(candidates)\n",
            "            matched.remove(np.argmax(A))\n",
            "\n",
            "\n",
            "    return matching_weight\n",
            "\n",
            "\n",
            "def compute_partial_matching_weight(i, data, masks, matching):\n",
            "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
            "\n",
            "    matching_weight = 0\n",
            "    A = data[i]\n",
            "    buyers_mask = masks[i]\n",
            "    n = int(np.sum(buyers_mask))\n",
            "    goods_mask = 1 - buyers_mask\n",
            "    m = int(np.sum(goods_mask))\n",
            "\n",
            "    # We only care about the buyer -> good connections\n",
            "    A_submatrix = A[:n, n:n + m]\n",
            "    matching = matching[:n, n:n + m]\n",
            "\n",
            "    max_weight = np.max(np.sum(matching, axis = 0))\n",
            "    print(f\"max weight: {max_weight}\")\n",
            "    matching /= max_weight\n",
            "    return np.sum(matching * A_submatrix)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "test_feedback = next(test_sampler)\n",
            "predictions, _ = model.predict(rng_key, test_feedback.features)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "predictions['match'].data[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_feedback.outputs[0].data[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "matching_value(test_feedback, predictions, partial = True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "collapsed": false
         },
         "source": [
            "## Preliminary results\n",
            "random permutation/matching: 0.18\n",
            "\n",
            "MPNN:\n",
            "learned predictions: 0.67\n",
            "\n",
            "GAT:\n",
            "learned predictions: 0.72\n",
            "\n",
            "Got better with double ended predictions\n",
            "\n",
            "Partial: 0.64 while greedy was doing about 0.92 on the same instance. Main reason seems to be that max weight is around 1.5 => can get at most 2/3 OPT\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "collapsed": false
         },
         "source": [
            "### Counting the number of matching constraints violated"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "# For two-way\n",
            "count = 0\n",
            "data = predictions[\"owners\"].data\n",
            "nb_graphs = data.shape[0]\n",
            "for datapoint in range(data.shape[0]):\n",
            "    for i in range(32):\n",
            "        owner = data[datapoint][i]\n",
            "        good = data[datapoint][int(owner)]\n",
            "        if good != i:\n",
            "            count += 1\n",
            "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# For self-loops\n",
            "count = 0\n",
            "data = predictions[\"owners\"].data\n",
            "nb_graphs = data.shape[0]\n",
            "for datapoint in range(data.shape[0]):\n",
            "    owners = set(np.array(data[datapoint][32:64]))\n",
            "    count += 32 - len(owners)\n",
            "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "predictions, hints = model.predict(rng_key, test_feedback.features, return_hints = True, return_all_outputs = True)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "predictions['owners'].data[::100].shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "np.argmax(hints[10]['owners_h'][0], axis =\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "len(hints)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "arr = np.array([1, 2, 3, 4, 5, 6])\n",
            "arr[np.arange(len(arr)) % 2 == 0]\n",
            "arr[::2]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "test_feedback.features.inputs[1].data[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "arr = np.array([[1,2],[2,3]])\n",
            "np.max(arr, axis = 0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "colab": {
         "gpuType": "T4",
         "provenance": []
      },
      "gpuClass": "standard",
      "kernelspec": {
         "display_name": "Python 3",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.3"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 0
}
