{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MzxRB1X7hRs",
    "outputId": "eee4d47e-b689-497b-e9b5-021121cac2b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 22:06:08.845269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/anders/miniconda3/envs/clrs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import clrs\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import pprint\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
<<<<<<< HEAD
    "rng_key = jax.random.PRNGKey(rng.randint(2**32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
=======
    "rng_key = jax.random.PRNGKey(rng.randint(2 ** 32))"
   ],
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEo_Gj1j3Z6M",
    "outputId": "5ef53c51-9bc2-4a49-b4cf-592a32dfa5b7"
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ('input', 'edge', 'scalar'),\n",
      " 'L': ('input', 'node', 'mask'),\n",
      " 'M_h': ('hint', 'edge', 'scalar'),\n",
      " 'match': ('output', 'node', 'pointer'),\n",
      " 'match_h': ('hint', 'node', 'pointer'),\n",
      " 'pos': ('input', 'node', 'scalar')}\n"
     ]
    }
   ],
   "source": [
    "# =================== BIPARTITE GRAPH GENERATOR SPECS =========================\n",
    "# - Erdos Reyni (ER)\n",
    "#     Generates an Erdos-Reyni random graph with edge probability [p]. If\n",
    "#       [weighted], edge weights are iid uniform([low], [high])\n",
    "#\n",
    "#     {'generator': ER, 'weighted': False, 'p': 0.5, 'low': 0.0, 'high': 1.0}\n",
    "#\n",
    "# - Barabasi-Albert (BA)\n",
    "#     Generates a Barabasi-Albert random graph with parameter [ba_param]. If\n",
    "#       [weighted], edge weights are iid uniform([low], [high])\n",
    "#\n",
    "#     {'generator': BA, 'ba_param': 1, 'weighted': False, 'low': 0.0, 'high': 1.0}\n",
    "#\n",
    "# - Geometric (GEOMETRIC)\n",
    "#     Generates a random graph by embedding nodes uniformly over the unit\n",
    "#       square. Edge weights are the euclidean distance between two nodes,\n",
    "#       with weights below [threshold] set to 0, then scaled by [scaling].\n",
    "#\n",
    "#     {'generator': GEOMETRIC, 'threshold': 0.25, 'scaling': 1.0}\n",
    "#\n",
    "# - Flow (FLOW)\n",
    "#     Generates a random ER reduction to a max flow input, with edge\n",
    "#       probability [p].\n",
    "#     {'generator': FLOW, 'p': 0.5}\n",
    "\n",
    "\n",
    "\n",
    "train_sampler, spec = clrs.build_sampler(\n",
    "    name='simplified_min_sum',\n",
    "    num_samples=1,\n",
    "    length=5,\n",
    "    generator='BA',\n",
    "    ba_param=2,\n",
    "    weighted=True,\n",
    "    low=1.0,\n",
    "    high=2.0\n",
    "    )\n",
    "\n",
    "test_sampler, spec = clrs.build_sampler(\n",
    "    name='simplified_min_sum',\n",
    "    num_samples=1,\n",
    "    length=4,\n",
    "    weighted=True,\n",
    "    generator='ER')\n",
    "\n",
    "pprint.pprint(spec)\n",
    "\n",
    "def _iterate_sampler(sampler, batch_size):\n",
    "  while True:\n",
    "    yield sampler.next(batch_size)\n",
    "\n",
    "train_sampler = _iterate_sampler(train_sampler, batch_size=1)\n",
    "test_sampler = _iterate_sampler(test_sampler, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 1.30032796, 1.06683875],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 1.33751051, 0.        ],\n",
       "        [1.30032796, 0.        , 1.33751051, 0.        , 0.        ],\n",
       "        [1.06683875, 0.        , 0.        , 0.        , 0.        ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_sampler)[0][0][1].data"
   ]
=======
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 18:07:43.147297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def samplers(num_samples, length, batch_size, low = 0.0, high = 1.0, graph_type = \"er\", p = (0.25,), ba_param = 5):\n",
    "\n",
    "    def _iterate_sampler(sampler, batch_size):\n",
    "        while True:\n",
    "            yield sampler.next(batch_size)\n",
    "\n",
    "    sampler, spec = clrs.build_sampler(\n",
    "        name = 'simplified_min_sum',\n",
    "        num_samples = num_samples,\n",
    "        length = length,\n",
    "        low = low,\n",
    "        high = high,\n",
    "        graph_type = graph_type,\n",
    "        p = p,\n",
    "        ba_param = ba_param,\n",
    "        weighted = True)  # number of nodes\n",
    "\n",
    "    train_sampler, spec = clrs.build_sampler(\n",
    "        name = 'simplified_min_sum',\n",
    "        num_samples = 100,\n",
    "        length = 16,\n",
    "        graph_type = \"er\",\n",
    "        weighted = True)  # number of nodes\n",
    "\n",
    "    sampler = _iterate_sampler(sampler, batch_size = batch_size)\n",
    "    return sampler, spec\n",
    "\n",
    "train_sampler, spec = samplers(100, 16, 32)\n",
    "test_sampler, _ = samplers(40, 64, 40)"
   ],
   "metadata": {
    "id": "OEo_Gj1j3Z6M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ef53c51-9bc2-4a49-b4cf-592a32dfa5b7"
   },
   "execution_count": 2,
   "outputs": []
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L-p0jOCq5sPV"
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "processor_factory = clrs.get_processor_factory('mpnn', use_ln=True, nb_triplet_fts=0) #use_ln => use layer norm\n",
    "# processor_factory = clrs.get_processor_factory('gat', use_ln=True, nb_heads = 4, nb_triplet_fts = 0)\n",
    "model_params = dict(\n",
    "    processor_factory=processor_factory, # contains the processor_factory\n",
    "    hidden_dim=64, # TODO put back to 32 if no difference\n",
    "    encode_hints=True,\n",
    "    decode_hints=True,\n",
    "    #decode_diffs=False,\n",
    "    #hint_teacher_forcing_noise=1.0,\n",
    "    hint_teacher_forcing=1.0,\n",
    "    use_lstm=False,\n",
    "    learning_rate=0.001,\n",
    "    checkpoint_path='/tmp/checkpt',\n",
    "    freeze_processor=False, # Good for post step\n",
    "    dropout_prob=0.5,\n",
    "    # nb_msg_passing_steps=3,\n",
    ")\n",
    "\n",
    "dummy_trajectory = next(train_sampler) # jax needs a trajectory that is plausible looking to init\n",
    "\n",
    "model = clrs.models.BaselineModel(\n",
    "    spec=spec,\n",
    "    dummy_trajectory=dummy_trajectory,\n",
    "    **model_params\n",
    ")\n",
    "\n",
    "model.init(dummy_trajectory.features, 1234) # 1234 is a random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
=======
    "def define_model(spec, train_sampler, model = \"mpnn\"):\n",
    "    if model == \"mpnn\":\n",
    "        processor_factory = clrs.get_processor_factory('mpnn', use_ln = True,\n",
    "                                                   nb_triplet_fts = 0)  #use_ln => use layer norm\n",
    "    elif model == \"gat\":\n",
    "        processor_factory = clrs.get_processor_factory('gat', use_ln=True, nb_heads = 4, nb_triplet_fts = 0)\n",
    "\n",
    "    model_params = dict(\n",
    "        processor_factory = processor_factory,  # contains the processor_factory\n",
    "        hidden_dim = 32,  # TODO put back to 32 if no difference\n",
    "        encode_hints = True,\n",
    "        decode_hints = True,\n",
    "        #decode_diffs=False,\n",
    "        #hint_teacher_forcing_noise=1.0,\n",
    "        hint_teacher_forcing = 1.0,\n",
    "        use_lstm = False,\n",
    "        learning_rate = 0.001,\n",
    "        checkpoint_path = '/tmp/checkpt',\n",
    "        freeze_processor = False,  # Good for post step\n",
    "        dropout_prob = 0.5,\n",
    "        # nb_msg_passing_steps=3,\n",
    "    )\n",
    "\n",
    "    dummy_trajectory = next(train_sampler)  # jax needs a trajectory that is plausible looking to init\n",
    "\n",
    "    model = clrs.models.BaselineModel(\n",
    "        spec = spec,\n",
    "        dummy_trajectory = dummy_trajectory,\n",
    "        **model_params\n",
    "    )\n",
    "\n",
    "    model.init(dummy_trajectory.features, 1234)  # 1234 is a random seed\n",
    "\n",
    "    return model\n",
    "\n",
    "model = define_model(spec, train_sampler, \"mpnn\")"
   ],
   "metadata": {
    "id": "L-p0jOCq5sPV"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# No evaluation since we are postprocessing with soft: TO CHANGE -> baselines.py line 336 outs change hard to False\n",
    "# step = 0\n",
    "#\n",
    "# while step <= 1:\n",
    "#     feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "#     rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "#     cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "#     rng_key = new_rng_key\n",
    "#     if step % 10 == 0:\n",
    "#         print(step)\n",
    "#     step += 1\n",
    "\n"
   ],
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pSKQ2wi62Br",
    "outputId": "b1acd203-e256-4f39-e059-9c7e55942a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 0 | loss = 6.914782524108887 | val_acc = 0.240234375 | test_acc = 0.15000000596046448\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 10 | loss = 2.262831211090088 | val_acc = 0.513671875 | test_acc = 0.38750001788139343\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 20 | loss = 1.125622034072876 | val_acc = 0.63671875 | test_acc = 0.44414064288139343\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 30 | loss = 0.7239480018615723 | val_acc = 0.69921875 | test_acc = 0.583984375\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 40 | loss = 0.5074636936187744 | val_acc = 0.654296875 | test_acc = 0.6363281607627869\n",
      "{'match': DataPoint(name=\"match\",\tlocation=node,\ttype=pointer,\tdata=Array(40, 64))}\n",
      "step = 50 | loss = 0.43849965929985046 | val_acc = 0.693359375 | test_acc = 0.581250011920929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# TODO remove - testing if uses hints on tests\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# shape = test_feedback.features.hints[0].data[0].shape\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m rng_key, new_rng_key \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(rng_key) \u001b[39m# jax needs new random seed at step\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m cur_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfeedback(rng_key, feedback) \u001b[39m# loss is contained in model somewhere\u001b[39;00m\n\u001b[1;32m     13\u001b[0m rng_key \u001b[39m=\u001b[39m new_rng_key\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/clrs/clrs/clrs/_src/baselines.py:371\u001b[0m, in \u001b[0;36mBaselineModel.feedback\u001b[0;34m(self, rng_key, feedback, algorithm_index)\u001b[0m\n\u001b[1;32m    369\u001b[0m rng_keys \u001b[39m=\u001b[39m _maybe_pmap_rng_key(rng_key)  \u001b[39m# pytype: disable=wrong-arg-types  # numpy-scalars\u001b[39;00m\n\u001b[1;32m    370\u001b[0m feedback \u001b[39m=\u001b[39m _maybe_pmap_data(feedback)\n\u001b[0;32m--> 371\u001b[0m loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_params, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_opt_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjitted_feedback(\n\u001b[1;32m    372\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device_params, rng_keys, feedback,\n\u001b[1;32m    373\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device_opt_state, algorithm_index)\n\u001b[1;32m    374\u001b[0m loss \u001b[39m=\u001b[39m _maybe_pick_first_pmapped(loss)\n\u001b[1;32m    375\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "import copy\n",
    "\n",
    "step = 0\n",
    "\n",
    "while step <= 100:\n",
    "  feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "  # TODO remove - testing if uses hints on tests\n",
    "  # shape = test_feedback.features.hints[0].data[0].shape\n",
    "  # test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\n",
    "\n",
    "  rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "  cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "  rng_key = new_rng_key\n",
    "  if step % 10 == 0:\n",
    "    predictions_val, _ = model.predict(rng_key, feedback.features)\n",
    "    out_val = clrs.evaluate(feedback.outputs, predictions_val)\n",
    "    predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "    out = clrs.evaluate(test_feedback.outputs, predictions)\n",
    "    print(predictions)\n",
    "    print(f'step = {step} | loss = {cur_loss} | val_acc = {out_val[\"score\"]} | test_acc = {out[\"score\"]}') # here, val accuracy is actually training accuracy, not great but is example\n",
    "  step += 1\n",
    "model2 = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Some intermediate results\n",
    "ALL: 0.5 dropout\n",
    "\n",
    "MPNN 100 train, 40 test, 100 epochs, self-loops -> loss = 0.9931782484054565 | val_acc = 0.8515625 | test_acc = 0.7484375238418579 | accuracy = 0.65, average nb non-matched: 13.45\n",
    "\n",
    "MPNN 200 train, 40 test, 100 epochs, self-loops -> loss = 0.8129420280456543 | val_acc = 0.8125 | test_acc = 0.77734375 | accuracy = 0.72, average nb non-matched: 11.125\n",
    "\n",
    "MPNN 100 train, 40 test, 100 epochs, double links -> loss = 0.8689386248588562 | val_acc = 0.7109375 | test_acc = 0.42695313692092896 | accuracy =? NOTE: only started \"learning\" in the last epochs => trying more, interestingly has less loss than self-loops but less accuracy too\n",
    "\n",
    "MPNN 100 train, 40 test, 200 epochs, double links -> step = 100 | loss = 0.6802611351013184 | val_acc = 0.806640625 | test_acc = 0.681640625 | accuracy = 0.89, average nb non-matched: 5.65\n",
    "\n",
    "MPNN 300 train, 40 test, 400 epochs, double links -> loss = 0.5485531091690063 | val_acc = 0.775390625 | test_acc = 0.6910156607627869 | accuracy = 0.928, average nb non-matched: 4.075 Note: best test_acc 0.727, similar test_acc to 100 train 200 epochs but better accuracy + still does not converge on training accuracy though\n",
    "\n",
    "Diff: length 100 testing instead of 64\n",
    "MPNN 100 train, 40 test LENGTH 100, 200 epochs, double links -> loss = 0.6958761215209961 | val_acc = 0.787109375 | test_acc = 0.503250002861023 | accuracy = 0.759, average nb non-matched: 7.9/100\n",
    "\n",
    "\n",
    "#### Now with actually bipartite graph (no owner-owner / good-good edges)\n",
    "Doesn't really change results\n",
    "\n",
    "ALL with 0 dropout\n",
    "\n",
    "#### No hints\n",
    "0 dropout Can get up to 0.78 of OPT, average nb non-matched: 10.5/64\n",
    "\n",
    "\n",
    "#### Training with 64 hidden dimensions\n",
    "MPNN 100 train, 40 test, 0 dropout, double links -> Get to 90% acc in 30 iterations, 93% in 60\n",
    "GAT 100 train, 40 test, 0 dropout, double links -> Get to 0.78 in 30 iterations, 91% in 60, 92% in 100, 92% in 200\n",
    "\n",
    "Same with 0.5 dropout\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links -> Get to 85% in 30 iterations, 93% in 60 iterations, 93% in 100 iterations\n",
    "GAT 100 train, 40 test, 0.5 dropout, double links -> 94.7% in 200\n",
    "\n",
    "#### 3 message passing steps\n",
    "64 dims, 3 message passing steps\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links -> 91.7% in 100 iterations (worse than 1 MP step), 93% in 200 iterations\n",
    "\n",
    "Back to 1 message passing step\n",
    "\n",
    "#### Larger MLP\n",
    "[out_size, out_size, out_size] MLP\n",
    "MPNN 100 train, 40 test, 0.5 dropout, double links, 3 layer MLP -> loss = 0.6642395257949829 | val_acc = 0.75390625 | test_acc = 0.699999988079071 | 90% in 100 iterations (worse than smaller MLP) | 93% in 200 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
=======
    "def train(model, epochs):\n",
    "    step = 0\n",
    "    rng_key = jax.random.PRNGKey(rng.randint(2 ** 32))\n",
    "\n",
    "    while step < epochs:\n",
    "        feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "        # TODO remove - testing if uses hints on tests\n",
    "        # shape = test_feedback.features.hints[0].data[0].shape\n",
    "        # test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\n",
    "\n",
    "        rng_key, new_rng_key = jax.random.split(rng_key)  # jax needs new random seed at step\n",
    "        cur_loss = model.feedback(rng_key, feedback)  # loss is contained in model somewhere\n",
    "        rng_key = new_rng_key\n",
    "        if step % 10 == 0:\n",
    "            predictions_val, _ = model.predict(rng_key, feedback.features)\n",
    "            out_val = clrs.evaluate(feedback.outputs, predictions_val)\n",
    "            predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "            out = clrs.evaluate(test_feedback.outputs, predictions)\n",
    "            print(\n",
    "                f'step = {step} | loss = {cur_loss} | val_acc = {out_val[\"score\"]} | test_acc = {out[\"score\"]}')  # here, val accuracy is actually training accuracy, not great but is example\n",
    "        step += 1\n",
    "    return model"
   ],
   "metadata": {
    "id": "3pSKQ2wi62Br",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1acd203-e256-4f39-e059-9c7e55942a87"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# train(model, 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def matching_value(samples, predictions, partial = False, match_rest = False, opt_scipy = False):\n",
    "    features = samples.features\n",
    "    gt_matchings = samples.outputs[0].data\n",
    "    # inputs for the matrix A are at index 1 (see spec.py)\n",
    "    data = features.inputs[1].data\n",
    "    masks = features.inputs[2].data\n",
    "    pred_accuracy = 0\n",
    "\n",
    "    # Iterating over all the samples\n",
    "    for i in range(data.shape[0]):\n",
<<<<<<< HEAD
    "        max_weight = compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n",
    "\n",
    "        # TODO remove\n",
    "        predicted_matching = predictions[\"match\"].data[i]\n",
    "        # buyers_mask = masks[i]\n",
    "        # n = int(np.sum(buyers_mask))\n",
    "        # permutation = np.random.permutation(np.arange(np.sum(buyers_mask == 0)))\n",
    "        # predicted_matching = np.concatenate((np.zeros(n), permutation))\n",
=======
    "        if opt_scipy:\n",
    "            row_ind, col_ind = linear_sum_assignment(data[i], maximize = True)\n",
    "            max_weight = data[i][row_ind, col_ind].sum() / 2  #TODO why /2\n",
    "        else:\n",
    "            max_weight = compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n",
    "\n",
    "        predicted_matching = predictions[\"match\"].data[i]\n",
    "\n",
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
    "        if partial:\n",
    "            preds_weight = compute_partial_matching_weight(i, data, masks, predicted_matching)\n",
    "            print(f\"opt: {max_weight}, greedy: {preds_weight}\")\n",
    "        else:\n",
    "            preds_weight = compute_greedy_matching_weight(i, data, masks, predicted_matching, match_rest = match_rest)\n",
    "            print(f\"opt: {max_weight}, partial: {preds_weight}\")\n",
    "\n",
    "        # assert preds_weight <= max_weight\n",
    "        pred_accuracy += preds_weight / max_weight\n",
    "\n",
    "    return pred_accuracy / data.shape[0]\n",
    "\n",
    "\n",
    "def compute_greedy_matching_weight(i, data, masks, matching, match_rest = False):\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # Only consider the matching values for consumers\n",
    "    matching = np.where(goods_mask == 1, matching, -1)\n",
    "    unmatched_goods = set(range(n, n + m))\n",
    "    unmatched_buyers = set(range(n))\n",
    "\n",
    "    for buyer in range(n):\n",
    "        if buyer in matching:\n",
    "            # If several goods point to the same buyer, keep the one with maximum weight\n",
<<<<<<< HEAD
    "            candidates = A[buyer, matching == buyer]\n",
    "            matching_weight += np.max(candidates)\n",
    "            matched.remove(np.argmax(A))\n",
    "\n",
=======
    "            mask = matching == buyer\n",
    "            matching_weight += np.max(A[buyer, mask])\n",
    "            # Recovering the index of the maximum, inspired by http://seanlaw.github.io/2015/09/10/numpy-argmin-with-a-condition/\n",
    "            subset_idx = np.argmax(A[buyer, mask])\n",
    "            good = np.arange(A.shape[1])[mask][subset_idx]\n",
    "            unmatched_goods.remove(good)\n",
    "            unmatched_buyers.remove(buyer)\n",
    "\n",
    "    if match_rest and len(unmatched_goods) > 0 and len(unmatched_buyers) > 0:\n",
    "        # Compute optimal matching on the remaining unmatched nodes\n",
    "        mask = np.zeros(A.shape)\n",
    "        # TODO this is a horrible solution, there's definitely a prettier solution\n",
    "        mask[list(unmatched_buyers)] += 1\n",
    "        mask[:, list(unmatched_goods)] += 1\n",
    "        mask = np.where(mask == 2, True, False)\n",
    "        remaining_bipartite_graph = A * mask\n",
    "        row_ind, col_ind = linear_sum_assignment(remaining_bipartite_graph, maximize = True)\n",
    "        opt = A[row_ind, col_ind].sum() / 2  #TODO do I always need the division by 2\n",
    "        matching_weight += opt\n",
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
    "\n",
    "    return matching_weight\n",
    "\n",
    "\n",
    "def compute_partial_matching_weight(i, data, masks, matching):\n",
    "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
    "\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # We only care about the buyer -> good connections\n",
    "    A_submatrix = A[:n, n:n + m]\n",
    "    matching = matching[:n, n:n + m]\n",
    "\n",
    "    max_weight = np.max(np.sum(matching, axis = 0))\n",
    "    print(f\"max weight: {max_weight}\")\n",
    "    matching /= max_weight\n",
    "    return np.sum(matching * A_submatrix)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
=======
   "execution_count": 8,
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
   "outputs": [],
   "source": [
    "test_feedback = next(test_sampler)\n",
    "predictions, _ = model.predict(rng_key, test_feedback.features)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  3.,  4., 12., 28., 25.,  3.,  0.,\n",
       "        3.,  3., 12.,  3.,  5., 22.,  0., 17.,  0.,  2., 22., 13.,  9.,\n",
       "       11.,  7.,  3.,  0., 11.,  5., 11., 17., 12.,  2.,  0., 13.],      dtype=float32)"
      ]
     },
     "execution_count": 24,
=======
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt: 22.971399640350818, partial: 21.686360301576556\n",
      "opt: 22.482248980420103, partial: 20.852873603776747\n",
      "opt: 22.830546969574144, partial: 21.9357134906788\n",
      "opt: 22.186927645585627, partial: 21.042179955078307\n",
      "opt: 23.283967866663005, partial: 20.452384362047553\n",
      "opt: 23.30261156396336, partial: 22.516463202814386\n",
      "opt: 22.316747385172583, partial: 20.14835950762408\n",
      "opt: 21.271372533697722, partial: 20.05429071396947\n",
      "opt: 22.204226413683536, partial: 20.463857674957694\n",
      "opt: 22.497598270547222, partial: 21.19355969315107\n",
      "opt: 21.882106412293027, partial: 19.965108934723926\n",
      "opt: 22.749721217316576, partial: 20.609906557175375\n",
      "opt: 21.168894404430212, partial: 19.9543654297649\n",
      "opt: 21.481610153666708, partial: 20.426715270282823\n",
      "opt: 23.83249814241975, partial: 21.776489715808935\n",
      "opt: 21.25712679500731, partial: 19.996418578661746\n",
      "opt: 22.133529631646248, partial: 21.112981881308247\n",
      "opt: 22.50710606928571, partial: 20.43689004457508\n",
      "opt: 23.741077786380274, partial: 22.47351908066294\n",
      "opt: 22.830546969574144, partial: 21.9357134906788\n",
      "opt: 22.124504104524945, partial: 20.194591225271488\n",
      "opt: 21.271372533697722, partial: 20.05429071396947\n",
      "opt: 21.168894404430212, partial: 19.9543654297649\n",
      "opt: 22.497598270547222, partial: 21.19355969315107\n",
      "opt: 21.882106412293027, partial: 19.965108934723926\n",
      "opt: 22.800305582676927, partial: 20.51132114708239\n",
      "opt: 21.271372533697722, partial: 20.05429071396947\n",
      "opt: 22.222658887849995, partial: 21.167867831211865\n",
      "opt: 21.25712679500731, partial: 19.996418578661746\n",
      "opt: 23.198559061873365, partial: 22.31523644693543\n",
      "opt: 23.283967866663005, partial: 20.452384362047553\n",
      "opt: 22.971399640350818, partial: 21.686360301576556\n",
      "opt: 22.971399640350818, partial: 21.686360301576556\n",
      "opt: 22.56210784542057, partial: 20.770610671148628\n",
      "opt: 21.271372533697722, partial: 20.05429071396947\n",
      "opt: 22.133529631646248, partial: 21.112981881308247\n",
      "opt: 22.204226413683536, partial: 20.463857674957694\n",
      "opt: 22.188081505456942, partial: 21.819738541571176\n",
      "opt: 21.8022447927874, partial: 20.155956877763483\n",
      "opt: 23.215797437019365, partial: 21.55229294798405\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9340528377264027"
     },
     "execution_count": 69,
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "predictions['match'].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45, 63, 61, 42, 33, 43, 47, 53, 37, -1, 57, 58, -1, -1, 36, 62, -1,\n",
       "       40, -1, -1, -1, 38, -1, 32, -1, -1, 44, 46, 35, -1, 59, 56, 23,  4,\n",
       "       -1, 28, 14,  8, 21, -1, 17, -1,  3,  5, 26,  0, 27,  6, -1, -1, -1,\n",
       "       -1, -1,  7, -1, -1, 31, 10, 11, 30, -1,  2, 15,  1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feedback.outputs[0].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1004",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m matching_value(test_feedback, predictions, partial \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mmatching_value\u001b[0;34m(samples, predictions, partial, match_rest)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Iterating over all the samples\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m---> 11\u001b[0m     max_weight \u001b[39m=\u001b[39m compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n\u001b[1;32m     13\u001b[0m     \u001b[39m# TODO remove\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     predicted_matching \u001b[39m=\u001b[39m predictions[\u001b[39m\"\u001b[39m\u001b[39mmatch\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mdata[i]\n",
      "Cell \u001b[0;32mIn[17], line 49\u001b[0m, in \u001b[0;36mcompute_greedy_matching_weight\u001b[0;34m(i, data, masks, matching, match_rest)\u001b[0m\n\u001b[1;32m     47\u001b[0m         candidates \u001b[39m=\u001b[39m A[buyer, matching \u001b[39m==\u001b[39m buyer]\n\u001b[1;32m     48\u001b[0m         matching_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(candidates)\n\u001b[0;32m---> 49\u001b[0m         matched\u001b[39m.\u001b[39;49mremove(np\u001b[39m.\u001b[39;49margmax(A))\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m matching_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: 1004"
     ]
    }
   ],
   "source": [
    "matching_value(test_feedback, predictions, partial = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preliminary results\n",
    "random permutation/matching: 0.18\n",
    "\n",
    "MPNN:\n",
    "learned predictions: 0.67\n",
    "\n",
    "GAT:\n",
    "learned predictions: 0.72\n",
    "\n",
    "Got better with double ended predictions\n",
    "\n",
    "Partial: 0.64 while greedy was doing about 0.92 on the same instance. Main reason seems to be that max weight is around 1.5 => can get at most 2/3 OPT\n"
   ]
  },
  {
   "cell_type": "markdown",
=======
    "matching_value(test_feedback, predictions, partial = False, match_rest = False, opt_scipy = True)"
   ],
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Counting the number of matching constraints violated"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
=======
   "execution_count": 9,
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 | loss = 5.424338340759277 | val_acc = 0.015625 | test_acc = 0.0003906250058207661\n",
      "step = 10 | loss = 1.9543354511260986 | val_acc = 0.07421875 | test_acc = 0.05859375\n",
      "step = 20 | loss = 0.9110847115516663 | val_acc = 0.318359375 | test_acc = 0.3187499940395355\n",
      "step = 30 | loss = 0.6695053577423096 | val_acc = 0.365234375 | test_acc = 0.353515625\n",
      "step = 40 | loss = 0.43217840790748596 | val_acc = 0.40234375 | test_acc = 0.4007812440395355\n",
      "step = 50 | loss = 0.3719578981399536 | val_acc = 0.4921875 | test_acc = 0.423828125\n",
      "step = 60 | loss = 0.35433998703956604 | val_acc = 0.365234375 | test_acc = 0.3984375\n",
      "step = 70 | loss = 0.39364153146743774 | val_acc = 0.615234375 | test_acc = 0.539843738079071\n",
      "step = 80 | loss = 0.3526900112628937 | val_acc = 0.4296875 | test_acc = 0.45234376192092896\n",
      "step = 90 | loss = 0.2759073078632355 | val_acc = 0.62109375 | test_acc = 0.622265636920929\n",
      "opt: 1.0134400908291838, partial: 0.28481307106770365\n",
      "opt: 1.0133761638689904, partial: 0.31641393384312094\n",
      "opt: 1.013582015596471, partial: 0.28491215534906217\n",
      "opt: 1.01344247004648, partial: 0.3165243415697026\n",
      "opt: 1.0135417383054612, partial: 0.28487276300644093\n",
      "opt: 1.0135820990104134, partial: 0.31658782293829085\n",
      "opt: 1.0133761638689904, partial: 0.31641393384312094\n",
      "opt: 1.0135428342473167, partial: 0.3481583477914658\n",
      "opt: 1.0136883952070594, partial: 0.31655512597549135\n",
      "opt: 1.0130269768562536, partial: 0.28479063742502836\n",
      "opt: 1.0130269768562536, partial: 0.28479063742502836\n",
      "opt: 1.0133610744431114, partial: 0.31636533376169984\n",
      "opt: 1.013571785677675, partial: 0.31656457404026184\n",
      "opt: 1.0136883952070594, partial: 0.31655512597549135\n",
      "opt: 1.0133088177410325, partial: 0.3797319747723052\n",
      "opt: 1.0135428342473167, partial: 0.3481583477914658\n",
      "opt: 1.0134400908291838, partial: 0.28481307106770365\n",
      "opt: 1.013215452848751, partial: 0.34810291212198363\n",
      "opt: 1.0136230441162133, partial: 0.3481681636916302\n",
      "opt: 1.0130269768562536, partial: 0.28479063742502836\n",
      "opt: 1.0133088177410325, partial: 0.3797319747723052\n",
      "opt: 1.01344247004648, partial: 0.3165243415697026\n",
      "opt: 1.0136426553991087, partial: 0.34817249645902304\n",
      "opt: 1.0133088177410325, partial: 0.3797319747723052\n",
      "opt: 1.0134804510819175, partial: 0.3482075848243902\n",
      "opt: 1.0136883952070594, partial: 0.31655512597549135\n",
      "opt: 1.0132839356829426, partial: 0.37971054824955475\n",
      "opt: 1.013478898259509, partial: 0.28492432062234463\n",
      "opt: 1.013478898259509, partial: 0.28492432062234463\n",
      "opt: 1.0135428342473167, partial: 0.3481583477914658\n",
      "opt: 1.0133920317976939, partial: 0.2847697089865787\n",
      "opt: 1.0134253084103892, partial: 0.3797502875859292\n",
      "opt: 1.0134257915283924, partial: 0.3798117916837944\n",
      "opt: 1.0133959847494234, partial: 0.34826067888720125\n",
      "opt: 1.0138526291175007, partial: 0.2532023177303865\n",
      "opt: 1.013713781914852, partial: 0.3482890021545575\n",
      "opt: 1.013674290668087, partial: 0.34833785478432255\n",
      "opt: 1.013713781914852, partial: 0.3482890021545575\n",
      "opt: 1.0133959847494234, partial: 0.34826067888720125\n",
      "opt: 1.0130506574484828, partial: 0.3798192222386332\n",
      "step = 0 | loss = 5.6779046058654785 | val_acc = 0.01953125 | test_acc = 0.0\n",
      "step = 10 | loss = 1.7979671955108643 | val_acc = 0.06640625 | test_acc = 0.07695312798023224\n",
      "step = 20 | loss = 0.9590617418289185 | val_acc = 0.28515625 | test_acc = 0.2925781309604645\n",
      "step = 30 | loss = 0.6906435489654541 | val_acc = 0.375 | test_acc = 0.35859376192092896\n",
      "step = 40 | loss = 0.4581388235092163 | val_acc = 0.40625 | test_acc = 0.392578125\n",
      "step = 50 | loss = 0.4327186942100525 | val_acc = 0.421875 | test_acc = 0.3960937559604645\n",
      "step = 60 | loss = 0.36500856280326843 | val_acc = 0.46484375 | test_acc = 0.38398438692092896\n",
      "step = 70 | loss = 0.38277530670166016 | val_acc = 0.564453125 | test_acc = 0.5\n",
      "step = 80 | loss = 0.33629608154296875 | val_acc = 0.416015625 | test_acc = 0.45429688692092896\n",
      "step = 90 | loss = 0.34938740730285645 | val_acc = 0.552734375 | test_acc = 0.587890625\n",
      "opt: 74.10933727135922, partial: 69.81886662924038\n",
      "opt: 74.77415576552579, partial: 64.5109349532079\n",
      "opt: 69.96583523626748, partial: 55.35770451276817\n",
      "opt: 69.08976329969751, partial: 52.75258427166189\n",
      "opt: 79.86724593081553, partial: 66.5045231268612\n",
      "opt: 69.96583523626748, partial: 55.35770451276817\n",
      "opt: 74.22396731697327, partial: 60.8268883616501\n",
      "opt: 78.11816160327868, partial: 67.81826346694952\n",
      "opt: 85.97035446681345, partial: 75.47557191540287\n",
      "opt: 70.6143562442911, partial: 58.31471573967899\n",
      "opt: 84.43843670888806, partial: 76.42821948567746\n",
      "opt: 76.73213430108143, partial: 62.50438083776023\n",
      "opt: 80.0693964071726, partial: 59.02445805422181\n",
      "opt: 84.43843670888806, partial: 76.42821948567746\n",
      "opt: 79.89382864468956, partial: 71.61168636897246\n",
      "opt: 73.81366240034909, partial: 63.87898033730593\n",
      "opt: 74.10933727135922, partial: 69.81886662924038\n",
      "opt: 74.22396731697327, partial: 60.8268883616501\n",
      "opt: 72.16163342593347, partial: 58.89124375714695\n",
      "opt: 73.81366240034909, partial: 63.87898033730593\n",
      "opt: 69.08976329969751, partial: 52.75258427166189\n",
      "opt: 69.08976329969751, partial: 52.75258427166189\n",
      "opt: 76.1222939401651, partial: 68.54573565691713\n",
      "opt: 76.82877862450275, partial: 69.733136898949\n",
      "opt: 80.14632717033037, partial: 73.8596177081764\n",
      "opt: 70.49535507099681, partial: 60.1695561525588\n",
      "opt: 74.33461347901444, partial: 65.91754934838463\n",
      "opt: 72.16163342593347, partial: 58.89124375714695\n",
      "opt: 79.89382864468956, partial: 71.61168636897246\n",
      "opt: 79.50813393104221, partial: 64.00370360374289\n",
      "opt: 84.43843670888806, partial: 76.42821948567746\n",
      "opt: 80.35048127312143, partial: 64.73770729624692\n",
      "opt: 80.35048127312143, partial: 64.73770729624692\n",
      "opt: 75.73044132477838, partial: 61.51023557145005\n",
      "opt: 79.50813393104221, partial: 64.00370360374289\n",
      "opt: 83.46632178891338, partial: 68.54101914268111\n",
      "opt: 80.13376821852997, partial: 72.93552957650805\n",
      "opt: 72.74408215858237, partial: 61.189818753428376\n",
      "opt: 74.18826253054152, partial: 65.51189941310182\n",
      "opt: 80.14632717033037, partial: 73.8596177081764\n",
      "step = 0 | loss = 5.419983863830566 | val_acc = 0.01171875 | test_acc = 0.0\n",
      "step = 10 | loss = 2.1054933071136475 | val_acc = 0.0703125 | test_acc = 0.06601562350988388\n",
      "step = 20 | loss = 0.9134783744812012 | val_acc = 0.322265625 | test_acc = 0.3148437440395355\n",
      "step = 30 | loss = 0.6589113473892212 | val_acc = 0.359375 | test_acc = 0.3511718809604645\n",
      "step = 40 | loss = 0.417625367641449 | val_acc = 0.423828125 | test_acc = 0.3929687440395355\n",
      "step = 50 | loss = 0.45159125328063965 | val_acc = 0.572265625 | test_acc = 0.4957031309604645\n",
      "step = 60 | loss = 0.43290555477142334 | val_acc = 0.390625 | test_acc = 0.39140626788139343\n",
      "step = 70 | loss = 0.3363957405090332 | val_acc = 0.513671875 | test_acc = 0.5078125\n",
      "step = 80 | loss = 0.3331146538257599 | val_acc = 0.505859375 | test_acc = 0.48046875\n",
      "step = 90 | loss = 0.3579390347003937 | val_acc = 0.65234375 | test_acc = 0.5894531607627869\n",
      "opt: 5.171917994923815, partial: 2.6371473591904038\n",
      "opt: 5.140721781452432, partial: 2.739580916022254\n",
      "opt: 4.916755913312547, partial: 2.55253166761577\n",
      "opt: 5.163062184934401, partial: 2.7937288780633995\n",
      "opt: 5.193568980472028, partial: 2.6426321915697\n",
      "opt: 5.39895888585025, partial: 3.3803705086816325\n",
      "opt: 5.3301396987135545, partial: 2.75853092576759\n",
      "opt: 5.4902791189331985, partial: 3.0144723533869087\n",
      "opt: 5.3301396987135545, partial: 2.75853092576759\n",
      "opt: 4.967850937733685, partial: 2.044724209581272\n",
      "opt: 4.501001385816092, partial: 1.930486134565815\n",
      "opt: 5.4469035913383035, partial: 2.400494913435486\n",
      "opt: 5.518886499598197, partial: 3.0650765780805456\n",
      "opt: 5.162051654409905, partial: 1.8870284973041556\n",
      "opt: 4.816920335921645, partial: 2.133152040797292\n",
      "opt: 5.39895888585025, partial: 3.3803705086816325\n",
      "opt: 5.0662954311708095, partial: 2.428660803195243\n",
      "opt: 5.692814945341118, partial: 3.2799790567616878\n",
      "opt: 4.962977198815726, partial: 2.7019339521749735\n",
      "opt: 5.140721781452432, partial: 2.739580916022254\n",
      "opt: 4.726887960638091, partial: 2.5394175746163263\n",
      "opt: 5.020033102692548, partial: 1.657346239175731\n",
      "opt: 5.07084114812009, partial: 2.3394713324613243\n",
      "opt: 5.39895888585025, partial: 3.3803705086816325\n",
      "opt: 4.9215348760051985, partial: 2.3789687866296516\n",
      "opt: 5.645227236071927, partial: 2.716627649446909\n",
      "opt: 4.792958457559774, partial: 1.9320778813911046\n",
      "opt: 4.760143971085174, partial: 2.1750091906082027\n",
      "opt: 5.568612432062589, partial: 2.584235472918109\n",
      "opt: 5.236465970888816, partial: 2.4458144337478136\n",
      "opt: 4.726887960638091, partial: 2.5394175746163263\n",
      "opt: 5.568612432062589, partial: 2.584235472918109\n",
      "opt: 5.4902791189331985, partial: 3.0144723533869087\n",
      "opt: 4.735615975697449, partial: 2.213968425255933\n",
      "opt: 5.4469035913383035, partial: 2.400494913435486\n",
      "opt: 5.4469035913383035, partial: 2.400494913435486\n",
      "opt: 5.004454185063102, partial: 2.5236499432656685\n",
      "opt: 5.692814945341118, partial: 3.2799790567616878\n",
      "opt: 5.3301396987135545, partial: 2.75853092576759\n",
      "opt: 5.004454185063102, partial: 2.5236499432656685\n",
      "step = 0 | loss = 5.38085412979126 | val_acc = 0.01171875 | test_acc = 0.0\n",
      "step = 10 | loss = 1.882428765296936 | val_acc = 0.044921875 | test_acc = 0.04921875149011612\n",
      "step = 20 | loss = 0.9121289253234863 | val_acc = 0.2578125 | test_acc = 0.29023438692092896\n",
      "step = 30 | loss = 0.6313062310218811 | val_acc = 0.380859375 | test_acc = 0.3628906309604645\n",
      "step = 40 | loss = 0.43250465393066406 | val_acc = 0.408203125 | test_acc = 0.37187501788139343\n",
      "step = 50 | loss = 0.4007394313812256 | val_acc = 0.4296875 | test_acc = 0.4105468690395355\n",
      "step = 60 | loss = 0.24904806911945343 | val_acc = 0.373046875 | test_acc = 0.3687500059604645\n",
      "step = 70 | loss = 0.31561723351478577 | val_acc = 0.5390625 | test_acc = 0.5250000357627869\n",
      "step = 80 | loss = 0.2701014578342438 | val_acc = 0.466796875 | test_acc = 0.46015626192092896\n",
      "step = 90 | loss = 0.3131938874721527 | val_acc = 0.62109375 | test_acc = 0.5453125238418579\n",
      "opt: 206.41217292803776, partial: 151.89475268429936\n",
      "opt: 212.83165500016008, partial: 156.81011698690386\n",
      "opt: 207.95839545911463, partial: 167.7352403392384\n",
      "opt: 205.45670630777244, partial: 149.47950997380462\n",
      "opt: 207.09995336055727, partial: 145.96421602013842\n",
      "opt: 209.0110211477517, partial: 151.0389680295282\n",
      "opt: 205.8836977475487, partial: 153.64799056269516\n",
      "opt: 212.21360993462315, partial: 157.2138927848579\n",
      "opt: 208.40215029490417, partial: 153.16756983116574\n",
      "opt: 203.15832036542952, partial: 162.01152183933195\n",
      "opt: 205.66170130560695, partial: 156.65072069242285\n",
      "opt: 220.20100998164975, partial: 181.97302273384418\n",
      "opt: 209.57379789985097, partial: 165.82600325141706\n",
      "opt: 216.34806337653708, partial: 171.3518559367703\n",
      "opt: 205.66170130560695, partial: 156.65072069242285\n",
      "opt: 203.15832036542952, partial: 162.01152183933195\n",
      "opt: 207.82727304575468, partial: 165.02628728101365\n",
      "opt: 208.25741669716973, partial: 152.81307525552586\n",
      "opt: 209.57379789985097, partial: 165.82600325141706\n",
      "opt: 213.0714955984499, partial: 163.44668633638108\n",
      "opt: 220.20100998164975, partial: 181.97302273384418\n",
      "opt: 204.36719379742414, partial: 157.23296022911728\n",
      "opt: 216.6330312000564, partial: 180.75649454349787\n",
      "opt: 213.34775961737864, partial: 170.97049991216565\n",
      "opt: 214.44640136494206, partial: 157.24527215876674\n",
      "opt: 216.6330312000564, partial: 180.75649454349787\n",
      "opt: 205.45670630777244, partial: 149.47950997380462\n",
      "opt: 202.35337955964278, partial: 146.52800146761763\n",
      "opt: 209.82799316656875, partial: 184.45194869979156\n",
      "opt: 214.64232512840448, partial: 184.8171428295437\n",
      "opt: 214.44640136494206, partial: 157.24527215876674\n",
      "opt: 207.82727304575468, partial: 165.02628728101365\n",
      "opt: 209.65896599106694, partial: 171.53257024470594\n",
      "opt: 211.51148985612747, partial: 159.4349513577917\n",
      "opt: 211.51148985612747, partial: 159.4349513577917\n",
      "opt: 205.8836977475487, partial: 153.64799056269516\n",
      "opt: 209.2408918377675, partial: 172.0053687874104\n",
      "opt: 211.19121476158267, partial: 152.1589176658933\n",
      "opt: 216.6330312000564, partial: 180.75649454349787\n",
      "opt: 220.20100998164975, partial: 181.97302273384418\n",
      "step = 0 | loss = 5.359658718109131 | val_acc = 0.013671875 | test_acc = 0.0\n",
      "step = 10 | loss = 1.994330644607544 | val_acc = 0.04296875 | test_acc = 0.03867187723517418\n",
      "step = 20 | loss = 1.092348337173462 | val_acc = 0.283203125 | test_acc = 0.2984375059604645\n",
      "step = 30 | loss = 0.5207468271255493 | val_acc = 0.37890625 | test_acc = 0.3597656190395355\n",
      "step = 40 | loss = 0.4441637396812439 | val_acc = 0.400390625 | test_acc = 0.365234375\n",
      "step = 50 | loss = 0.40719181299209595 | val_acc = 0.5078125 | test_acc = 0.4625000059604645\n",
      "step = 60 | loss = 0.36441245675086975 | val_acc = 0.400390625 | test_acc = 0.3832031190395355\n",
      "step = 70 | loss = 0.3424670696258545 | val_acc = 0.494140625 | test_acc = 0.46484375\n",
      "step = 80 | loss = 0.31262680888175964 | val_acc = 0.498046875 | test_acc = 0.4964843690395355\n",
      "step = 90 | loss = 0.37911075353622437 | val_acc = 0.5546875 | test_acc = 0.49531251192092896\n",
      "opt: 567.1000844013138, partial: 423.0076166070487\n",
      "opt: 636.3620383667378, partial: 561.3914247699284\n",
      "opt: 550.8803041281844, partial: 417.07599700015896\n",
      "opt: 621.4613904224124, partial: 502.19997009408746\n",
      "opt: 620.8092473205867, partial: 477.0185639187729\n",
      "opt: 596.8874615817808, partial: 488.6506432978449\n",
      "opt: 636.3620383667378, partial: 561.3914247699284\n",
      "opt: 677.5042197113042, partial: 587.7444722735926\n",
      "opt: 649.477741197298, partial: 513.0409002385805\n",
      "opt: 600.5375277010289, partial: 516.4766800911258\n",
      "opt: 641.9562257834698, partial: 525.9496361415456\n",
      "opt: 556.4033048146143, partial: 475.973372044464\n",
      "opt: 615.5837907099865, partial: 481.0658320676802\n",
      "opt: 649.477741197298, partial: 513.0409002385805\n",
      "opt: 563.7602677896741, partial: 439.2315260152464\n",
      "opt: 543.0675157258978, partial: 446.2920880594332\n",
      "opt: 677.5042197113042, partial: 587.7444722735926\n",
      "opt: 590.0259073558741, partial: 483.39218671254065\n",
      "opt: 620.2692172554852, partial: 538.8580369621884\n",
      "opt: 620.2692172554852, partial: 538.8580369621884\n",
      "opt: 532.2841998612743, partial: 429.0700995335027\n",
      "opt: 596.3659652062838, partial: 498.1325855163696\n",
      "opt: 609.7645747019121, partial: 503.7633071001053\n",
      "opt: 556.4033048146143, partial: 475.973372044464\n",
      "opt: 615.6962844334097, partial: 497.76263673729653\n",
      "opt: 615.6962844334097, partial: 497.76263673729653\n",
      "opt: 535.6011490915334, partial: 419.36588694234376\n",
      "opt: 573.0719944743221, partial: 485.0087738178408\n",
      "opt: 615.6962844334097, partial: 497.76263673729653\n",
      "opt: 573.0719944743221, partial: 485.0087738178408\n",
      "opt: 606.6304044937697, partial: 523.3820218173197\n",
      "opt: 667.4962472810693, partial: 535.556062016912\n",
      "opt: 606.6304044937697, partial: 523.3820218173197\n",
      "opt: 653.4989366084471, partial: 507.823025410922\n",
      "opt: 573.0719944743221, partial: 485.0087738178408\n",
      "opt: 572.2544704810421, partial: 490.0042627912184\n",
      "opt: 594.7672134229347, partial: 510.14383054556225\n",
      "opt: 532.2841998612743, partial: 429.0700995335027\n",
      "opt: 606.595973720865, partial: 511.4379623007876\n",
      "opt: 596.3659652062838, partial: 498.1325855163696\n",
      "step = 0 | loss = 5.498566150665283 | val_acc = 0.01171875 | test_acc = 0.0\n",
      "step = 10 | loss = 1.9361965656280518 | val_acc = 0.03515625 | test_acc = 0.03359375149011612\n",
      "step = 20 | loss = 1.0144754648208618 | val_acc = 0.22265625 | test_acc = 0.2679687440395355\n",
      "step = 30 | loss = 0.6497214436531067 | val_acc = 0.3671875 | test_acc = 0.3492187559604645\n",
      "step = 40 | loss = 0.4522680342197418 | val_acc = 0.392578125 | test_acc = 0.3812499940395355\n",
      "step = 50 | loss = 0.45353031158447266 | val_acc = 0.451171875 | test_acc = 0.4242187440395355\n",
      "step = 60 | loss = 0.3740110695362091 | val_acc = 0.416015625 | test_acc = 0.37773439288139343\n",
      "step = 70 | loss = 0.34462201595306396 | val_acc = 0.443359375 | test_acc = 0.455078125\n",
      "step = 80 | loss = 0.2951373755931854 | val_acc = 0.501953125 | test_acc = 0.4789062440395355\n",
      "step = 90 | loss = 0.2966746985912323 | val_acc = 0.47265625 | test_acc = 0.45625001192092896\n",
      "opt: 5198.0322654492065, partial: 4447.707095602851\n",
      "opt: 5131.661044108816, partial: 4068.670312641545\n",
      "opt: 5262.0386517107145, partial: 4429.715595889013\n",
      "opt: 5099.618805545832, partial: 4370.421723249921\n",
      "opt: 4525.112399086971, partial: 3484.283783911301\n",
      "opt: 5079.123313497003, partial: 3947.1917528293966\n",
      "opt: 5150.866785686584, partial: 4219.083172590534\n",
      "opt: 5298.1937006703065, partial: 4426.016425477421\n",
      "opt: 5327.623277615265, partial: 4283.915380142394\n",
      "opt: 4933.453319280748, partial: 4055.69757893462\n",
      "opt: 4933.453319280748, partial: 4055.69757893462\n",
      "opt: 5049.686826972309, partial: 4268.9328397315985\n",
      "opt: 4430.053964582706, partial: 3270.741936184058\n",
      "opt: 5099.618805545832, partial: 4370.421723249921\n",
      "opt: 5040.120177048144, partial: 3772.978455916111\n",
      "opt: 5262.0386517107145, partial: 4429.715595889013\n",
      "opt: 5131.661044108816, partial: 4068.670312641545\n",
      "opt: 4896.943054686468, partial: 3861.1597648117945\n",
      "opt: 5564.548275433557, partial: 4277.480189770089\n",
      "opt: 4525.112399086971, partial: 3484.283783911301\n",
      "opt: 5127.596618082271, partial: 4080.990293024376\n",
      "opt: 5198.0322654492065, partial: 4447.707095602851\n",
      "opt: 4896.943054686468, partial: 3861.1597648117945\n",
      "opt: 5087.107246596086, partial: 3946.982145918584\n",
      "opt: 4933.0005965641585, partial: 3829.4167779072495\n",
      "opt: 5688.39620666956, partial: 4479.9520290226765\n",
      "opt: 4992.703002532555, partial: 3529.1317560523703\n",
      "opt: 5198.0322654492065, partial: 4447.707095602851\n",
      "opt: 5127.596618082271, partial: 4080.990293024376\n",
      "opt: 5327.623277615265, partial: 4283.915380142394\n",
      "opt: 4896.943054686468, partial: 3861.1597648117945\n",
      "opt: 5049.686826972309, partial: 4268.9328397315985\n",
      "opt: 5150.126694359203, partial: 4436.52318011652\n",
      "opt: 4992.703002532555, partial: 3529.1317560523703\n",
      "opt: 5327.623277615265, partial: 4283.915380142394\n",
      "opt: 4933.0005965641585, partial: 3829.4167779072495\n",
      "opt: 5149.90268863488, partial: 4134.267625668784\n",
      "opt: 5131.661044108816, partial: 4068.670312641545\n",
      "opt: 5327.623277615265, partial: 4283.915380142394\n",
      "opt: 5298.1937006703065, partial: 4426.016425477421\n"
     ]
    },
    {
     "data": {
      "text/plain": "[({'low': 0, 'high': 0.001}, {'low': 0, 'high': 0.001}, 0.323240070947223),\n ({'low': 1, 'high': 1.001}, {'low': 1, 'high': 1.001}, 0.8491779489208051),\n ({'low': 0, 'high': 0.1}, {'low': 0, 'high': 0.1}, 0.4984568560485999),\n ({'low': 5, 'high': 1}, {'low': 5, 'high': 1}, 0.7766011202708257),\n ({'low': 5, 'high': 10}, {'low': 5, 'high': 10}, 0.8256278336354737),\n ({'low': 5, 'high': 100}, {'low': 5, 'high': 100}, 0.8027066643325751)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# For two-way\n",
    "count = 0\n",
    "data = predictions[\"owners\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    for i in range(32):\n",
    "        owner = data[datapoint][i]\n",
    "        good = data[datapoint][int(owner)]\n",
    "        if good != i:\n",
    "            count += 1\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
=======
    "def variation_testing(train_params, test_params, model = None, bypass_training = False):\n",
    "    if model is None and bypass_training:\n",
    "        print(\"Need a model to bypass training\")\n",
    "        return\n",
    "\n",
    "    matching_values = []\n",
    "    for train_param, test_param in zip(train_params, test_params):\n",
    "        train_sampler, spec = samplers(100, 16, 32, **train_param)\n",
    "        test_sampler, _ = samplers(40, 64, 40, **test_param)\n",
    "\n",
    "        if not bypass_training:\n",
    "            model = define_model(spec, train_sampler, model=\"mpnn\")\n",
    "            train(model, 100)\n",
    "        else:\n",
    "            print(\"Bypassing training\")\n",
    "\n",
    "        test_feedback = next(test_sampler)\n",
    "        predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "        accuracy = matching_value(test_feedback, predictions, partial = False, match_rest = False, opt_scipy = True)\n",
    "\n",
    "        matching_values.append((train_param, test_param, accuracy))\n",
    "    return model, matching_values\n",
    "\n",
    "weight_params = [{\"low\": 0, \"high\": 0.001},\n",
    "                 {\"low\": 1, \"high\": 1.001},\n",
    "                 {\"low\": 0, \"high\": 0.1},\n",
    "                 {\"low\": 0, \"high\": 1},\n",
    "                 {\"low\": 0, \"high\": 10},\n",
    "                 {\"low\": 0, \"high\": 100},\n",
    "                 {\"low\": 50, \"high\": 200},\n",
    "                 {\"low\": 500, \"high\": 2000},\n",
    "                 {\"low\": 5000, \"high\": 20000}\n",
    "                 ]\n",
    "\n",
    "weight_params = [{\"low\": 0, \"high\": 0.001},\n",
    "                 {\"low\": 1, \"high\": 1.001},\n",
    "                 {\"low\": 0, \"high\": 0.1},\n",
    "                 {\"low\": 5, \"high\": 1},\n",
    "                 {\"low\": 5, \"high\": 10},\n",
    "                 {\"low\": 5, \"high\": 100},\n",
    "                 ]\n",
    "\n",
    "model, results = variation_testing(weight_params, weight_params, model=model, bypass_training = False)\n",
    "\n",
    "results\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
   "outputs": [
    {
     "data": {
      "text/plain": "<clrs._src.baselines.BaselineModel at 0x7f8aa0566da0>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
<<<<<<< HEAD
    "# For self-loops\n",
    "count = 0\n",
    "data = predictions[\"owners\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    owners = set(np.array(data[datapoint][32:64]))\n",
    "    count += 32 - len(owners)\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
=======
    "ER p=0.25, 100 8x8 train and 40 32x32 test => 0.94 in 100 iterations\n",
    "\n",
    "BA param=3, 100 8x8 train and 40 32x32 test => 0.97 in 100 iterations\n",
    "\n",
    "BA param=5, 100 8x8 train and 40 32x32 test => 0.95 in 100 iterations (0.951 in 200 so has pretty much converged after 100)\n",
    "\n",
    "BA param=7, 100 8x8 train and 40 32x32 test => 0.946 in 100 iterations\n",
    "\n",
    "#### Cross training\n",
    "BA param=7 to BA param=3\n",
    "\n",
    "BA param=7 to ER p=0.25 0.946 with BA to 0.939 with ER (same as if trained only on BA)\n",
    "\n",
    "ER p=0.25 to BA param=3 went from 0.939 with ER to 0.967 with BA (BA param 3 was 0.97 so basically nothing lost)\n",
    "\n",
    "#### Weight variations\n",
    "Uniform\n",
    "* 0,0.001 -> 0.928\n",
    "* 1,1.001 -> 0.962\n",
    "* 0,0.1 -> 0.931\n",
    "* 0,10 -> 0.883\n",
    "* 0,100 -> 0.77\n",
    "* 50, 200 -> 0.72\n",
    "* 500, 2000 -> 0.69\n",
    "* 5000, 20000 -> 0.7\n",
    "\n",
    "Normal:\n",
    "Basically same.\n",
    "\n",
    "#### Weight cross training\n",
    "Train ER p=0.25 unif 0,1:\n",
    "* 0,0.001 -> 0.948\n",
    "* 1,1.001 -> 0.967\n",
    "* 0,0.1 -> 0.916\n",
    "* 0,10 -> 0.86\n",
    "* 0,100 -> 0.75\n",
    "* 50, 200 -> 0.72\n",
    "* 500, 2000 -> 0.72\n",
    "* 5000, 20000 -> 0.69\n",
    "\n",
    "=> Seems to weight generalize quite well. Actually even better because basically no statistical difference with if we trained separately.\n",
    "\n",
    "Train normal 5000, 20000:\n",
    "* 0, 0.001 -> 0.39 (maybe it's the large to small that was a problem here? Also those values make little sense for a normal RV)\n",
    "\n",
    "Other direction train normal 0, 0.001 (got to 0.78):\n",
    "* 5000, 20000 -> 0.76\n",
    "\n",
    "=> small to large seems better\n",
    "\n",
    "\n",
    "#### Larger graphs\n",
    "Same training\n",
    "ER p=0.25 8x8 train:\n",
    "* 100x100 test goes to 0.88\n",
    "* 200x200 goes to 0.63 (only 12 prediction mismatches though)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
   "outputs": [
    {
     "data": {
      "text/plain": "Array([34., 42., 59., 43., 45., 49., 61., 34., 35., 48., 35., 36., 38.,\n       55., 50., 33., 39., 42., 54., 36., 39., 39., 52., 62., 44., 34.,\n       53., 39., 59., 56., 58., 63.,  5., 15., 25.,  8., 15., 20., 12.,\n       21., 28.,  1., 17.,  3., 25.,  4., 14., 11.,  9.,  5., 13., 21.,\n       22., 26., 18., 13., 29., 30., 30.,  2., 30.,  6., 20.,  7.],      dtype=float32)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "predictions, hints = model.predict(rng_key, test_feedback.features, return_hints = True, return_all_outputs = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 64)"
      ]
=======
    "predictions['match'].data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "array([49, 38, 46, 43, -1, -1, 50, -1, 44, -1, 53, -1, 51, 58, -1, 47, 54,\n       40, 52, -1, 32, -1, 35, 42, -1, 55, 48, -1, 34, 62, 39, 37, 20, -1,\n       28, 22, -1, 31,  1, 30, 17, -1, 23,  3,  8, -1,  2, 15, 26,  0,  6,\n       12, 18, 10, 16, 25, -1, -1, 13, -1, -1, -1, 29, -1])"
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "predictions['owners'].data[::100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (118648624.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "np.argmax(hints[10]['owners_h'][0], axis =\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "arr[np.arange(len(arr)) % 2 == 0]\n",
    "arr[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feedback.features.inputs[1].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = np.array([[1,2],[2,3]])\n",
    "np.max(arr, axis = 0)"
   ]
=======
    "test_feedback.outputs[0].data[0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminary results\n",
    "random permutation/matching: 0.18\n",
    "\n",
    "MPNN:\n",
    "learned predictions: 0.67\n",
    "\n",
    "GAT:\n",
    "learned predictions: 0.72\n",
    "\n",
    "Got better with double ended predictions\n",
    "\n",
    "Partial: 0.64 while greedy was doing about 0.92 on the same instance. Main reason seems to be that max weight is around 1.5 => can get at most 2/3 OPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting the number of matching constraints violated"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 12.2\n"
     ]
    }
   ],
   "source": [
    "# For two-way\n",
    "count = 0\n",
    "data = predictions[\"match\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    for i in range(32):\n",
    "        owner = data[datapoint][i]\n",
    "        good = data[datapoint][int(owner)]\n",
    "        if good != i:\n",
    "            count += 1\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 0.4\n"
     ]
    }
   ],
   "source": [
    "# For self-loops\n",
    "count = 0\n",
    "data = predictions[\"match\"].data\n",
    "nb_graphs = data.shape[0]\n",
    "for datapoint in range(data.shape[0]):\n",
    "    owners = set(np.array(data[datapoint][32:64]))\n",
    "    count += 32 - len(owners)\n",
    "print(f\"average number of edges contradicting matching: {count / nb_graphs}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "b = np.array([2, 3])\n",
    "print(np.concatenate((a, b)))"
   ],
   "metadata": {
    "collapsed": false
   }
>>>>>>> 1bb855af530d3d3fae7e58c6e3d79a4e220c5ddc
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
