{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MzxRB1X7hRs",
    "outputId": "eee4d47e-b689-497b-e9b5-021121cac2b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 00:08:50.257262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/anders/miniconda3/envs/clrs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import clrs\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2 ** 32))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"# =================== BIPARTITE GRAPH GENERATOR SPECS =========================\\n\",\n",
    "#   \"# - Erdos Reyni (ER)\\n\",\n",
    "#   \"#     Generates an Erdos-Reyni random graph with edge probability [p]. If\\n\",\n",
    "#   \"#       [weighted], edge weights are iid uniform([low], [high])\\n\",\n",
    "#   \"#\\n\",\n",
    "#   \"#     {'generator': ER, 'weighted': False, 'p': 0.5, 'low': 0.0, 'high': 1.0}\\n\",\n",
    "#   \"#\\n\",\n",
    "#   \"# - Barabasi-Albert (BA)\\n\",\n",
    "#   \"#     Generates a Barabasi-Albert random graph with parameter [ba_param]. If\\n\",\n",
    "#   \"#       [weighted], edge weights are iid uniform([low], [high])\\n\",\n",
    "#   \"#\\n\",\n",
    "#   \"#     {'generator': BA, 'ba_param': 1, 'weighted': False, 'low': 0.0, 'high': 1.0}\\n\",\n",
    "#   \"#\\n\",\n",
    "#   \"# - Geometric (GEOMETRIC)\\n\",\n",
    "#   \"#     Generates a random graph by embedding nodes uniformly over the unit\\n\",\n",
    "#   \"#       square. Edge weights are the euclidean distance between two nodes,\\n\",\n",
    "#   \"#       with weights below [threshold] set to 0, then scaled by [scaling].\\n\",\n",
    "#   \"#\\n\",\n",
    "#   \"#     {'generator': GEOMETRIC, 'threshold': 0.25, 'scaling': 1.0}\\n\",\n",
    "#   \"#\\n\",\n",
    "#   \"# - Flow (FLOW)\\n\",\n",
    "#   \"#     Generates a random ER reduction to a max flow input, with edge\\n\",\n",
    "#   \"#       probability [p].\\n\",\n",
    "#   \"#     {'generator': FLOW, 'p': 0.5}\\n\",\n",
    "#   \"\\n\",\n",
    "#   \"\\n\",\n",
    "#   \"\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEo_Gj1j3Z6M",
    "outputId": "5ef53c51-9bc2-4a49-b4cf-592a32dfa5b7"
   },
   "outputs": [],
   "source": [
    "# If you don't want BipartiteMatching, just pass empty generator list and\n",
    "# length separately\n",
    "\n",
    "sampler_spec = {\n",
    "    'num_samples': 100,\n",
    "    'batch_size': 32,\n",
    "    'schematics': [\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 0.5,\n",
    "            'length': 16,\n",
    "            'kwargs': {'p': 1, 'low': 5, 'high': 100, 'weighted': True}\n",
    "        },\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 0.5,\n",
    "            'length': 16,\n",
    "            'kwargs': {'p': 1, 'low': 1, 'high': 1.0001, 'weighted': True}\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "def samplers(sampler_spec, batch_size, **kwargs):\n",
    "\n",
    "    def _iterate_sampler(sampler, batch_size):\n",
    "        while True:\n",
    "            yield sampler.next(batch_size)\n",
    "\n",
    "    sampler, spec = clrs.build_sampler(\n",
    "        name = 'simplified_min_sum',\n",
    "        sampler_spec = sampler_spec,\n",
    "        **kwargs)  # number of nodes\n",
    "\n",
    "    sampler = _iterate_sampler(sampler, batch_size = batch_size)\n",
    "    return sampler, spec\n",
    "\n",
    "train_sampler, spec = samplers(sampler_spec, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L-p0jOCq5sPV"
   },
   "outputs": [],
   "source": [
    "def define_model(spec, train_sampler, model = \"mpnn\"):\n",
    "    if model == \"mpnn\":\n",
    "        processor_factory = clrs.get_processor_factory('mpnn', use_ln = True,\n",
    "                                                   nb_triplet_fts = 0)  #use_ln => use layer norm\n",
    "    elif model == \"gat\":\n",
    "        processor_factory = clrs.get_processor_factory('gat', use_ln=True, nb_heads = 4, nb_triplet_fts = 0)\n",
    "\n",
    "    model_params = dict(\n",
    "        processor_factory = processor_factory,  # contains the processor_factory\n",
    "        hidden_dim = 32,  # TODO put back to 32 if no difference\n",
    "        encode_hints = True,\n",
    "        decode_hints = True,\n",
    "        #decode_diffs=False,\n",
    "        #hint_teacher_forcing_noise=1.0,\n",
    "        hint_teacher_forcing = 1.0,\n",
    "        use_lstm = False,\n",
    "        learning_rate = 0.001,\n",
    "        checkpoint_path = '/tmp/checkpt',\n",
    "        freeze_processor = False,  # Good for post step\n",
    "        dropout_prob = 0.5,\n",
    "        # nb_msg_passing_steps=3,\n",
    "    )\n",
    "\n",
    "    dummy_trajectory = next(train_sampler)  # jax needs a trajectory that is plausible looking to init\n",
    "\n",
    "    model = clrs.models.BaselineModel(\n",
    "        spec = spec,\n",
    "        dummy_trajectory = dummy_trajectory,\n",
    "        **model_params\n",
    "    )\n",
    "\n",
    "    model.init(dummy_trajectory.features, 1234)  # 1234 is a random seed\n",
    "\n",
    "    return model\n",
    "\n",
    "model = define_model(spec, train_sampler, \"mpnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# No evaluation since we are postprocessing with soft: TO CHANGE -> baselines.py line 336 outs change hard to False\n",
    "# step = 0\n",
    "#\n",
    "# while step <= 1:\n",
    "#     feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "#     rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "#     cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "#     rng_key = new_rng_key\n",
    "#     if step % 10 == 0:\n",
    "#         print(step)\n",
    "#     step += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pSKQ2wi62Br",
    "outputId": "b1acd203-e256-4f39-e059-9c7e55942a87"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, train_sampler, test_sampler):\n",
    "    step = 0\n",
    "    rng_key = jax.random.PRNGKey(rng.randint(2 ** 32))\n",
    "    while step < epochs:\n",
    "        feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "        # TODO remove - testing if uses hints on tests\n",
    "        # shape = test_feedback.features.hints[0].data[0].shape\n",
    "        # test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\n",
    "        rng_key, new_rng_key = jax.random.split(rng_key)  # jax needs new random seed at step\n",
    "        cur_loss = model.feedback(rng_key, feedback)  # loss is contained in model somewhere\n",
    "        rng_key = new_rng_key\n",
    "        if step % 10 == 0:\n",
    "            predictions_val, _ = model.predict(rng_key, feedback.features)\n",
    "            out_val = clrs.evaluate(feedback.outputs, predictions_val)\n",
    "            predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "            out = clrs.evaluate(test_feedback.outputs, predictions)\n",
    "            print(\n",
    "                f'step = {step} | loss = {cur_loss} | val_acc = {out_val[\"score\"]} | test_acc = {out[\"score\"]}')  # here, val accuracy is actually training accuracy, not great but is example\n",
    "        step += 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def matching_value(samples, predictions, partial = False, match_rest = False, opt_scipy = False):\n",
    "    features = samples.features\n",
    "    gt_matchings = samples.outputs[0].data\n",
    "    # inputs for the matrix A are at index 1 (see spec.py)\n",
    "    data = features.inputs[1].data\n",
    "    masks = features.inputs[3].data\n",
    "    pred_accuracy = 0\n",
    "\n",
    "    # Iterating over all the samples\n",
    "    for i in range(data.shape[0]):\n",
    "        if opt_scipy:\n",
    "            row_ind, col_ind = linear_sum_assignment(data[i], maximize = True)\n",
    "            max_weight = data[i][row_ind, col_ind].sum() / 2  #TODO why /2\n",
    "            #print(max_weight, row_ind, col_ind)\n",
    "        else:\n",
    "            max_weight = compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n",
    "\n",
    "        predicted_matching = predictions[\"match\"].data[i]\n",
    "\n",
    "        if partial:\n",
    "            preds_weight = compute_partial_matching_weight(i, data, masks, predicted_matching)\n",
    "            print(f\"opt: {max_weight}, greedy: {preds_weight}\")\n",
    "        else:\n",
    "            preds_weight = compute_greedy_matching_weight(i, data, masks, predicted_matching, match_rest = match_rest)\n",
    "            print(f\"opt: {max_weight}, partial: {preds_weight}\")\n",
    "\n",
    "        # assert preds_weight <= max_weight\n",
    "        pred_accuracy += preds_weight / max_weight\n",
    "\n",
    "    return pred_accuracy / data.shape[0]\n",
    "\n",
    "\n",
    "def compute_greedy_matching_weight(i, data, masks, matching, match_rest = False):\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # Only consider the matching values for consumers\n",
    "    matching = np.where(goods_mask == 1, matching, -1)\n",
    "    unmatched_goods = set(range(n, n + m))\n",
    "    unmatched_buyers = set(range(n))\n",
    "\n",
    "    for buyer in range(n):\n",
    "        if buyer in matching:\n",
    "            # If several goods point to the same buyer, keep the one with maximum weight\n",
    "            mask = matching == buyer\n",
    "            matching_weight += np.max(A[buyer, mask])\n",
    "            # Recovering the index of the maximum, inspired by http://seanlaw.github.io/2015/09/10/numpy-argmin-with-a-condition/\n",
    "            subset_idx = np.argmax(A[buyer, mask])\n",
    "            good = np.arange(A.shape[1])[mask][subset_idx]\n",
    "            unmatched_goods.remove(good)\n",
    "            unmatched_buyers.remove(buyer)\n",
    "\n",
    "    if match_rest and len(unmatched_goods) > 0 and len(unmatched_buyers) > 0:\n",
    "        # Compute optimal matching on the remaining unmatched nodes\n",
    "        mask = np.zeros(A.shape)\n",
    "        # TODO this is a horrible solution, there's definitely a prettier solution\n",
    "        mask[list(unmatched_buyers)] += 1\n",
    "        mask[:, list(unmatched_goods)] += 1\n",
    "        mask = np.where(mask == 2, True, False)\n",
    "        remaining_bipartite_graph = A * mask\n",
    "        row_ind, col_ind = linear_sum_assignment(remaining_bipartite_graph, maximize = True)\n",
    "        opt = A[row_ind, col_ind].sum() / 2  #TODO do I always need the division by 2\n",
    "        matching_weight += opt\n",
    "\n",
    "    return matching_weight\n",
    "\n",
    "\n",
    "def compute_partial_matching_weight(i, data, masks, matching):\n",
    "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
    "\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # We only care about the buyer -> good connections\n",
    "    A_submatrix = A[:n, n:n + m]\n",
    "    matching = matching[:n, n:n + m]\n",
    "\n",
    "    max_weight = np.max(np.sum(matching, axis = 0))\n",
    "    print(f\"max weight: {max_weight}\")\n",
    "    matching /= max_weight\n",
    "    return np.sum(matching * A_submatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 | loss = 5.61204195022583 | val_acc = 0.009765625 | test_acc = 0.0\n",
      "step = 10 | loss = 2.0707576274871826 | val_acc = 0.009765625 | test_acc = 0.009765625\n",
      "step = 20 | loss = 0.9331364035606384 | val_acc = 0.236328125 | test_acc = 0.22226563096046448\n",
      "step = 30 | loss = 0.5320643782615662 | val_acc = 0.294921875 | test_acc = 0.32421875\n",
      "step = 40 | loss = 0.35010239481925964 | val_acc = 0.44921875 | test_acc = 0.37421876192092896\n",
      "step = 50 | loss = 0.31805557012557983 | val_acc = 0.298828125 | test_acc = 0.4566406309604645\n",
      "step = 60 | loss = 0.36847934126853943 | val_acc = 0.623046875 | test_acc = 0.34375\n",
      "step = 70 | loss = 0.22498081624507904 | val_acc = 0.66796875 | test_acc = 0.6429687738418579\n",
      "step = 80 | loss = 0.22404323518276215 | val_acc = 0.6484375 | test_acc = 0.606249988079071\n",
      "step = 90 | loss = 0.242916077375412 | val_acc = 0.642578125 | test_acc = 0.59765625\n",
      "opt: 26.136921539304925, partial: 24.342402954919613\n",
      "opt: 27.664400340876504, partial: 24.806261017452268\n",
      "opt: 25.778835418144652, partial: 23.77411152247599\n",
      "opt: 26.136921539304925, partial: 24.342402954919613\n",
      "opt: 26.97884553537703, partial: 24.53768921191668\n",
      "opt: 26.168991301747, partial: 25.25628024407805\n",
      "opt: 25.254941013046423, partial: 22.955505747868855\n",
      "opt: 26.136921539304925, partial: 24.342402954919613\n",
      "opt: 25.169107979443467, partial: 22.680524772044574\n",
      "opt: 25.010242249448346, partial: 22.304702166065752\n",
      "opt: 25.918668760838585, partial: 23.880724396489853\n",
      "opt: 26.213131833122272, partial: 24.34979889861373\n",
      "opt: 24.06782780888245, partial: 22.583361405165157\n",
      "opt: 25.918668760838585, partial: 23.880724396489853\n",
      "opt: 25.89302729120876, partial: 23.185878026689636\n",
      "opt: 25.390207783328094, partial: 23.121289539382012\n",
      "opt: 25.41427777207018, partial: 23.49820821880278\n",
      "opt: 25.63365231768958, partial: 24.543257087360207\n",
      "opt: 25.390207783328094, partial: 23.121289539382012\n",
      "opt: 24.99246149246058, partial: 23.28662276303153\n",
      "opt: 25.89302729120876, partial: 23.185878026689636\n",
      "opt: 26.136921539304925, partial: 24.342402954919613\n",
      "opt: 25.63365231768958, partial: 24.543257087360207\n",
      "opt: 26.715795645332165, partial: 26.033160676001913\n",
      "opt: 27.664400340876504, partial: 24.806261017452268\n",
      "opt: 24.06782780888245, partial: 22.583361405165157\n",
      "opt: 24.759612666914485, partial: 23.029042936306496\n",
      "opt: 27.664400340876504, partial: 24.806261017452268\n",
      "opt: 25.254941013046423, partial: 22.955505747868855\n",
      "opt: 25.778835418144652, partial: 23.77411152247599\n",
      "opt: 25.778835418144652, partial: 23.77411152247599\n",
      "opt: 26.554238147888555, partial: 23.722623828987693\n",
      "opt: 26.080036751415477, partial: 24.484927090082888\n",
      "opt: 27.664400340876504, partial: 24.806261017452268\n",
      "opt: 25.778835418144652, partial: 23.77411152247599\n",
      "opt: 26.554238147888555, partial: 23.722623828987693\n",
      "opt: 25.89302729120876, partial: 23.185878026689636\n",
      "opt: 24.80727150640645, partial: 23.91906326042252\n",
      "opt: 25.169107979443467, partial: 22.680524772044574\n",
      "opt: 25.841203764210945, partial: 24.055970172504008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9210022602295075"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def heterogenous_testing(train_sample_spec, test_sample_spec, epochs = 100, model = None, bypass_training = False):\n",
    "    if model is None and bypass_training:\n",
    "        print(\"Need a model to bypass training\")\n",
    "        return\n",
    "\n",
    "    matching_values = []\n",
    "    train_sampler, spec = samplers(train_sample_spec, train_sample_spec['batch_size'])\n",
    "    test_sampler, _ = samplers(test_sample_spec, test_sample_spec['batch_size'])\n",
    "\n",
    "    if not bypass_training:\n",
    "        model = define_model(spec, train_sampler, model=\"mpnn\")\n",
    "        train(model, 100, train_sampler, test_sampler)\n",
    "    else:\n",
    "        print(\"Bypassing training\")\n",
    "\n",
    "    test_feedback = next(test_sampler)\n",
    "    #adj = test_feedback[0][0][1].data\n",
    "    #print(adj)\n",
    "    predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "    accuracy = matching_value(test_feedback, predictions, partial = False, match_rest = True, opt_scipy = True)\n",
    "\n",
    "    return model, accuracy, predictions, test_feedback.features\n",
    "\n",
    "\n",
    "\n",
    "train_sampler_spec = {\n",
    "    'num_samples': 100,\n",
    "    'batch_size': 32,\n",
    "    'schematics': [\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 1,\n",
    "            'length': 16,\n",
    "            'kwargs': {'p': 0.25, 'low': 0, 'high': 1, 'weighted': True}\n",
    "        },\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 0,\n",
    "            'length': 16,\n",
    "            'kwargs': {'p': 0.5, 'low': 1, 'high': 1.0001, 'weighted': True}\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "test_sampler_spec = {\n",
    "    'num_samples': 40,\n",
    "    'batch_size': 40,\n",
    "    'schematics': [\n",
    "        {\n",
    "            'generator': 'ER',\n",
    "            'proportion': 1,\n",
    "            'length': 64,\n",
    "            'kwargs': {'p': 0.25, 'low': 0, 'high': 1, 'weighted': True}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "model, results, preds, x = heterogenous_testing(\n",
    "    train_sampler_spec,\n",
    "    test_sampler_spec,\n",
    "    bypass_training = False\n",
    ")\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 | loss = 4.910767555236816 | val_acc = 0.005859375 | test_acc = 0.0\n",
      "step = 10 | loss = 2.28489351272583 | val_acc = 0.08984375 | test_acc = 0.0234375\n",
      "step = 20 | loss = 1.2414355278015137 | val_acc = 0.21875 | test_acc = 0.02773437462747097\n",
      "step = 30 | loss = 0.7597364783287048 | val_acc = 0.255859375 | test_acc = 0.05429687723517418\n",
      "step = 40 | loss = 0.5549033880233765 | val_acc = 0.265625 | test_acc = 0.05156249925494194\n",
      "step = 50 | loss = 0.40181857347488403 | val_acc = 0.345703125 | test_acc = 0.06953125447034836\n",
      "step = 60 | loss = 0.3417959213256836 | val_acc = 0.3359375 | test_acc = 0.07460937649011612\n",
      "step = 70 | loss = 0.3065289556980133 | val_acc = 0.349609375 | test_acc = 0.06718750298023224\n",
      "step = 80 | loss = 0.284324049949646 | val_acc = 0.3359375 | test_acc = 0.06210937723517418\n",
      "step = 90 | loss = 0.2781578004360199 | val_acc = 0.35546875 | test_acc = 0.05859375\n",
      "opt: 23.079858381635592, partial: 21.265089693048367\n",
      "opt: 23.079858381635592, partial: 21.265089693048367\n",
      "opt: 23.600560698481686, partial: 21.687251861420734\n",
      "opt: 24.63673217304494, partial: 21.70916759559742\n",
      "opt: 25.146011877281104, partial: 22.58045417923822\n",
      "opt: 24.138511032778034, partial: 22.928422739045622\n",
      "opt: 22.794268815567662, partial: 21.163725774398095\n",
      "opt: 24.618331244995826, partial: 22.65721493796653\n",
      "opt: 24.19261232657256, partial: 22.55166901461699\n",
      "opt: 25.968563358043568, partial: 24.02688620567126\n",
      "opt: 24.286138228363786, partial: 22.33026987199831\n",
      "opt: 24.768779824135514, partial: 22.097422287276075\n",
      "opt: 23.079858381635592, partial: 21.265089693048367\n",
      "opt: 25.968563358043568, partial: 24.02688620567126\n",
      "opt: 24.637278093868954, partial: 23.06893248820558\n",
      "opt: 25.18918424852101, partial: 23.930296942344995\n",
      "opt: 24.412872463579266, partial: 21.594094044439565\n",
      "opt: 23.33167436591462, partial: 22.108753057302735\n",
      "opt: 23.812117930991274, partial: 23.112921231512964\n",
      "opt: 23.85415503341212, partial: 22.69968539430703\n",
      "opt: 23.079858381635592, partial: 21.265089693048367\n",
      "opt: 24.618331244995826, partial: 22.65721493796653\n",
      "opt: 23.079858381635592, partial: 21.265089693048367\n",
      "opt: 25.307640749438406, partial: 23.026667842164315\n",
      "opt: 24.412872463579266, partial: 21.594094044439565\n",
      "opt: 24.42108333138229, partial: 22.506669781204252\n",
      "opt: 24.41627043227835, partial: 21.753301044062216\n",
      "opt: 23.54259595800665, partial: 22.464525372913158\n",
      "opt: 22.94846063809144, partial: 20.927745002383688\n",
      "opt: 24.412872463579266, partial: 21.594094044439565\n",
      "opt: 25.50438800921642, partial: 23.2390896545407\n",
      "opt: 23.54259595800665, partial: 22.464525372913158\n",
      "opt: 23.459401815089315, partial: 21.005222858542588\n",
      "opt: 25.50438800921642, partial: 23.2390896545407\n",
      "opt: 24.412872463579266, partial: 21.594094044439565\n",
      "opt: 25.50438800921642, partial: 23.2390896545407\n",
      "opt: 24.286138228363786, partial: 22.33026987199831\n",
      "opt: 23.459401815089315, partial: 21.005222858542588\n",
      "opt: 24.443411805473808, partial: 22.328942542764207\n",
      "opt: 23.079858381635592, partial: 21.265089693048367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'generator': 'GEOMETRIC', 'train': 16, 'test': 64},\n",
       "  {'generator': 'GEOMETRIC', 'train': 16, 'test': 64},\n",
       "  0.9183484967662874)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def variation_testing(train_params, test_params, epochs = 100, model = None, bypass_training = False):\n",
    "    if model is None and bypass_training:\n",
    "        print(\"Need a model to bypass training\")\n",
    "        return\n",
    "\n",
    "    matching_values = []\n",
    "    for train_param, test_param in zip(train_params, test_params):\n",
    "        train_sampler, spec = samplers(100, train_param['train'], 32, **train_param)\n",
    "        test_sampler, _ = samplers(40, test_param['test'], 40, **test_param)\n",
    "\n",
    "        if not bypass_training:\n",
    "            model = define_model(spec, train_sampler, model=\"mpnn\")\n",
    "            train(model, 100, train_sampler, test_sampler)\n",
    "        else:\n",
    "            print(\"Bypassing training\")\n",
    "\n",
    "        test_feedback = next(test_sampler)\n",
    "        #adj = test_feedback[0][0][1].data\n",
    "        #print(adj)\n",
    "        predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "        accuracy = matching_value(test_feedback, predictions, partial = False, match_rest = True, opt_scipy = True)\n",
    "\n",
    "        matching_values.append((train_param, test_param, accuracy))\n",
    "    return model, matching_values, predictions, test_feedback.features\n",
    "weight_params = [{\"low\": 0, \"high\": 0.001},\n",
    "                 {\"low\": 1, \"high\": 1.001},\n",
    "                 {\"low\": 0, \"high\": 0.1},\n",
    "                 {\"low\": 0, \"high\": 1},\n",
    "                 {\"low\": 0, \"high\": 10},\n",
    "                 {\"low\": 0, \"high\": 100},\n",
    "                 {\"low\": 50, \"high\": 200},\n",
    "                 {\"low\": 500, \"high\": 2000},\n",
    "                 {\"low\": 5000, \"high\": 20000}\n",
    "                 ]\n",
    "\n",
    "params = [\n",
    "    {\"generator\": \"GEOMETRIC\", 'train': 16, 'test': 64}\n",
    "]\n",
    "\n",
    "# params = [\n",
    "#     {\"generator\": \"DATASET\", \"filepath\": 'data/gmission_edges.txt', 'train': 16, 'test': 16}\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model, results, preds, x = variation_testing(params, params, bypass_training = False)\n",
    "\n",
    "results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n",
      "193.7541\n"
     ]
    }
   ],
   "source": [
    "test = x[0][1].data[0]\n",
    "np.sum(test != 0, axis=1)\n",
    "row_ind, col_ind = linear_sum_assignment(test != 0, maximize = True)\n",
    "print((test != 0)[row_ind, col_ind].sum() / 2)\n",
    "row_ind, col_ind = linear_sum_assignment(test, maximize = True)\n",
    "print(test[row_ind, col_ind].sum() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  2,  1,  8,  4,  3,  7,  3,  1,  1,  5,  2,  3,  8,  1,  0,  3,\n",
       "        5,  5,  6,  4,  2,  7,  2,  2,  2,  7,  5,  0,  5,  2,  1,  4,  0,\n",
       "        3,  2,  1,  9,  1,  1,  4,  1,  2,  4,  2,  2, 11,  2,  1,  0,  2,\n",
       "        3,  2,  3,  1,  2,  2,  7,  8,  7,  5, 10,  6,  5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test != 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([44., 37., 52., 58., 58., 55., 58., 61., 52., 34., 35., 63., 48.,\n",
       "       50., 47., 15., 63., 58., 63., 58., 37., 37., 58., 37., 54., 51.,\n",
       "       58., 63., 28., 63., 51., 56.,  3.,  5., 12., 10.,  0., 29., 18.,\n",
       "        0., 29., 24., 10., 26.,  0., 10., 29., 10., 12.,  5.,  3., 16.,\n",
       "        8., 16., 24., 12.,  5., 26., 26., 23., 26., 26., 26., 18.],      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = preds['match'].data[0]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]),\n",
       " array([36,  2,  1, 50, 61, 55, 60, 32, 52, 34, 44, 40, 48, 62, 47, 42, 51,\n",
       "        35, 38, 43, 33, 59, 57, 37, 54, 53, 58, 46, 49, 63, 30, 56,  7, 45,\n",
       "         9, 17,  0, 23, 18, 39, 11, 28, 41, 19, 10, 15, 27, 14, 12, 20,  3,\n",
       "        16,  8, 25, 24,  5, 31, 22, 26, 21,  6,  4, 13, 29]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_ind, col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 24.1\n"
     ]
    }
   ],
   "source": [
    "count_mismatches_two_way(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMISSION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16, 64 -> 0.6146\n",
    "16, 100 -> 0.4943\n",
    "16, 200 -> 0.3309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_unif_accuracy(results):\n",
    "    xs, ys = list(zip(*results.keys()))\n",
    "    vals = list(results.values())\n",
    "    plt.scatter(xs, ys, c=vals, cmap='BuGn')\n",
    "    plt.xlabel('Offset')\n",
    "    plt.ylabel('Scale')\n",
    "    plt.show()\n",
    "\n",
    "unif_dist_results = {\n",
    "    (0, 0.001): 0.928,\n",
    "    (1, 0.001): 0.962,\n",
    "    (0, 0.1): 0.931,\n",
    "    (0, 10): 0.883,\n",
    "    (0, 100): 0.77,\n",
    "    (50, 150): 0.72,\n",
    "    (500, 1500): 0.69,\n",
    "    (5000, 15000): 0.7  \n",
    "}\n",
    "\n",
    "unif_cross_train = {\n",
    "    (0, 0.001): 0.948,\n",
    "    (1, 0.001): 0.967,\n",
    "    (0, 0.1): 0.916,\n",
    "    (0, 10): 0.86,\n",
    "    (0, 100): 0.75,\n",
    "    (50, 150): 0.72,\n",
    "    (500, 1500): 0.72,\n",
    "    (5000, 15000): 0.69  \n",
    "}\n",
    "\n",
    "plot_unif_accuracy(unif_dist_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ER p=0.25, 100 8x8 train and 40 32x32 test => 0.94 in 100 iterations\n",
    "\n",
    "BA param=3, 100 8x8 train and 40 32x32 test => 0.97 in 100 iterations\n",
    "\n",
    "BA param=5, 100 8x8 train and 40 32x32 test => 0.95 in 100 iterations (0.951 in 200 so has pretty much converged after 100)\n",
    "\n",
    "BA param=7, 100 8x8 train and 40 32x32 test => 0.946 in 100 iterations\n",
    "\n",
    "#### Cross training\n",
    "BA param=7 to BA param=3\n",
    "\n",
    "BA param=7 to ER p=0.25 0.946 with BA to 0.939 with ER (same as if trained only on BA)\n",
    "\n",
    "ER p=0.25 to BA param=3 went from 0.939 with ER to 0.967 with BA (BA param 3 was 0.97 so basically nothing lost)\n",
    "\n",
    "#### Weight variations\n",
    "Uniform\n",
    "* 0,0.001 -> 0.928\n",
    "* 1,1.001 -> 0.962\n",
    "* 0,0.1 -> 0.931\n",
    "* 0,10 -> 0.883\n",
    "* 0,100 -> 0.77\n",
    "* 50, 200 -> 0.72\n",
    "* 500, 2000 -> 0.69\n",
    "* 5000, 20000 -> 0.7\n",
    "\n",
    "Normal:\n",
    "Basically same.\n",
    "\n",
    "Gumbel\n",
    "* 0,0.001 -> 0.323\n",
    "* 1,1.001 -> 0.849\n",
    "* 0,0.1 -> 0.498\n",
    "* 5,10 -> 0.82\n",
    "* 5,100 -> 0.8\n",
    "\n",
    "#### Weight cross training\n",
    "Train ER p=0.25 unif 0,1:\n",
    "* 0,0.001 -> 0.948\n",
    "* 1,1.001 -> 0.967\n",
    "* 0,0.1 -> 0.916\n",
    "* 0,10 -> 0.86\n",
    "* 0,100 -> 0.75\n",
    "* 50, 200 -> 0.72\n",
    "* 500, 2000 -> 0.72\n",
    "* 5000, 20000 -> 0.69\n",
    "\n",
    "=> Seems to weight generalize quite well. Actually even better because basically no statistical difference with if we trained separately.\n",
    "\n",
    "Train normal 5000, 20000:\n",
    "* 0, 0.001 -> 0.39 (maybe it's the large to small that was a problem here? Also those values make little sense for a normal RV)\n",
    "\n",
    "Other direction train normal 0, 0.001 (got to 0.78):\n",
    "* 5000, 20000 -> 0.76\n",
    "\n",
    "=> small to large seems better\n",
    "\n",
    "\n",
    "#### Larger graphs\n",
    "Same training\n",
    "ER p=0.25 8x8 train:\n",
    "* 100x100 test goes to 0.88\n",
    "* 200x200 goes to 0.63 (only 12 prediction mismatches though)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preliminary results\n",
    "random permutation/matching: 0.18\n",
    "\n",
    "MPNN:\n",
    "learned predictions: 0.67\n",
    "\n",
    "GAT:\n",
    "learned predictions: 0.72\n",
    "\n",
    "Got better with double ended predictions\n",
    "\n",
    "Partial: 0.64 while greedy was doing about 0.92 on the same instance. Main reason seems to be that max weight is around 1.5 => can get at most 2/3 OPT\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Counting the number of matching constraints violated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For two-way\n",
    "def count_mismatches_two_way(predictions):\n",
    "    count = 0\n",
    "    data = predictions[\"match\"].data\n",
    "    nb_graphs = data.shape[0]\n",
    "    for datapoint in range(data.shape[0]):\n",
    "        for i in range(32):\n",
    "            owner = data[datapoint][i]\n",
    "            good = data[datapoint][int(owner)]\n",
    "            if good != i:\n",
    "                count += 1\n",
    "    print(f\"average number of edges contradicting matching: {count / nb_graphs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For self-loops\n",
    "def count_mismatches_self_loop(predictions):\n",
    "    count = 0\n",
    "    data = predictions[\"match\"].data\n",
    "    nb_graphs = data.shape[0]\n",
    "    for datapoint in range(data.shape[0]):\n",
    "        owners = set(np.array(data[datapoint][32:64]))\n",
    "        count += 32 - len(owners)\n",
    "    print(f\"average number of edges contradicting matching: {count / nb_graphs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "b = np.array([2, 3])\n",
    "print(np.concatenate((a, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
