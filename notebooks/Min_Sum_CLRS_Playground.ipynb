{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:34:44.070186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import clrs\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "rng_key = jax.random.PRNGKey(rng.randint(2 ** 32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def samplers(num_samples, length, batch_size, **kwargs):\n",
    "    if batch_size > num_samples:\n",
    "        batch_size = num_samples\n",
    "\n",
    "    def _iterate_sampler(sampler, batch_size):\n",
    "        while True:\n",
    "            yield sampler.next(batch_size)\n",
    "\n",
    "    sampler, spec = clrs.build_sampler(\n",
    "        name = 'simplified_min_sum',\n",
    "        num_samples = num_samples,\n",
    "        length = length,\n",
    "        weighted = True,\n",
    "        **kwargs)  # number of nodes\n",
    "\n",
    "    sampler = _iterate_sampler(sampler, batch_size = batch_size)\n",
    "    return sampler, spec\n",
    "\n",
    "train_sampler, spec = samplers(100, 8, 32, generator = \"ER\")\n",
    "test_sampler, _ = samplers(40, 64, 40)"
   ],
   "metadata": {
    "id": "OEo_Gj1j3Z6M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ef53c51-9bc2-4a49-b4cf-592a32dfa5b7"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.00847525, 0.41870585,\n        0.40326243],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.00847525, 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.41870585, 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.40326243, 0.        , ..., 0.        , 0.        ,\n        0.        ]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(test_sampler)\n",
    "sample.features.inputs[1].data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def define_model(spec, train_sampler, model = \"mpnn\"):\n",
    "    if model == \"mpnn\":\n",
    "        processor_factory = clrs.get_processor_factory('mpnn', use_ln = True,\n",
    "                                                   nb_triplet_fts = 0)  #use_ln => use layer norm\n",
    "    elif model == \"gat\":\n",
    "        processor_factory = clrs.get_processor_factory('gat', use_ln=True, nb_heads = 4, nb_triplet_fts = 0)\n",
    "\n",
    "    elif model == \"mpnndoublemax\":\n",
    "        processor_factory = clrs.get_processor_factory('mpnndoublemax', use_ln = True,\n",
    "                                                       nb_triplet_fts = 0)  #use_ln => use layer norm\n",
    "\n",
    "    model_params = dict(\n",
    "        processor_factory = processor_factory,  # contains the processor_factory\n",
    "        hidden_dim = 32,  # TODO put back to 32 if no difference\n",
    "        encode_hints = True,\n",
    "        decode_hints = True,\n",
    "        #decode_diffs=False,\n",
    "        #hint_teacher_forcing_noise=1.0,\n",
    "        hint_teacher_forcing = 1.0,\n",
    "        use_lstm = False,\n",
    "        learning_rate = 0.001,\n",
    "        checkpoint_path = '/tmp/checkpt',\n",
    "        freeze_processor = False,  # Good for post step\n",
    "        dropout_prob = 0.5,\n",
    "        # nb_msg_passing_steps=3,\n",
    "    )\n",
    "\n",
    "    dummy_trajectory = next(train_sampler)  # jax needs a trajectory that is plausible looking to init\n",
    "\n",
    "    model = clrs.models.BaselineModel(\n",
    "        spec = spec,\n",
    "        dummy_trajectory = dummy_trajectory,\n",
    "        **model_params\n",
    "    )\n",
    "\n",
    "    model.init(dummy_trajectory.features, 1234)  # 1234 is a random seed\n",
    "\n",
    "    return model\n",
    "\n",
    "model = define_model(spec, train_sampler, \"mpnndoublemax\")"
   ],
   "metadata": {
    "id": "L-p0jOCq5sPV"
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduction: doublemax\n",
      "Double max!\n",
      "reduction: doublemax\n",
      "Double max!\n",
      "reduction: doublemax\n",
      "Double max!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# No evaluation since we are postprocessing with soft: TO CHANGE -> baselines.py line 336 outs change hard to False\n",
    "# step = 0\n",
    "#\n",
    "# while step <= 1:\n",
    "#     feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "#     rng_key, new_rng_key = jax.random.split(rng_key) # jax needs new random seed at step\n",
    "#     cur_loss = model.feedback(rng_key, feedback) # loss is contained in model somewhere\n",
    "#     rng_key = new_rng_key\n",
    "#     if step % 10 == 0:\n",
    "#         print(step)\n",
    "#     step += 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, epochs, train_sampler, test_sampler):\n",
    "    step = 0\n",
    "    rng_key = jax.random.PRNGKey(rng.randint(2 ** 32))\n",
    "\n",
    "    while step <= epochs:\n",
    "        feedback, test_feedback = next(train_sampler), next(test_sampler)\n",
    "        # TODO remove - testing if uses hints on tests\n",
    "        # shape = test_feedback.features.hints[0].data[0].shape\n",
    "        # test_feedback.features.hints[0].data = test_feedback.features.hints[0].data[0, :, :].reshape((1, *shape))\n",
    "\n",
    "        rng_key, new_rng_key = jax.random.split(rng_key)  # jax needs new random seed at step\n",
    "        cur_loss = model.feedback(rng_key, feedback)  # loss is contained in model somewhere\n",
    "        rng_key = new_rng_key\n",
    "        if step % 10 == 0:\n",
    "            predictions_val, _ = model.predict(rng_key, feedback.features)\n",
    "            out_val = clrs.evaluate(feedback.outputs, predictions_val)\n",
    "            predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "            out = clrs.evaluate(test_feedback.outputs, predictions)\n",
    "            print(\n",
    "                f'step = {step} | loss = {cur_loss} | val_acc = {out_val[\"score\"]} | test_acc = {out[\"score\"]}')  # here, val accuracy is actually training accuracy, not great but is example\n",
    "        step += 1\n",
    "    return model"
   ],
   "metadata": {
    "id": "3pSKQ2wi62Br",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1acd203-e256-4f39-e059-9c7e55942a87"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 | loss = 0.15068848431110382 | val_acc = 0.875 | test_acc = 0.592578113079071\n",
      "step = 10 | loss = 0.14301234483718872 | val_acc = 0.8125 | test_acc = 0.604687511920929\n",
      "step = 20 | loss = 0.15729369223117828 | val_acc = 0.90625 | test_acc = 0.6285156607627869\n",
      "step = 30 | loss = 0.15455292165279388 | val_acc = 0.79296875 | test_acc = 0.5980468988418579\n",
      "step = 40 | loss = 0.14386148750782013 | val_acc = 0.78515625 | test_acc = 0.595703125\n",
      "step = 50 | loss = 0.14222665131092072 | val_acc = 0.82421875 | test_acc = 0.5703125\n",
      "step = 60 | loss = 0.1405799686908722 | val_acc = 0.80078125 | test_acc = 0.596875011920929\n",
      "step = 70 | loss = 0.12779049575328827 | val_acc = 0.83203125 | test_acc = 0.599609375\n",
      "step = 80 | loss = 0.15173453092575073 | val_acc = 0.796875 | test_acc = 0.598437488079071\n",
      "step = 90 | loss = 0.17027558386325836 | val_acc = 0.859375 | test_acc = 0.6117187738418579\n",
      "step = 100 | loss = 0.13329805433750153 | val_acc = 0.84765625 | test_acc = 0.6078125238418579\n"
     ]
    }
   ],
   "source": [
    "model = train(model, 100, train_sampler, test_sampler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def matching_value(samples, predictions, partial = False, match_rest = False, opt_scipy = False):\n",
    "    features = samples.features\n",
    "    gt_matchings = samples.outputs[0].data\n",
    "    # inputs for the matrix A are at index 1 (see spec.py)\n",
    "    data = features.inputs[1].data\n",
    "    masks = features.inputs[3].data\n",
    "    pred_accuracy = 0\n",
    "    greedy_accuracy = 0\n",
    "\n",
    "    # Iterating over all the samples\n",
    "    for i in range(data.shape[0]):\n",
    "        if opt_scipy:\n",
    "            row_ind, col_ind = linear_sum_assignment(data[i], maximize = True)\n",
    "            max_weight = data[i][row_ind, col_ind].sum() / 2  #TODO why /2\n",
    "        else:\n",
    "            max_weight = compute_greedy_matching_weight(i, data, masks, gt_matchings[i])\n",
    "\n",
    "        predicted_matching = predictions[\"match\"].data[i]\n",
    "\n",
    "        if partial:\n",
    "            preds_weight = compute_partial_matching_weight(i, data, masks, predicted_matching)\n",
    "            print(f\"opt: {max_weight}, greedy learned: {preds_weight}\")\n",
    "        else:\n",
    "            preds_weight = compute_greedy_matching_weight(i, data, masks, predicted_matching, match_rest = match_rest)\n",
    "            print(f\"opt: {max_weight}, partial: {preds_weight}\")\n",
    "\n",
    "        # assert preds_weight <= max_weight\n",
    "        greedy_matching_weight = naive_greedy(i, data, masks)\n",
    "        print(f\"Naive greedy: {greedy_matching_weight}\")\n",
    "        greedy_accuracy += greedy_matching_weight / max_weight\n",
    "        pred_accuracy += preds_weight / max_weight\n",
    "\n",
    "    return pred_accuracy / data.shape[0], greedy_accuracy / data.shape[0]\n",
    "\n",
    "def naive_greedy(i, data, masks):\n",
    "    \"\"\"Computes a matching greedily by for each node adding the maximum neighbor that\n",
    "    hasn't yet been added to the matching\"\"\"\n",
    "\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    # At the start, all the right hand side values are possible matches\n",
    "    matching_mask = np.full(A.shape[0], True)\n",
    "\n",
    "    for buyer in range(n):\n",
    "        # Checking if there are more elements to match (if more buyers than goods)\n",
    "        if A[buyer, matching_mask].shape[0] != 0:\n",
    "            matching_weight += np.max(A[buyer, matching_mask])\n",
    "            # Recovering the index of the maximum, inspired by http://seanlaw.github.io/2015/09/10/numpy-argmin-with-a-condition/\n",
    "            subset_idx = np.argmax(A[buyer, matching_mask])\n",
    "            good = np.arange(A.shape[1])[matching_mask][subset_idx]\n",
    "            # The corresponding good cannot be used anymore\n",
    "            matching_mask[good] = False\n",
    "    return matching_weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_greedy_matching_weight(i, data, masks, matching, match_rest = False):\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # Only consider the matching values for consumers\n",
    "    matching = np.where(goods_mask == 1, matching, -1)\n",
    "    unmatched_goods = set(range(n, n + m))\n",
    "    unmatched_buyers = set(range(n))\n",
    "\n",
    "    for buyer in range(n):\n",
    "        if buyer in matching:\n",
    "            # If several goods point to the same buyer, keep the one with maximum weight\n",
    "            mask = matching == buyer\n",
    "            matching_weight += np.max(A[buyer, mask])\n",
    "            # Recovering the index of the maximum, inspired by http://seanlaw.github.io/2015/09/10/numpy-argmin-with-a-condition/\n",
    "            subset_idx = np.argmax(A[buyer, mask])\n",
    "            good = np.arange(A.shape[1])[mask][subset_idx]\n",
    "            unmatched_goods.remove(good)\n",
    "            unmatched_buyers.remove(buyer)\n",
    "\n",
    "    if match_rest and len(unmatched_goods) > 0 and len(unmatched_buyers) > 0:\n",
    "        # Compute optimal matching on the remaining unmatched nodes\n",
    "        mask = np.zeros(A.shape)\n",
    "        # TODO this is a horrible solution, there's definitely a prettier solution\n",
    "        mask[list(unmatched_buyers)] += 1\n",
    "        mask[:, list(unmatched_goods)] += 1\n",
    "        mask = np.where(mask == 2, True, False)\n",
    "        remaining_bipartite_graph = A * mask\n",
    "        row_ind, col_ind = linear_sum_assignment(remaining_bipartite_graph, maximize = True)\n",
    "        opt = A[row_ind, col_ind].sum() / 2  #TODO do I always need the division by 2\n",
    "        matching_weight += opt\n",
    "\n",
    "    return matching_weight\n",
    "\n",
    "\n",
    "def compute_partial_matching_weight(i, data, masks, matching):\n",
    "    # Matching is expected to be a (n+m)x(n+m) matrix where each row sums to 1 (weights assigned to other nodes)\n",
    "\n",
    "    matching_weight = 0\n",
    "    A = data[i]\n",
    "    buyers_mask = masks[i]\n",
    "    n = int(np.sum(buyers_mask))\n",
    "    goods_mask = 1 - buyers_mask\n",
    "    m = int(np.sum(goods_mask))\n",
    "\n",
    "    # We only care about the buyer -> good connections\n",
    "    A_submatrix = A[:n, n:n + m]\n",
    "    matching = matching[:n, n:n + m]\n",
    "\n",
    "    max_weight = np.max(np.sum(matching, axis = 0))\n",
    "    print(f\"max weight: {max_weight}\")\n",
    "    matching /= max_weight\n",
    "    return np.sum(matching * A_submatrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt: 28.987174894406635, partial: 27.698218043886605\n",
      "Naive greedy: 26.751829752404866\n",
      "opt: 28.673538026677384, partial: 26.631343399451104\n",
      "Naive greedy: 25.375112883780695\n",
      "opt: 29.28512827497548, partial: 27.08840981100459\n",
      "Naive greedy: 26.767052757473845\n",
      "opt: 28.567972726482722, partial: 27.516866746498213\n",
      "Naive greedy: 26.555935021967624\n",
      "opt: 28.285468675657334, partial: 27.03739412638852\n",
      "Naive greedy: 25.51905125840595\n",
      "opt: 29.807240647746205, partial: 26.892068653328785\n",
      "Naive greedy: 28.63823640433621\n",
      "opt: 28.77835963216673, partial: 26.642746485970438\n",
      "Naive greedy: 25.591410127184165\n",
      "opt: 29.50758037200704, partial: 27.644329485438174\n",
      "Naive greedy: 27.663127212119207\n",
      "opt: 29.578480584670196, partial: 27.18969576791876\n",
      "Naive greedy: 26.788630524188175\n",
      "opt: 28.845570090536263, partial: 26.271818073117043\n",
      "Naive greedy: 26.308393553431728\n",
      "opt: 29.50758037200704, partial: 27.644329485438174\n",
      "Naive greedy: 27.663127212119207\n",
      "opt: 28.922458960290797, partial: 26.264473318838363\n",
      "Naive greedy: 26.640023647352347\n",
      "opt: 28.519282983760966, partial: 27.201661023101632\n",
      "Naive greedy: 27.332112060360192\n",
      "opt: 28.513765858001605, partial: 26.534850663472653\n",
      "Naive greedy: 25.774531999548838\n",
      "opt: 28.922458960290797, partial: 26.264473318838363\n",
      "Naive greedy: 26.640023647352347\n",
      "opt: 29.10483787452452, partial: 27.727296867173738\n",
      "Naive greedy: 27.326035505951946\n",
      "opt: 28.922458960290797, partial: 26.264473318838363\n",
      "Naive greedy: 26.640023647352347\n",
      "opt: 29.664959453564865, partial: 29.307141669702148\n",
      "Naive greedy: 28.945701191730162\n",
      "opt: 28.935059129377294, partial: 28.275153758851996\n",
      "Naive greedy: 27.549391976462612\n",
      "opt: 28.673538026677384, partial: 26.631343399451104\n",
      "Naive greedy: 25.375112883780695\n",
      "opt: 28.65380933036102, partial: 26.822944934682482\n",
      "Naive greedy: 25.474437427946082\n",
      "opt: 28.845570090536263, partial: 26.271818073117043\n",
      "Naive greedy: 26.308393553431728\n",
      "opt: 28.845570090536263, partial: 26.271818073117043\n",
      "Naive greedy: 26.308393553431728\n",
      "opt: 28.920849203388137, partial: 27.22073555541177\n",
      "Naive greedy: 27.555891025218298\n",
      "opt: 29.4003594317771, partial: 28.283277806570606\n",
      "Naive greedy: 26.354923986956617\n",
      "opt: 28.922458960290797, partial: 26.264473318838363\n",
      "Naive greedy: 26.640023647352347\n",
      "opt: 29.50758037200704, partial: 27.644329485438174\n",
      "Naive greedy: 27.663127212119207\n",
      "opt: 28.922458960290797, partial: 26.264473318838363\n",
      "Naive greedy: 26.640023647352347\n",
      "opt: 29.50758037200704, partial: 27.644329485438174\n",
      "Naive greedy: 27.663127212119207\n",
      "opt: 28.342850891330066, partial: 26.7683334381835\n",
      "Naive greedy: 25.65860962575157\n",
      "opt: 28.285468675657334, partial: 27.03739412638852\n",
      "Naive greedy: 25.51905125840595\n",
      "opt: 28.342850891330066, partial: 26.7683334381835\n",
      "Naive greedy: 25.65860962575157\n",
      "opt: 28.848084356181282, partial: 27.478347314386067\n",
      "Naive greedy: 26.164822946885128\n",
      "opt: 28.567972726482722, partial: 27.516866746498213\n",
      "Naive greedy: 26.555935021967624\n",
      "opt: 28.611211686143477, partial: 27.906571631361977\n",
      "Naive greedy: 26.986033771271188\n",
      "opt: 28.922458960290797, partial: 26.264473318838363\n",
      "Naive greedy: 26.640023647352347\n",
      "opt: 29.807240647746205, partial: 26.892068653328785\n",
      "Naive greedy: 28.63823640433621\n",
      "opt: 29.038836110448365, partial: 27.24733161387423\n",
      "Naive greedy: 26.096401066749788\n",
      "opt: 29.034856701078517, partial: 28.174473616010953\n",
      "Naive greedy: 27.044086202729993\n",
      "opt: 29.664959453564865, partial: 29.307141669702148\n",
      "Naive greedy: 28.945701191730162\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.9377233284672846, 0.9233354604051114)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feedback = next(test_sampler)\n",
    "predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "matching_value(test_feedback, predictions, partial = False, match_rest = False, opt_scipy = True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def variation_testing(train_params, test_params, epochs = 100, model = None, bypass_training = False):\n",
    "    if model is None and bypass_training:\n",
    "        print(\"Need a model to bypass training\")\n",
    "        return\n",
    "\n",
    "\n",
    "    matching_values = []\n",
    "    for train_param, test_param in zip(train_params, test_params):\n",
    "        train_length = train_param.get(\"length\", 16)\n",
    "        test_length = test_param.get(\"length\", 64)\n",
    "        train_param.pop(\"length\", None)\n",
    "        test_param.pop(\"length\", None)\n",
    "\n",
    "        test_num_samples = 10\n",
    "        batch_size = test_num_samples\n",
    "\n",
    "        test_sampler, _ = samplers(test_num_samples, test_length, batch_size, **test_param)\n",
    "\n",
    "        if not bypass_training:\n",
    "            train_sampler, spec = samplers(100, train_length, 40, **train_param)\n",
    "            model = define_model(spec, train_sampler, model=\"mpnn\")\n",
    "            train(model, 150, train_sampler, test_sampler)\n",
    "        else:\n",
    "            print(\"Bypassing training\")\n",
    "\n",
    "        test_feedback = next(test_sampler)\n",
    "        predictions, _ = model.predict(rng_key, test_feedback.features)\n",
    "        accuracy = matching_value(test_feedback, predictions, partial = False, match_rest = False, opt_scipy = True)\n",
    "\n",
    "        matching_values.append((train_param, test_param, accuracy))\n",
    "    return model, matching_values\n",
    "\n",
    "weight_params = [{\"low\": 0, \"high\": 0.001},\n",
    "                 {\"low\": 1, \"high\": 1.001},\n",
    "                 {\"low\": 0, \"high\": 0.1},\n",
    "                 {\"low\": 0, \"high\": 1},\n",
    "                 {\"low\": 0, \"high\": 10},\n",
    "                 {\"low\": 0, \"high\": 100},\n",
    "                 {\"low\": 50, \"high\": 200},\n",
    "                 {\"low\": 500, \"high\": 2000},\n",
    "                 {\"low\": 5000, \"high\": 20000}\n",
    "                 ]\n",
    "\n",
    "length_training = [{\"generator\": \"ER\"}]\n",
    "length_testing = [{\"generator\": \"ER\", \"length\": 1000, \"p\": 0.01}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model, results = variation_testing(length_training, length_testing, model = model, bypass_training = True)\n",
    "\n",
    "results\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# import copy\n",
    "# model2 = copy.deepcopy(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ER p=0.25, 100 8x8 train and 40 32x32 test => 0.94 in 100 iterations\n",
    "\n",
    "BA param=3, 100 8x8 train and 40 32x32 test => 0.97 in 100 iterations\n",
    "\n",
    "BA param=5, 100 8x8 train and 40 32x32 test => 0.95 in 100 iterations (0.951 in 200 so has pretty much converged after 100)\n",
    "\n",
    "BA param=7, 100 8x8 train and 40 32x32 test => 0.946 in 100 iterations\n",
    "\n",
    "#### Cross training\n",
    "BA param=7 to BA param=3\n",
    "\n",
    "BA param=7 to ER p=0.25 0.946 with BA to 0.939 with ER (same as if trained only on BA)\n",
    "\n",
    "ER p=0.25 to BA param=3 went from 0.939 with ER to 0.967 with BA (BA param 3 was 0.97 so basically nothing lost)\n",
    "\n",
    "#### Weight variations\n",
    "Uniform\n",
    "* 0,0.001 -> 0.928\n",
    "* 1,1.001 -> 0.962\n",
    "* 0,0.1 -> 0.931\n",
    "* 0,10 -> 0.883\n",
    "* 0,100 -> 0.77\n",
    "* 50, 200 -> 0.72\n",
    "* 500, 2000 -> 0.69\n",
    "* 5000, 20000 -> 0.7\n",
    "\n",
    "Normal:\n",
    "Basically same.\n",
    "\n",
    "Gumbel\n",
    "* 0,0.001 -> 0.323\n",
    "* 1,1.001 -> 0.849\n",
    "* 0,0.1 -> 0.498\n",
    "* 5,10 -> 0.82\n",
    "* 5,100 -> 0.8\n",
    "\n",
    "#### Weight cross training\n",
    "Train ER p=0.25 unif 0,1:\n",
    "* 0,0.001 -> 0.948\n",
    "* 1,1.001 -> 0.967\n",
    "* 0,0.1 -> 0.916\n",
    "* 0,10 -> 0.86\n",
    "* 0,100 -> 0.75\n",
    "* 50, 200 -> 0.72\n",
    "* 500, 2000 -> 0.72\n",
    "* 5000, 20000 -> 0.69\n",
    "\n",
    "=> Seems to weight generalize quite well. Actually even better because basically no statistical difference with if we trained separately.\n",
    "\n",
    "Train normal 5000, 20000:\n",
    "* 0, 0.001 -> 0.39 (maybe it's the large to small that was a problem here? Also those values make little sense for a normal RV)\n",
    "\n",
    "Other direction train normal 0, 0.001 (got to 0.78):\n",
    "* 5000, 20000 -> 0.76\n",
    "\n",
    "=> small to large seems better\n",
    "\n",
    "\n",
    "#### Larger graphs\n",
    "Same training\n",
    "ER p=0.25 8x8 train:\n",
    "* 100x100 test goes to 0.88\n",
    "* 200x200 goes to 0.63 (only 12 prediction mismatches though)\n",
    "* 200x200 p=0.3 =>\n",
    "* 250x250 => 0.9448 (BUT p=0.1 to not kill my computer)\n",
    "*\n",
    "Try this but 16x16 train\n",
    "\n",
    "#### RIDESHARE\n",
    "8x8 train,\n",
    "* 32x32 test => 0.96\n",
    "* 50x50 test => 0.96\n",
    "* 100x100 test => 0.938\n",
    "* 250x250 test => 0.9\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminary results\n",
    "random permutation/matching: 0.18\n",
    "\n",
    "MPNN:\n",
    "learned predictions: 0.67\n",
    "\n",
    "GAT:\n",
    "learned predictions: 0.72\n",
    "\n",
    "Got better with double ended predictions\n",
    "\n",
    "Partial: 0.64 while greedy was doing about 0.92 on the same instance. Main reason seems to be that max weight is around 1.5 => can get at most 2/3 OPT\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting the number of matching constraints violated"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 12.2\n"
     ]
    }
   ],
   "source": [
    "# For two-way\n",
    "def count_mismatches_two_way(predictions):\n",
    "    count = 0\n",
    "    data = predictions[\"match\"].data\n",
    "    nb_graphs = data.shape[0]\n",
    "    for datapoint in range(data.shape[0]):\n",
    "        for i in range(32):\n",
    "            owner = data[datapoint][i]\n",
    "            good = data[datapoint][int(owner)]\n",
    "            if good != i:\n",
    "                count += 1\n",
    "    print(f\"average number of edges contradicting matching: {count / nb_graphs}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of edges contradicting matching: 0.4\n"
     ]
    }
   ],
   "source": [
    "# For self-loops\n",
    "def count_mismatches_self_loop(predictions):\n",
    "    count = 0\n",
    "    data = predictions[\"match\"].data\n",
    "    nb_graphs = data.shape[0]\n",
    "    for datapoint in range(data.shape[0]):\n",
    "        owners = set(np.array(data[datapoint][32:64]))\n",
    "        count += 32 - len(owners)\n",
    "    print(f\"average number of edges contradicting matching: {count / nb_graphs}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "b = np.array([2, 3])\n",
    "print(np.concatenate((a, b)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
